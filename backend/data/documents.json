[
  {
    "id": "092a3e8d-6213-484d-af16-ab3223eb2d46",
    "title": "基于深度学习和图像识别的CAD建筑图纸识别_李培德",
    "fileName": "基于深度学习和图像识别的CAD建筑图纸识别_李培德.pdf",
    "fileType": "pdf",
    "fileSize": 8480652,
    "uploadTime": "2025-12-29T18:17:45.228103",
    "parsed": false,
    "parseStatus": "pending",
    "tags": [],
    "folderId": "root",
    "filePath": "./uploads/20251229181745_基于深度学习和图像识别的CAD建筑图纸识别_李培德.pdf"
  },
  {
    "id": "563790f8-5fef-4cea-981d-41f492b1fa25",
    "title": "基于深度学习和图像识别的CAD建筑图纸识别_李培德",
    "fileName": "基于深度学习和图像识别的CAD建筑图纸识别_李培德.pdf",
    "fileType": "pdf",
    "fileSize": 8480652,
    "uploadTime": "2025-12-29T19:05:51.909726",
    "parsed": true,
    "parseStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251229190551_基于深度学习和图像识别的CAD建筑图纸识别_李培德.pdf",
    "errorMessage": "vlm-vllm-engine backend is not supported in async mode, please use vlm-vllm-async-engine backend",
    "markdownContent": "# 北京邮电大学\n\n# 硕士学位论文\n\n![](images/efe571ea83f7a69b1384ff4bc1eeeca54c09de3e73ca9c4bcbd472b677625344.jpg)\n\n题目： 基于深度学习和图像识别的CAD建筑图纸识别\n\n学号： 2021180009\n\n姓名： 李培德\n\n学科专业： 通信工程\n\n培养方式： 非全日制\n\n导师： 张欣\n\n学 院： 信息与通信工程学院\n\n2024年05月26日\n\n中国·北京\n\n# 北京邮电大学\n\n# 硕士学位论文(学术学位)\n\n![](images/334ac893110c3321160fdced37f115d29cb1b071411020e4696c1e2896603465.jpg)\n\n# 题目： 基于深度学习和图像识别的CAD建筑图纸识别\n\n学号： 2021180009\n\n姓名： 李培德\n\n学科专业：通信工程（含宽带网络、移动通信等）\n\n培养方式： 非全日制\n\n导师： 张欣\n\n学 院： 信息与通信工程学院\n\n2024年05月26日\n\n![](images/6c5983a531a2bb978b49af05cdc8fc3e31575c70de851778473d8dff98157de9.jpg)\n\n# BEIJING UNIVERSITY OF POSTS AND\n\n# TELECOMMUNICATIONS\n\n# Master/Doctoral Dissertation\n\n# Recognition of CAD Architectural Drawings through Image Analysis and Deep Learning\n\nStudent ID: 2021180009\n\nAuthor: LiPeide\n\nSubject: Communication Engineering (including\n\nbroadband network, mobile communication)\n\nSupervisor: Zhangxin\n\nInstitute: School of Information and Communication\n\nEngineering\n\nMay 26,2024\n\n答辩委员会名单  \n\n<table><tr><td>职务</td><td>姓名</td><td>职称</td><td>工作单位</td></tr><tr><td>主席</td><td>王亚峰</td><td>教授</td><td>北京邮电大学</td></tr><tr><td>委员</td><td>袁超伟</td><td>教授</td><td>北京邮电大学</td></tr><tr><td>委员</td><td>高月红</td><td>副教授</td><td>北京邮电大学</td></tr><tr><td>委员</td><td>曹亘</td><td>高级工程师</td><td>中国联通研究院</td></tr><tr><td>委员</td><td></td><td></td><td></td></tr><tr><td>秘书</td><td>郑平</td><td></td><td>北京邮电大学</td></tr><tr><td>答辩日期</td><td colspan=\"3\">2024年5月26日</td></tr></table>\n\n# 摘要\n\n随着数字化网络和数字孪生的快速发展，传统的计算机辅助设计 (Computer Aided Design, CAD) 图纸难以再满足工程的需要，自动化识别 CAD 建筑图纸信息的需求日益增长。本文提出了一种结合深度学习和图像识别的 CAD 建筑图纸分析模型，用于 CAD 建筑图纸的元素识别。通过采集大量最新复杂的 CAD 建筑图纸，利用图像识别和深度学习网络架构，结合语义分割和图像处理，实现对 CAD 建筑图纸中的墙体、门和窗等元素进行自动提取和分类。本论文研究工作如下：\n\n（1）在建筑元素墙体识别方面，超越了传统双线墙的识别，创新性地加入了多线墙和实心墙的识别，更贴近工程实际，满足复杂图纸的识别需求。  \n（2）在墙体和窗户识别算法方面，引入阈值设置，通过精细调整参数，显著提升了识别准确率。  \n（3）在建筑元素门识别方面，为提升泛化能力和准确性，采用了基于YOLOv8的方法，并优化了图片预处理策略，有效提升了门元素的识别率。  \n（4）在数据集方面，采用了一个包含15663张图纸的大规模数据集，增强了模型的泛化能力。\n\n综上所述，基于图像识别的墙体识别模型不仅实现了双线墙识别，还对实心墙和多线墙进行了从无到有的识别拓展，并且添加了阈值设定，显著提升了墙体和窗户的识别率，其中双线墙、多线墙、实心墙和窗户的识别率分别达到了0.886、0.893、0.911和0.857；基于优化后的YOLOv8模型在高分辨率图像中实现了多种门元素的识别，并且门元素识别率达到了0.877。本论文的研究内容为数字孪生技术的发展提供了技术支持。通过提高建筑元素识别的效率，不仅可以加快了3D建筑模型的创建过程，还为数字孪生中的信号覆盖预测模拟提供了数据基础。\n\n关键词：CAD；建筑图纸；图像识别；YOLOv8；自动化识别\n\n# ABSTRACT\n\nWith the rapid development of digital networks and digital twins, traditional Computer Aided Design (CAD) drawings can no longer meet the needs of engineering. There is an increasing demand for the automated recognition of information in CAD architectural drawings. This paper proposes an analysis model for CAD architectural drawings that combines deep learning with image recognition to identify elements within the drawings. By collecting a large number of the latest and most complex CAD architectural drawings, and utilizing image recognition and deep learning network architectures, combined with semantic segmentation and image processing, the model is capable of automatically extracting and classifying elements such as walls, doors, and windows from CAD architectural drawings. The research work of this paper is as follows:\n\n(1) In terms of the identification of architectural element walls, it goes beyond the identification of traditional double-line walls and innovatively adds the identification of multi-line walls and solid walls, which is closer to the actual project and meets the identification needs of complex drawings.  \n(2) In terms of wall and window recognition algorithms, threshold settings are introduced and the recognition accuracy is significantly improved by finely adjusting parameters.  \n(3) In terms of architectural element door recognition, in order to improve the generalization ability and accuracy, a method based on YOLO v8 was adopted, and the image preprocessing strategy was optimized, which effectively improved the recognition rate of door elements.  \n(4) In terms of data set, a large-scale data set containing 15,663 drawings was used to enhance the generalization ability of the model.\n\nIn summary, the wall recognition model based on image recognition\n\nnot only achieved recognition of double-line walls but also expanded the recognition of solid walls and multi-line walls from scratch. Thresholds were added to significantly improve the recognition rates of walls and windows, with recognition rates for double-line walls, multi-line walls, solid walls, and windows reaching 0.886, 0.893, 0.911, and 0.857 respectively. The YOLO v8 model, after optimization, achieved recognition of various door elements in high-resolution images, with a recognition rate of 0.877 for door elements. The research content of this paper provides technical support for the development of digital twin technology. By improving the efficiency of building element recognition, not only can the creation process of 3D building models be accelerated, but it also provides a data foundation for signal coverage prediction simulation in digital twins.\n\nKEY WORDS: CAD; architectural drawings; Image recognition; YOLO v8; Automated Recognition\n\n# 目录\n\n# 第一章 绪论\n\n1.1 研究背景 1  \n1.2 国内外研究现状 2\n\n1.2.1 国外研究进展 2  \n1.2.2 国内研究进展 3  \n1.2.3 技术对比分析 3  \n1.2.4 研究存在问题 4\n\n1.3 本论文主要工作 5  \n1.4 论文组织结构 6\n\n# 第二章 相关理论与方法 7\n\n2.1 CAD建筑图纸预处理和栅格化 7  \n2.2 数据集概述 7\n\n2.3 OpenCV相关理论 9\n\n2.3.1canny边缘检测概述 9  \n2.3.2 形态学操作概述 9  \n2.3.3 矩形检测概述 10  \n2.3.4 直线检测概述 ..... 10  \n2.3.5 图像分割概述 10\n\n2.4YOLOv8相关理论 11\n\n2.4.1 YOLO v8 概述 11  \n2.4.2 YOLO v8 网络模型结构图 ..... 11  \n2.4.3 YOLO v8 网络模型结构设计 ..... 13  \n2.4.4 YOLO v8 检测技术的突破与应用前景 ..... 14\n\n2.5 CAD建筑图纸图像问题 14\n\n2.5.1 建筑图纸问题概述 14  \n2.5.2 建筑图纸优化方向 16\n\n2.6 本章小结 ..... 17\n\n# 第三章 基于 OpenCV 的 CAD 建筑图纸图像处理与分析 ..... 19\n\n3.1 双线墙识别 19\n\n3.1.1 算法概述 ..... 19  \n3.1.2 算法工作流程 ..... 19  \n3.1.3 数据输出 23  \n3.1.4 实验结果与分析 24\n\n3.2多线墙识别 25\n\n3.2.1 算法概述 ..... 25  \n3.2.2 算法工作流程 25  \n3.2.3 数据输出 26  \n3.2.4 实验结果与分析 26\n\n3.3实心墙识别 27\n\n3.3.1 算法概述 27  \n3.3.2 算法工作流程 ..... 27  \n3.3.3 数据输出 28  \n3.3.4 实验结果与分析 29\n\n3.4 窗户识别 30\n\n3.4.1 算法概述 30  \n3.4.2 算法工作流程 30  \n3.4.3 数据输出 32  \n3.4.4 实验结果与分析 32\n\n3.5 本章小节 33\n\n第四章 基于优化的YOLOv8的门检测方法 35\n\n4.1 引言 35  \n4.2 数据收集与预处理 35  \n4.3 参数优化 37  \n4.4 模型应用层优化 37  \n4.5 实验设计与结果 38\n\n4.5.1 未加入切割整合算法识别实验结果 ..... 39  \n4.5.2 加入切割整合算法实验结果 41  \n4.5.3 实验对比分析 42\n\n4.6 本章小结 ..... 44\n\n第五章 总结与展望 45\n\n5.1总结 45  \n5.2 研究展望 46\n\n参考文献. 48\n\n# 第一章 绪论\n\n# 1.1 研究背景\n\n随着科技的迅速发展，数字化转型已成为数字孪生技术领域的重要趋势。这种转型不仅改变了数字孪生模型的设计方法和工具，还对整个数字孪生领域的运作方式产生了深远影响。数字技术的引入，特别是CAD技术，已成为现代数字孪生项目不可或缺的一部分。CAD技术使设计师能够以前所未有的精度设计复杂的模型，从而大大提高了工作的效率。数字化不仅提升了设计质量，还促进了项目管理和协作的便利性，使得项目团队能够更容易地共享和修改设计方案。\n\nCAD技术的发展历程体现了数字孪生模型从传统手绘草图到高精度数字模型的转变。最初CAD系统被视为一种简单的绘图工具，帮助设计师创建更精确的图纸。但是随着数字孪生项目的复杂性增加，对自动化CAD图纸分析和三维建模的需求也随之增长。这种需求的驱动力来自提高设计效率、减少错误和降低成本的目标。自动化技术能够快速分析大量图纸，识别设计中的建筑元素，从而在建模阶段节约时间和资源。此外随着建筑信息模型（Building Information Modeling，BIM）技术的普及，设计师和工程师越来越多地依赖自动化工具来创建和管理复杂的项目信息。\n\n尽管 CAD 技术带来了诸多好处，但在将深度学习和图像识别技术应用于 CAD 图纸自动识别和分析的过程中，仍然存在许多挑战。首先 CAD 图纸的复杂性和多样性要求算法能够处理不同的设计风格和技术标准。此外深度学习模型需要大量的标注数据来训练，而高质量的训练数据在数字孪生领域往往难以获得。另一个挑战是算法的准确性和可靠性，这对于确保设计决策的正确性至关重要。为了克服这些挑战，研究人员需要开发更加先进和适应性强的算法，并设计有效的数据处理和增强策略。\n\n总而言之，随着数字化转型的深入，CAD技术已经成为数字孪生模型设计和规划的核心工具。同时随着对自动化和效率需求的日益增长，数字孪生领域对更先进的解决方案的渴望也随之增强。这种需求催生了将图像识别和深度学习技术整合进CAD工具中的趋势，以实现更高效、智能化的设计和分析流程。\n\n在将图像识别技术整合进 CAD 工具的过程中，尤其是在处理高分辨率的图纸时，图像识别展现出其独特价值。这种技术可以精确地捕捉图纸上的细节，识别出各种设计元素，并将这些信息转化为可用的数据。这一过程不仅提高了设计数据的可访问性，也为后续的分析和建模工作打下了坚实的基础。\n\n在将深度学习技术整合进 CAD 工具的过程中，深度学习为解决传统方法中的时间消耗和精度限制问题提供了新的途径。通过深度学习模型，可以自动识别和分类图纸中的不同元素，从而加速设计审核和修改过程。\n\n然而图像识别和深度学习技术在 CAD 图纸分析中的应用仍然面临着一系列挑战。除了前面提到的数据集和算法准确性问题，还有如何有效整合这些技术到现有的 CAD 系统中，以及如何确保这些技术的实用性和用户友好性。此外由于数字孪生领域的特性，技术解决方案必须符合行业标准，并能够应对各种复杂和多变的设计要求。\n\n数字化转型已在数字孪生领域树立了新的标准，CAD技术的发展和自动化需求的增长为图像识别和深度学习技术的融入提供了肥沃的土壤。尽管面临挑战，但这些先进技术的应用预示着数字孪生领域设计和规划的一个新时代的到来。因此深入研究和开发适用于CAD图纸的图像识别和深度学习技术，对于推动数字孪生领域的发展具有重要意义。\n\n# 1.2 国内外研究现状\n\n# 1.2.1 国外研究进展\n\n在国际层面，深度学习、图像识别和CAD图纸分析的研究已取得显著进展。一些研究团队集中在如何将OpenCV技术融入建筑设计和分析中，以提高效率和准确性。例如地平线网络(Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation，HorizonNet)[1]和双视图网络(A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama Abstract，DuLa-Net)[2]的研究展示了处理复杂室内场景中的强大潜力。HorizonNet通过双投影网络和特征融合策略实现了对复杂房间布局的高效识别，而DuLa-Net则通过结合全景视图和透视天花板视图提高了3D房间布局估计的准确性。这些方法为图像识别技术提供了新的思路，并为CAD图纸识别开辟了新的可能性。除了HorizonNet和DuLa-Net，全景上下文(PanoContext)[3]也为CAD图纸的3D重建提供了新的视角。这些方法虽然在处理非立方体布局和非曼哈顿世界假设的房间时可能会遇到挑战，但它们展示了深度学习在处理复杂室内场景中的潜力。\n\n同时 Simonyan 和 Zisserman[4]通过增加卷积网络的深度，显著提高了图像识别的准确性，而 Long 等人[5]则展示了全卷积网络（FCN）在语义分割任务中的应用，能够对图像进行像素级别的分类。Dodge 等人[6]进一步通过墙分割、对象检测和光学字符识别(Optical Character Recognition，OCR)技术，实现了从楼层平面图图像中自动提取室内空间信息，为建筑图纸的自动化识别提供了强有力的技术支持。\n\n此外其他团队则专注于YOLOv8技术的应用。如Ahmed等人[7]提出的改进的建筑自动分析楼层平面图系统，通过引入新的预处理方法和结构分析，显著提高了房间识别的准确率。Liu等人[8]实现了从光栅化楼层平面图到矢量图形表示的转换，这一方法在精度和召回率上都达到了生产就绪的性能水平。文献[9]提出了一种基于墙片的分割方法，有效地处理了建筑平面图中的墙片分割任务，而文献[10]探讨了基于空间分析的方法，为3D重建提供了基础。这些研究不仅提高了建筑元素识别的准确性，也为后续数字孪生建筑模型领域的发展提供了新的工具和方法。\n\n# 1.2.2 国内研究进展\n\n在国内，图像识别和深度学习技术在CAD图纸分析方面的应用也正在快速发展。首先房间网（RoomNet）技术通过端到端的学习方法，直接从单张RGB图像预测房间布局关键点，简化了传统多步骤的识别流程[11]。此外深度学习被应用于图形基装配产品表示的特征识别，这不仅为自动化装配规划提供了新的思路[12]，也推动了CAD图纸识别技术的发展。特别是Gimenez等人提出了一种从扫描的二维平面图中自动重建三维建筑模型的方法，该方法能够从2D平面图中提取信息，并生成IFC标准的3D模型[13]。此外Wei等人提出了一种自动化的墙检测过程，利用计算机视觉和2D图纸中的信息来减少3D建模时间，进一步提高了建筑图纸识别的效率[14]。\n\n在建筑平面图的图像文本检测方面，Schönfelder和Konig基于深度学习提出了专门训练的模型，用于检测和分类建筑平面图中的文本元素，为图纸信息提取和处理提供了基础[15]。同时Kim通过层次分析法对自动平面图分析技术进行评估，为技术价值的量化提供了新的视角[16]。此国内外研究者还探索了如何利用深度学习技术提高CAD图纸的语义理解，这对于建筑信息的自动化提取和利用具有重要意义[17,18]。并且金字塔场景解析网络的提出，结合全局上下文信息，在多个数据集上取得了最先进的性能，进一步推动了建筑图纸识别技术的发展[19]。\n\n尽管这些研究取得了重要进展，国内研究者仍面临着如何提高在复杂建筑环境中识别精度的挑战，以及如何克服深度学习模型训练数据集构建方面的局限性，这些限制了模型的泛化能力和应用范围。未来的研究需要继续探索新的技术和方法，以实现CAD图纸识别技术的进一步优化和应用拓展。\n\n# 1.2.3 技术对比分析\n\n在技术对比分析中，图像识别和深度学习方法与传统图像处理技术在CAD图纸识别领域的应用展现出明显的差异与优势。深度学习在特征提取和模式识别方面具有显著的优势，为CAD图纸识别提供了新的技术路径。例如文献[20]通过在跳层结构中引入短连接，显著提高了显著目标检测的性能，而文献[21]则通过利用更丰富的卷积特征，提升了边缘检测的准确性。这些进步不仅优化了模型性能，也为处理复杂CAD图纸数据提供了支持。\n\n然而，深度学习方法在处理大规模 CAD 图纸数据时可能遇到性能瓶颈，且相比于传统技术，它们通常需要更多的计算资源和训练时间。尽管如此文献[22]提供了一种将矢量绘图转换为产品模型的方法，对于自动化BIM的创建具有重要意义，尽管处理非标准CAD图纸时可能存在挑战。国际研究在深度学习模型的创新和优化方面较为成熟，如使用全卷积网络（Fully Convolutional Network，FCN）进行语义分割以及结合OCR技术进行信息提取，体现了在大规模数据集处理和三维模型重建方面的先进技术。与此同时，国内研究更注重于解决特定问题和实际应用的结合，如文献[23]和文献[24]展现了深度学习在实际应用中的努力，特别是在从建筑图纸中提取三维模型方面。\n\n通过对比国内外的研究，可以看到国外研究在理论探索方面较为成熟，而国内则在技术应用和实践方面取得了显著进展。例如文献[25]描述了一个包含图形识别算法、2D和3D建模过程以及用户界面的完整建筑图纸分析系统，展示了国内研究在系统集成和应用方面的特色。\n\n综合来看，图像识别和深度学习技术在 CAD 图纸识别领域显示出强大的潜力，但也面临着性能和实用性的挑战。未来的研究应该着重于优化深度学习模型的性能，以便更好地适应大规模和非标准 CAD 图纸的处理需求，同时也需要在国内外具有不同侧重点的研究之间寻找技术互补和融合的机会，以推动整个领域的发展。\n\n# 1.2.4 研究存在问题\n\n在 CAD 建筑图纸识别领域，尽管已有研究取得了一定的成果，但仍存在很多问题。这些问题涉及图纸中的噪声处理、信息的不完整性、模型的泛化能力、实时高效识别流程的实现、以及高质量 CAD 图纸数据集的缺乏等方面。图像识别和深度学习技术虽然在此领域已取得显著进展，但如何有效处理遮挡、模糊、尺度变化、非标准图纸识别、以及如何提高在不同视角下的鲁棒性等问题仍待解决。\n\n特别是研究如何从数据中自动识别元素特征，这对于理解复杂的建筑结构是至关重要的[26]。然而将这些特征有效地应用于CAD图纸的自动识别和理解，以及实现与CAD软件中现有数据结构的无缝集成和互操作性，依然面临技术挑战。另一方面，文献[27]提出的将矢量绘图转换为产品模型的方法，对于自动化BIM的创建具有重要意义，但其在处理大规模和复杂图纸时的效率和准确性需进一步研究。\n\n此外文献[28]的工作虽然在墙分割和对象检测方面取得了进展，但对于复杂建筑结构的识别和建模，尤其是非直线墙体的识别，仍然是一个挑战。Kalervo等人[29]提出的CubiCasa5K数据集为地板平面图分析提供了大规模样本，但如何将\n\n这些数据有效地应用于实际的建筑图纸识别和三维建模仍待解决。同时 Seo 等人[30]展示了语义分割在建筑图纸元素推断中的应用，但如何将这些技术应用于实际的建筑设计和施工流程，以及与其他建筑信息系统的集成仍需深入研究。虽然文献[31]和文献[32]在文本检测和公寓结构估计方面取得了成果，但在处理具有复杂背景和多样化风格的建筑图纸时，面临的挑战依旧存在。如何将识别结果与 BIM 等先进技术相结合，实现更高层次的建筑信息集成和应用，也是一个值得探索的领域。\n\n综上所述，未来的研究需要在提高识别准确性、处理复杂图纸、模型泛化能力、与现有建筑技术融合、以及图像识别和深度学习模型的可解释性和泛化能力等方面进行深入探索。这些问题的解决将对数字孪生的数字化和自动化产生深远影响，推动数字孪生中模型建立技术的发展和应用。\n\n# 1.3 本论文主要工作\n\n尽管国内外在将图像识别和深度学习技术融入于CAD图纸分析方面取得了一定的成果，但仍有许多问题和挑战。本研究的目标是在这一领域取得进一步的突破，为数字化转型提供更加有效的技术支持。通过深入研究和创新，希望能够克服现有的限制，推动这一领域的发展，为数字孪生的未来带来更多的可能性，本论文的主要研究内容有：\n\n1)深入了解学习有关CAD建筑图纸元素识别的方法，调研文献结果，总结国内外对于CAD建筑图纸元素识别的技术优势，并阐述其中遇到挑战和解决方案。  \n2)采用了一个包含15663张CAD建筑设计图纸的高分辨率数据集[33]，保证了识别元素的多样性和准确性，并且精细地标注不同类型的建筑元素。  \n3)基于图像识别方法，构建一个使用图像预处理、边缘检测和直线检测等技术的CAD建筑元素识别系统，详细描述了基于开源计算机视觉库（Open Source Computer Vision Library，OpenCV）的算法如何自动检测和识别建筑图纸中的双线墙、多线墙、实心墙和窗户。此外系统还引入了阈值设定等方法，在双线墙识别的基础上，实现了实心墙和多线墙识别从无到有的拓展，以及最终建筑元素的位置和属性输出。  \n4)本研究针对CAD建筑图纸中的门识别问题，提出了一种基于优化的YOLOv8算法，旨在自动化识别建筑平面图纸中的门元素。考虑到传统门建模方法的局限性，包括大量人工干预的需求、对专业知识和技术的依赖，以及建筑图纸质量和复杂性的限制，采取参数优化、应用层图片预处理等策略，显著提高了YOLOv8模型的识别准确度和处理效率。实验结果表明，通过这些优化措施，模型的识别准确率显著提升，证明了该方法在自动化提取建筑平面图中门信息方面的有效性和实用性。\n\n# 1.4 论文组织结构\n\n本文一共分成五个章节，各章节的内容概要如下：\n\n1）第一章节从宏观角度出发，阐述了数字化转型对建筑行业带来的影响，以及图像识别和深度学习技术在自动化CAD图纸分析中的应用及其面临的挑战。并且还概述了本研究的主要工作内容，包括国内外的研究进展对比，突出了技术研究的空白和挑战，以及本研究旨在解决的问题。  \n2）第二章节深入到技术层面，细致讨论了CAD图纸预处理、数据集构建、OpenCV和YOLOv8算法的应用，特别强调了这些技术在处理建筑图纸中的具体实践。此外本章还对从CAD图纸到3D模型转换过程中遇到的问题进行了系统性的分析，并提出了一系列针对性的优化方向，旨在提高转换效率和模型质量。  \n3）第三章节深入探讨了利用OpenCV实现CAD建筑图纸中关键元素：双线墙、多线墙、实心墙、和窗户的自动识别与分析。本章节详细描述了识别算法、关键数据结构定义、图像处理方法、及识别策略，并且展示了图像处理技术在自动化建筑设计分析领域的实用性和高效性。  \n4）第四章节专注于探索和优化YOLOv8算法，以提高在复杂CAD建筑图纸中自动检测门的准确性和效率。此外本章内容涵盖了从数据收集、预处理，到参数优化，再到模型应用层的优化，最后通过实验设计和结果分析，全面展示了改进后的YOLOv8模型在门检测任务上的性能表现。  \n5）第五章节主要是对本论文的研究进行了系统总结，归纳了本论文的研究成果，对遇到问题进行分析并提出未来改进的思路和方向。\n\n# 第二章 相关理论与方法\n\n# 2.1 CAD建筑图纸预处理和栅格化\n\nCAD图纸预处理和栅格化是将复杂大型建筑图纸转换为栅格图像的关键过程，以便在计算机上显示、编辑和处理。这个过程通常包括两个主要阶段：人工预处理和栅格化处理。\n\n在人工预处理阶段，CAD建筑图纸中包含了建筑设计的详细信息，包括墙体、门窗、家具等各个元素。由于CAD文件的复杂性和精细性，为了优化后续栅格化处理，需要进行人工预处理以减少图像中的噪声和不必要的元素。在人工预处理阶段需要消除图纸中的噪音（图纸中存在的不必要、多余的线条、图形或小的构件）。这些噪音元素可能是绘图过程中的错误或其他无关信息，消除它们有助于简化图纸并提高后续栅格化的准确性。通过利用CAD软件工具，如擦除和修剪功能，手动删除这些噪音元素，保留主要的建筑结构。预处理后的图纸更加凸显建筑的基本轮廓和要素，使得栅格化后的图像更加清晰和易于理解，如图2-1所示。\n\n![](images/27e8684b1dbd542454148dcdee61feb4560d17639e7bcd2791e4983db00a42d9.jpg)  \n图2-1 人工处理前后对比，左图为处理前，右图为处理后[33]\n\n![](images/b2507b0ee8b458c440abe00d880b3583da562b6b663b5ba4ae040061822e6a06.jpg)\n\n在完成人工预处理后，需要将CAD矢量图形转换为像素化的栅格图像。为了保证直线、圆弧等简单图形元素的准确性，以及复杂曲线和多边形的平滑性，本模型将栅格图像分辨率调整为为  $9599 \\times 7199$\n\n# 2.2 数据集概述\n\n在数字孪生中建筑模型的设计和规划方面，CAD建筑图纸是不可或缺的重要元素，这些图纸不仅包含了丰富的信息，而且对项目的顺利进行和有效管理至关重要。在本研究中使用了一个包含15663张原始CAD建筑图纸的数据集[33]。这些图纸涵盖了许多现代建筑设计的特点和细节。为了适应图像识别和YOLOv8模型技术的需求，本模型首先将所有CAD图纸转换成了高分辨率的  $9599\\times$\n\n7199像素的JPEG格式图像。采用高分辨率图像的目的是保留原始图纸的细节和质量，同时提供足够的信息和特征，以支持后续的识别任务。\n\n在自动建模中，墙体、窗户和门的识别对于提取和分析建筑信息具有重要意义。因此本研究采用了图像识别技术来识别这些基本的墙体和窗户元素。在墙体元素识别方面，考虑到建筑图纸的复杂性和最终图像分析的效果评估、参数调优、算法比较和验证，除了剪切了大量双线墙图像用于测试外，还剪切了实心墙和多线墙图像。实心墙通常用于将一个大空间分隔成不同的功能区域，而多线墙体常用于标示柱子或墙角的位置，具有一定的特殊性。此外还剪切了大量窗户图纸用于评估和测试，如图2-2和图2-3所示。\n\n![](images/d08fb0da8cddd465bd3da7a03d16892c291170da0fe55c7b0f92d42980e6b2ab.jpg)  \n图2-2左边为双线墙，右边为实心墙\n\n![](images/c1993cad0b6180b20955e2003d09a4dd65639793021d71b5e8d8a243301272bd.jpg)\n\n![](images/8083678f8ab2182931af1375e4002a65524f06747abd3dcf4fb3c4567f240730.jpg)  \n图2-3 左边为多线墙，右边为窗户\n\n![](images/7d091960a5225a78a37f7cc328f820456fbbe127b6d8b38607bbb3a6d98c1f37.jpg)\n\n对于门的识别，本研究采用了优化后的YOLO v8深度学习模型。门的标注采用了一系列包含门位置和类别信息的坐标，并记录在TXT文本中。YOLO v8是一种流行的目标检测算法，以其在图像中同时进行目标检测和分类的能力而闻名。然而由于原始图纸尺寸较大，直接将这些图纸输入到YOLO v8模型进行训练并不实际，这不仅会导致计算资源的巨大浪费，而且还会降低模型的训练效率。因此在进行模型训练之前，采用了图像切割技术，将原始图纸切割成多个  $640 \\times 640$  像素的小图像。这种方法不仅可以减少图像的尺寸，从而加快模型的训练速度，而且还能保留足够的图像细节和信息，以确保识别任务的准确性。\n\n为了更有效地训练和评估模型，将整个门图像数据集划分为比例为6:2:2的训练集、测试集和验证集。训练集用于模型训练，而测试集和验证集则用于评估模型的泛化能力和准确性。模型的性能通过精确度和召回率等指标进行衡量，并根据需要对模型进行调整和优化。\n\n# 2.3 OpenCV相关理论\n\n# 2.3.1 Canny边缘检测概述\n\nCanny边缘检测算法是John Canny在1986年提出的一种多级边缘检测技术，它旨在从图像中准确、清晰地检测到边缘[34]。Canny边缘检测算法的执行步骤主要包括：\n\n1）噪声降低：使用高斯滤波器平滑图像消除噪声。高斯滤波器的核心是高斯函数：\n\n$$\nG (x, y) = \\frac {1}{2 \\pi \\sigma^ {2}} e ^ {- \\frac {x ^ {2} + y ^ {2}}{2 \\sigma^ {2}}} \\tag {2-1}\n$$\n\n其中， $\\sigma$  是标准差，控制滤波器的宽度。\n\n2）寻找图像梯度强度：滤波后使用Sobel算子计算水平和垂直方向的梯度强度和方向。梯度的大小给出了边缘响应的强度，而梯度的方向给出了边缘的方向。  \n3）非极大值抑制：该步骤旨在“瘦化”边缘。对图像的每个像素，算法只保留梯度方向上局部最大梯度值的像素，其他非最大的梯度值被置为0，这样可以去除非边缘的像素。  \n4）滞后阈值：最后通过设置两个阈值：低阈值和高阈值，来确定真实的边缘。高于高阈值的像素被认为是强边缘，低于低阈值的被排除，介于两者之间的被视为弱边缘，仅当连接到强边缘时才被保留。\n\n# 2.3.2 形态学操作概述\n\n形态学操作是基于图像形状的一组图像处理操作，主要用于图像前处理或后处理中。这些操作包括膨胀、腐蚀、开运算和闭运算等[35]。\n\n1）腐蚀：用于消除边界点，使边界向内缩小，可以移除小且无意义的对象。基本操作是将结构元素与图像进行卷积，只保留结构元素内所有元素都是图像一部分的中心元素。  \n2）膨胀：与腐蚀相反，用于填补对象内的小孔，使边界向外扩张。在膨胀操作中，只要结构元素与图像的交集非空，就将结构元素的中心元素加入图像。  \n3）开运算和闭运算：开运算是先腐蚀后膨胀的过程，用于移除小对象或分离对象。闭运算是先膨胀后腐蚀的过程，用于填补对象内的小孔或连接近邻对象。\n\n# 2.3.3 矩形检测概述\n\n矩形检测涉及识别图像中的矩形形状和结构。矩形检测通过查找轮廓和最小面积算法实现[36]。首先将图像二值化，然后计算图像的轮廓。对每个轮廓使用CV函数计算其最小面积边界矩形。这个矩形可能有旋转，因此并非所有边都与图像轴平行。\n\n# 2.3.4 直线检测概述\n\n直线检测是一种使用霍夫变换（Hough Transform）来识别图像中特定形状的技术。对于直线检测，霍夫变换将图像空间中的每个点映射到霍夫空间中的一条曲线上，并查找这个空间中的交点，交点表示图像空间中的直线[37]。直线方程为：\n\n$$\n\\mathbf {y} = m \\mathbf {x} + \\mathbf {b} \\tag {2-2}\n$$\n\n其中  $m$  是直线的斜率， $b$  是直线的截距。\n\n# 2.3.5 图像分割概述\n\n在过去的几十年里，图像分割技术经历了从经典方法到基于深度学习的方法的演变。经典方法包括基于阈值、区域生长、边缘检测等技术，而基于深度学习的方法则利用了卷积神经网络（Convolutional Neural Network，CNN）的强大能力来实现更为复杂和精细的分割任务。文献[38]提供了对经典图像分割技术的综述，讨论了滤波技术、统计模型、神经-模糊-遗传范式、小波分析等多种方法，并对它们在图像预处理和分割方面的应用进行了分析。这些方法在处理特定类型的图像，如彩色图像，时显示出了它们的有效性和适用性。并且随着深度学习技术的发展，图像分割领域出现了许多基于深度学习的新方法[39]。这些深度学习方法因其在处理复杂场景和高度变化的图像中显示出的卓越性能而受到广泛关注。\n\n在图像分割的预处理阶段有多种预处理方法，包括点操作、线操作、区域操作等[40]。预处理步骤可以显著提高分割算法的性能，通过改善图像质量和增强图像特征，使得后续的分割任务更加准确和高效。文献[41]提出了一种结合空间和频率特征提取的预处理方法，即使在没有预处理的情况下也能提供良好的识别结果，从而强调了预处理在图像分割和识别任务中的重要性。\n\n图像分割技术的发展涵盖了从经典方法到深度学习方法的广泛范围。经典方法依赖于传统图像处理技术，而深度学习方法则利用了人工智能的进步来处理更为复杂的图像。无论是在预处理阶段还是在分割算法的选择上，不同的方法和技术都有其特定的应用场景和优势。通过结合这些技术，可以在各种不同的应用中实现高效和准确的图像分割。\n\n# 2.4YOLOv8相关理论\n\n# 2.4.1 YOLO v8概述\n\nYOLO算法自从其首个版本发布以来，就在实时目标检测领域内引起了广泛关注。随着技术的不断进步，YOLO已经发展到第八版YOLOv8，表现出更高的准确性和效率。YOLOv8通过深度学习的强大功能，能够在单次前向传播中同时预测多个目标的类别和位置，极大地提高了目标检测的速度和准确度。\n\n在文献[42]中，研究人员展示了YOLO v8在图像领域的应用，特别是在识别油菜荚及计算其属性方面，以提高产量估计的准确性。该研究利用了YOLO v8的高精确率和速度，实现了超过  $90\\%$  的图像分割精度。在文献[43]中，提出了一种基于YOLO v8的改进火灾检测方法，适用于智能城市环境。该方法通过YOLO v8实现实时火灾检测，展现了在精度和召回率方面的优势，证明了YOLO v8在紧急情况响应系统中的实用性。在无人机图像处理方面，在文献[44]中，通过引入轻量化技术和结构优化，提出了一种轻量级YOLO v8（LW-YOLO v8）模型。该模型特别适用于无人机航拍图像中小目标的识别，提高了小目标检测的准确性和效率。\n\n针对电力系统，文献[45]介绍了一种增强版YOLOv8算法，专门用于检测高压输电线路的损伤。通过算法优化，实现了高精度和高速度的损伤检测，为电力系统的安全运行提供了技术支持。而在文献[46]中探讨了YOLOv8在医学图像分析中的应用，尤其是在无线视频胶囊内窥镜图像中进行息肉样病变分割。通过YOLOv8，研究提供了一种高效的息肉检测方法，有助于提早发现和预防胃肠道疾病。\n\nYOLO v8 的发展和应用展示了深度学习在多个领域的广泛适用性和强大能力，从农业、城市安全、无人机监控、电力系统维护到医学图像分析，都取得了显著的成果。这些研究不仅验证了YOLO v8 在各个领域的实用价值，也推动了其在未来研究和实际应用中的进一步开发和优化。\n\n# 2.4.2 YOLO v8 网络模型结构图\n\nYOLOv8 代表了 YOLO 系列在对象检测领域的最新进展，继承并发展了 YOLO 系列的核心设计，即通过单个神经网络实时预测对象的边界框和类别概率。YOLO v8 的网络模型结构通过精心设计，提升了检测效率和准确性，特别是在处理不同尺度对象的能力上表现十分稳定。该模型大致分为四个主要部分：输入层、骨干网、颈部以及头部，每个部分针对性地解决对象检测过程中的特定挑战。\n\n1）输入层的主要任务是对进入网络的图像数据进行预处理，包括调整图像尺寸、归一化等，以确保图像数据适合后续网络层的处理需求。这一步骤是确保模型处理效率和性能的关键起点。\n\n2)骨干网部分是YOLOv8结构的核心，基于交叉阶段部分网络（Cross Stage Partial Network，CSPNet）架构，旨在从输入图像中有效提取特征。CSPNet通过引入部分跨阶段连接，既增加了网络的学习能力，又减少了计算复杂性，从而在提高模型性能的同时保持了较低的计算成本。  \n3）颈部部分采用了特征金字塔网络（Feature Pyramid Networks，FPN）和路径聚合网络（Path Aggregation Network，PANet）结构，这两种结构共同工作，以生成不同尺度的特征图。这一设计使得YOLO v8能够更好地处理尺度变化大的对象检测问题，尤其是改善了对小对象的检测能力。通过融合不同层次的特征信息，模型能够从多尺度特征中学习到更丰富的语义信息，从而提高检测的准确性和鲁棒性。\n\n![](images/0336d31ccc53001638fd05a050d2d3aee6ce02640a2d1ef0980177e48b6fa7df.jpg)  \n图2-4YOLOv8模型结构[47]\n\n4) 头部是YOLOv8的决策层，负责根据颈部提供的特征图生成最终的边界框、对象置信度和类别概率。这一部分的设计直接影响到模型的检测性能，包括检测的精确度和召回率。\n\nYOLO v8 的这一系列设计使其在对象检测领域表现卓越，无论是在速度还是在准确性方面都实现了显著的提升。通过这种高效的网络架构设计，YOLO v8 适用于广泛的应用场景，从图片检测到自动驾驶辅助系统，都能够提供准确且快速的对象检测能力。图2-4展示的YOLOv8模型结构图，直观展现了其网络架构的复杂性和高效性。\n\n# 2.4.3 YOLO v8 网络模型结构设计\n\nYOLOv8 的网络模型结构设计体现了对高效处理能力和广泛适用性的追求，旨在提供一个既快速又准确的对象检测框架。该设计通过选取的结构组件和创新技术，使得YOLO v8 在处理各种尺寸的对象时表现出色，尤其是在检测小对象方面的能力得到了显著增强。\n\nYOLOv8 的网络模型结构核心设计理念之一是利用交叉阶段部分（Cross Stage Partial networks，CSP）结构来构建骨干网，这一结构的引入显著降低了模型的计算复杂度。CSP 结构通过分割特征图并在网络的不同阶段中独立处理，之后再将它们合并，这样不仅减少了计算量，还保证了信息在网络中的高效流动。这种设计使得 YOLO v8 能够在不牺牲性能的情况下，快速处理大量数据，极大提升了检测任务的实时性。\n\n为了进一步提升模型对不同尺度对象的检测能力，YOLOv8采用了细粒度的特征融合策略，尤其是在颈部通过FPN和PANet结构的结合使用。这些结构协同工作，有效地构建了一个多尺度的特征图融合框架，通过这种方式，YOLO v8可以更好地捕捉到小对象的特征，同时保留对大对象的高识别能力。此外模型能够从细微的特征变化中学习到丰富的上下文信息，进一步增强了检测的准确性和鲁棒性。\n\n在头部设计中，YOLOv8引入了无锚点（Anchor-Free）模块，这标志着对传统锚框方法的进一步优化。通过这种模块，模型直接在特征图上预测边界框的中心点和尺寸，而无需预定义的锚框。这种设计简化了预测过程，减少了模型对锚框尺寸选择的依赖，使得检测过程更加灵活和准确。\n\n整合这些创新技术和结构设计，YOLOv8不仅在速度和效率上实现了显著的提升，而且在提高模型的通用性方面也取得了重要进展。无论是在复杂的实时视频监控、人流统计还是在自动驾驶系统中的应用，YOLOv8都能提供卓越的检测性能。\n\n图2-5展示了骨干网络和颈部的C2f模块，这一部分展示了如何通过CSP结构和特征融合策略来优化信息流和处理效率。图2-6则聚焦于头部的Anchor-Free模块，展现了YOLOv8如何通过创新的预测机制来提高模型的准确性和灵活性。这些设计细节共同构成了YOLOv8强大的检测框架，为实现高效且准确的对象检测提供了坚实的基础。\n\n![](images/9b70a7d0c6e83e543ee259928ce70a18c67c6595237fbfce165b6d7c143e80f1.jpg)  \n图2-5 骨干网络和颈部的C2f模块[47]\n\n![](images/5e202b86eff53e2a29b44cb2db174dceae5db4690ff7abb02a2bec57eda313ad.jpg)  \n图2-6 头部的Anchor-Free模块[47]\n\n# 2.4.4 YOLO v8 检测技术的突破与应用前景\n\nYOLO v8 作为对象检测技术的最新进展，跟前几代YOLO 模型相比，模型不仅优化了网络结构、训练策略和不同尺度对象检测，还实现了在保持较高帧率的同时显著提升检测准确率的目标。这一成就不仅让YOLO v8 在常见的对象检测任务中表现出色，也使其能够适应更为复杂的场景和多样的应用需求，如建筑元素识别等。\n\n在YOLOv8的损失函数设计中，通过使用边界框回归损失、置信度损失和分类损失的综合优化，使得YOLOv8能够实现高速且准确的目标检测，同时保持了模型的高效性和准确性。此外YOLOv8采用了一系列先进的训练策略，包括数据增强、批量归一化、权重衰减和学习率调整等，这些策略共同作用，有助于模型在多种任务和场景中实现稳健的检测性能，减少过拟合的风险。\n\n# 2.5 CAD建筑图纸图像问题\n\n# 2.5.1 建筑图纸问题概述\n\n随着技术的快速进步和发展，CAD软件已成为设计和绘制建筑图纸的标准工具。这些先进的软件解决方案极大地提高了工作效率，同时也提升了设计作品的质量。CAD软件允许专业人员以前所未有的精确度创建复杂的设计方案，从而在建筑设计领域引领了一场革命。然而在这一进步的背后，当尝试将2D的\n\nCAD图纸转换为精确的3D模型以便更好地可视化和数字化建模时，遇到了一系列挑战。\n\n1）建筑元素识别准确性。自动识别CAD图纸中的元素（如门、窗、墙等）是建立3D模型的一个关键步骤。这一过程的复杂性在于不同设计者可能遵循不同的绘图标准和习惯，使得算法难以一概而论地正确识别所有元素。例如，一个设计者可能使用特定的线型来表示窗户，而另一个设计者可能使用完全不同的符号。此外设计中的注释和维度标记也可能干扰自动识别过程，导致算法误解设计意图或完全漏检关键元素。解决这一问题需要开发更为先进的识别算法，这些算法能够学习并适应多样的设计习惯，并通过上下文线索来提高识别的准确性。  \n2）几何信息的转换。将2D图纸中的元素转换为3D模型涉及到复杂的几何计算，确保转换后的元素在三维空间中保持其准确的尺寸、位置和方向。这一过程的难点在于处理2D图纸与3D空间表达之间的固有差异。2D图纸通常只提供有限的视角，而要从这些平面视图中推断出完整的三维形状，就需要依赖于算法的智能解读和几何推导。这一转换过程中，处理不一致性和歧义成为重大挑战，因为同一2D图像可能对应于多个可能的3D结构。此外2D设计中的简化表示（如忽略小细节或使用符号代替实际结构）进一步增加了从2D到3D转换的复杂性。研究能够理解这些简化符号并准确地将它们转换为详细3D模型的算法，是实现高质量3D建模的关键。  \n3）细节丢失问题。细节丢失是从2D图纸转换到3D模型过程中的一个常见问题。在自动化转换过程中，某些设计的细微之处可能不被算法识别，导致在最终的3D模型中缺失重要信息。例如2D图纸上的轻微线条差异可能表示不同的建筑元素或结构特征，但这些细微差别在自动转换过程中可能会被忽略。此外一些特定的设计元素，如墙体线条数量细节或特殊填充结构，如果没有被准确识别和建模，也可能在转换过程中丢失。为了减少这种细节丢失，需要开发更加精细的识别技术和算法，这些技术能够捕捉和解释图纸中的细节信息，并将其准确地反映在3D模型中。  \n4）软件兼容性与集成。在多个不同的 CAD 和 3D 建模软件之间实现高效的数据交换和无缝转换，是建立精确 3D 模型的另一个挑战。软件间的兼容性问题可能导致从 2D 图纸到 3D 模型的转换过程中需要进行额外的手动调整，从而增加了工作量和错误发生的可能性。不同软件可能采用不同的文件格式，使得直接导入或导出设计信息变得复杂。即便是在支持相同文件格式的情况下，数据的解释方式也可能因软件而异，导致设计元素的属性或位置在转换过程中发生变化。解决这一问题的关键在于推动行业标准的统一和软件开发商之间的合作，以确保更高的互操作性和更流畅的数据集成。此外研究能够连接不同软件之间差异的中间件工具，也是提高工作效率和减少转换错误的有效途径。\n\n5）计算资源需求。3D模型的建立和渲染对计算资源的需求很高，尤其是在处理复杂的建筑项目时。详细的3D建模不仅需要大量的存储空间，而且在模型构建和渲染过程中需要进行大量的计算处理。对于大型项目，这可能超出了标准硬件的处理能力，导致建模过程缓慢，影响设计和审查的效率。此外随着项目的规模和复杂度增加，维护和更新3D模型也变得更加困难和耗时。采用云计算和分布式处理技术可以为3D建模提供必要的计算资源，并且通过在云端进行重计算和数据存储，可以显著提高处理速度和数据可管理性。同时优化3D建模软件的性能，减少对硬件资源的依赖，也是提高大型项目处理能力的关键策略。\n\n随着建筑设计向更高精度和更复杂结构发展，从CAD图纸到3D模型的转换面临着元素识别准确性、几何信息转换、细节丢失、软件兼容性与集成以及计算资源需求等挑战。通过研究更先进的技术和算法、推动行业标准的统一、采用云计算和优化软件性能，可以有效地克服这些挑战，进一步提升从2D到3D转换的效率和质量，为数字孪生中建筑模型设计提供更强大的支持。\n\n# 2.5.2 建筑图纸优化方向\n\n随着数字孪生领域的快速发展，将CAD建筑图纸转换成3D模型已成为数字孪生模型设计过程中不可或缺的一环。这一过程不仅可以提高设计的可视化水平，还能有效促进项目各方之间的沟通与协作。然而从2D图纸到3D模型的转换面临着多个技术挑战，包括元素识别准确性、几何信息的转换、细节的丢失、软件的兼容性与集成问题，以及对高计算资源的需求。针对这些问题，提出了几个潜在的优化方向，旨在提高转换过程的质量和效率。\n\n1）改进建筑元素识别技术。元素识别的准确性是从2D图纸转换到3D模型过程中的首要挑战。通过引入更先进的图像识别和机器学习算法，可以显著提高元素识别的准确率。这些算法可以从大量的设计数据中学习，从而识别出不同设计风格的建筑元素，如门、窗和墙等。此外深度学习技术可以使系统更好地理解图纸中的上下文信息，从而减少误识别和漏检的情况。通过持续训练和优化，这些技术能够适应多变的设计规范和绘图习惯，为数字孪生3D模型的建立提供坚实的基础。  \n2）标准化设计规范。设计和标注的标准化对于简化从2D图纸到3D模型的转换过程至关重要。推广和采用标准化的设计规范和标注方法可以有效减少歧义和误差，确保转换过程的一致性和可靠性。这不仅有助于改善自动识别算法的效率，还可以促进项目参与方之间的沟通，减少因理解差异而导致的错误。实施标准化设计规范还可以提升整个数字孪生领域的协同工作效率，推动行业朝着更加开放和互联的方向发展。\n\n3）增强软件互操作性。软件之间的兼容性问题长期以来一直是技术挑战之一。通过开发专门的插件或中间件，可以极大地提高不同CAD和3D建模软件之间的互操作性。这些工具可以帮助转换和传递不同软件系统中的数据，确保信息在传递过程中的准确性和完整性。例如，一些通用的数据交换格式，如工业基础类库(Industry Foundation Classes, IFC)标准，已被广泛接受并应用于多个软件平台之间，实现了高效的数据共享和沟通。进一步增强软件之间的互操作性不仅能简化设计流程，还能加速数字孪生项目的开发周期。  \n4）优化模型细节的保留。在从2D转换到3D的过程中保留尽可能多的设计细节对于最终模型的质量至关重要。研发高级算法，特别是在处理细节和小元素方面，可以确保这些重要的信息不会在转换过程中丢失。通过精细化建模技术和增强的视觉效果处理，可以使3D模型在视觉上更加真实和准确，更好地反映原始设计的意图。此外这些技术还能够支持更复杂的设计元素和结构，为建筑师和设计师提供更广阔的创作空间，进而提高数字孪生中信号覆盖预测的准确性。  \n5）利用云计算和分布式处理。随着3D模型在数字孪生设计和施工中的应用日益广泛，对计算资源的需求也不断增加。云计算和分布式处理技术提供了一种有效的解决方案，通过远程服务器群的强大计算能力，支持复杂3D模型的快速建立和渲染。这不仅可以减轻本地硬件的负担，还可以实现更高效的团队协作和数据共享。云平台上的3D建模和渲染服务使得即使是大规模和高复杂度的项目也能够得到及时和高质量的处理，进而提高数字孪生中信号覆盖预测的效率。\n\n通过采取上述优化方向，可以有效地提高从CAD建筑图纸到3D模型转换的质量和效率。这些技术和方法的进步将不仅有助于解决当前面临的挑战，还将推动数字孪生领域的发展，特别是在建筑模型设计和信号覆盖预测方面。随着技术的不断演进和行业标准的逐步统一，未来的数字孪生设计将更加精准、高效，同时也更具创新性和可持续性。\n\n# 2.6 本章小结\n\n本章节深入探讨了将CAD建筑图纸转换为3D模型的相关理论与方法，涵盖了从人工预处理与栅格化处理到数据集概述、图像处理技术、YOLOv8对象检测算法的应用，以及面临的挑战和优化方向。这一过程对于现代化建筑设计和规划来说至关重要，因为它不仅增强了设计的可视化表达，还提高了项目管理的效率和精确性。\n\n在预处理和栅格化处理方面，通过人工预处理消除噪声和不必要的元素，随后将CAD图纸转换为高分辨率的栅格图像，是确保后续处理阶段能高效运行的基础。这一步骤是优化图纸，使其更加适合计算机处理和图像识别技术的关键。数据集概述部分强调了使用现代CAD建筑图纸的重要性，并展示了如何利用高分辨率图像来支持复杂的图像识别和对象检测任务。图纸的详细信息和高质量的图像是实现精确识别和分类的基础。在图像处理技术方面，介绍了Canny边缘检测、形态学操作、矩形和直线检测等基本方法，这些方法为识别建筑图纸中的关键元素提供了支持。这些技术的应用，是分析复杂图纸的前提，为进一步的图像识别和模型构建打下了坚实的基础。\n\nYOLO v8 的引入是在建筑图纸图像识别方面的一大进步，特别是在门这个关键建筑元素的识别上。通过优化的网络结构和高效的训练策略，YOLO v8 实现了快速而准确的对象检测，展现了深度学习技术在建筑图纸处理上的强大潜力。\n\n在面临的挑战与优化方向部分深入讨论了将2D CAD图纸转换为3D模型的过程中遇到的问题，包括建筑元素的识别准确性、几何信息的转换、细节丢失、软件兼容性以及计算资源需求等。这些挑战揭示了转换过程的复杂性和多变性，同时也指出了未来发展的方向，包括改进识别技术、标准化设计规范、增强软件互操作性、优化模型细节保留和利用云计算等。\n\n通过深入分析和综合考虑这些相关理论与方法，本章节不仅提供了对当前技术和挑战的全面了解，还展望了3D模型在构建方面的发展趋势。随着技术的不断进步和行业标准的逐渐统一，预期未来数字孪生中建筑模型的设计和规划将变得更加精准、高效，同时也更具创新性和可持续性。\n\n# 第三章 基于 OpenCV 的 CAD 建筑图纸图像处理与分析\n\n# 3.1 双线墙识别\n\n# 3.1.1 算法概述\n\n双线墙识别算法，旨在自动检测和识别CAD建筑图像中的水泥墙体。该算法首先利用先进的图像预处理技术，如灰度转换、噪声去除和图像增强，以优化图像质量并为后续的边缘与直线检测步骤创造理想条件。接着通过应用Canny边缘检测和霍夫变换直线检测，精确地识别出图像中的墙体边缘及直线。这些检测到的直线经过过滤和匹配，仅当它们满足预设的长度、角度和间隔条件时，才被判定为双线墙。最终从匹配成功的双线墙中提取关键属性，包括长度、宽度和方向。\n\n此流程不仅极大提升了墙体检测的准确度，也显著增强了数字孪生中建筑模型设计和室内设计的自动化水平。此外它还突显了图像处理技术在数字孪生领域的应用潜力。\n\n# 3.1.2 算法工作流程\n\n双线墙识别算法的工作流程可以分为以下几个关键步骤：\n\n# 3.1.2.1 图像预处理\n\n图像预处理是图像处理和计算机视觉领域中的一个重要步骤，尤其是在复杂的图像识别任务中显得尤为重要。图像预处理可以提升复杂CAD建筑图像的质量，确保后续的算法能够在最佳条件下运作，从而提高整个系统的识别准确率和效率。在双线墙识别的背景下，预处理步骤尤为关键，因为算法需要识别图像中的微小细节来确定墙体的准确位置和属性。\n\n# 1）灰度值转换\n\n灰度转换是图像预处理中的第一步，它将彩色图像转换为灰度图像。彩色图像通常由红、绿、蓝三个颜色通道组成。而在CAD建筑图像处理任务中，色彩信息并不总是必要的，尤其是在涉及形状、边缘检测或者纹理识别的场景中。灰度图像通过合并彩色通道并减少颜色信息的复杂性，不仅可以减少计算负担，还能突出图像的结构特征。\n\n$$\nG (x, y) = 0. 2 9 9 R (x, y) + 0. 5 8 7 G (x, y) + 0. 1 1 4 B (x, y) \\tag {3-1}\n$$\n\n通过公式3-1描述了从彩色图像到灰度图像转换的标准过程，其中  $R$  ，  $G$  ，  $B$  分别代表红色、绿色、蓝色通道的像素值，而是计算得到的灰度值。这个公式不是随意确定的，而是基于CAD建筑图像中颜色取值数量决定的。通过这种方式，转换后的灰度图像在视觉上保持了与原彩色图像相似的亮度感知，为后续的图像处理提供了一个更加合适的基础。\n\n# 2）噪声去除\n\n噪声去除是图像预处理的一个关键环节，特别是在进行精细的如双线墙识别时，并且在实际建筑图纸识别中，CAD建筑图像往往会受到各种噪声的影响。噪声不仅降低了图像质量，也会对后续的图像处理步骤，尤其是边缘检测和特征提取等操作产生负面影响。因此噪声去除是保证图像处理任务成功进行的先决条件之一。\n\n高斯滤波器是一种广泛使用的噪声去除技术，其核心原理是对图像进行平滑处理，减少图像中的随机噪声。高斯滤波通过替换每个像素值为其邻域内像素值的加权平均来实现，权重由高斯函数确定。高斯函数的公式如下：\n\n$$\nG (x, y) = \\frac {1}{2 \\pi \\sigma^ {2}} e ^ {\\frac {x ^ {2} + y ^ {2}}{2 \\sigma^ {2}}} \\tag {3-2}\n$$\n\n其中  $x, y$  表示像素位置相对于滤波器中心的横纵坐标， $\\sigma$  是高斯分布的标准差，控制权重随距离变化的速率。较小的  $\\sigma$  值会使权重迅速降低，导致只有与中心像素非常接近的邻域像素对平均值有显著贡献，这样能够有效去除噪声同时保留图像边缘的清晰度。而较大的  $\\sigma$  值则会使更远的像素也能对平均值产生贡献，从而实现更强的平滑效果但可能模糊边缘。\n\n# 3）图像增强\n\n图像增强是另一个重要的预处理步骤，其目的是改善图像的视觉效果，使得图像中的重要特征如墙体边缘更加明显。直方图均衡化是一种常用的图像增强技术，它通过改变图像的直方图分布来提升图像的全局对比度。具体而言，直方图均衡化通过将原始图像的直方图映射到一个较宽的亮度范围上，从而使得图像的高亮区域变得更亮，暗区域变得更暗，增加了图像的动态范围，使得细节部分更加清晰可见。这一处理对于突出图像中的边缘和纹理特征非常有效，对于双线墙识别等需要精确边缘信息的任务尤为重要。\n\n通过以上两种技术的应用，图像中的噪声被有效去除，同时图像的对比度得到增强，为双线墙识别等后续的图像处理步骤奠定了坚实的基础。高斯滤波保证了图像的平滑性同时尽可能保留边缘信息，而直方图均衡化则进一步提升了图像的对比度和清晰度，使得算法能够更准确、更有效地识别出图像中的墙体边缘和特征。\n\n# 3.1.2.2 双线墙检测\n\n墙体检测阶段涉及边缘检测和直线检测算法, 常用的是 Canny 边缘检测和霍夫变换 (Hough Transform)。\n\n# 1）Canny边缘检测\n\nCanny边缘检测算法因其优秀的边缘检测性能和鲁棒性而被广泛使用。Canny边缘检测算法的核心思想是找到图像中亮度变化最显著的地方，即边缘，\n\n而且尽可能减少错误检测和位置偏差。Canny算法的具体步骤包括：\n\n应用高斯滤波平滑图像：首先使用高斯滤波器对图像进行平滑处理，以去除噪声。这一步骤是必要的，因为噪声可能导致边缘检测的假阳性，即错误地将噪声识别为边缘。\n\n计算图像的梯度幅度和方向：然后计算图像每个像素点的梯度幅度和方向。梯度幅度可以通过Sobel算子等边缘检测算子来求得，梯度方向则表明了边缘的走向。\n\n应用非极大值抑制(NMS)确定边缘的精确位置：非极大值抑制是一种细化技术，通过保留局部最大梯度幅度的像素点，而抑制其他非边缘像素点，从而实现边缘的细化。\n\n使用双阈值算法和边缘跟踪通过滞后来检测和连接边缘: 最后 Canny 算法采用双阈值技术来确定真正的边缘。较高的阈值用于检测明显的边缘, 较低的阈值用于追踪边缘的延续部分。通过这种方式, 算法能够有效地连接断裂的边缘, 提高检测结果的完整性。\n\n# 2）霍夫变换直线检测\n\n霍夫变换(Hough Transform)是另一种在图像分析中常用的技术，用于从图像中识别几何形状，特别是直线和圆。在墙体检测中，霍夫变换主要用于从Canny边缘检测算法的结果中识别直线，为双线墙匹配提供基础。\n\n霍夫变换的基本原理是将图像空间中的点映射到参数空间中的曲线，通过寻找参数空间中的交点来识别特定的几何形状。对于直线检测，霍夫变换使用极坐标表示法来描述直线，即\n\n$$\n\\rho = x \\cos \\theta + y \\sin \\theta \\tag {3-3}\n$$\n\n其中  $\\rho$  是直线到原点的距离， $\\theta$  是直线的方向角。通过这种表示，每个图像中的点都对应于参数空间中的一条正弦曲线，而共线的点集则在参数空间中交于一点。通过识别这些交点，可以确定图像中直线的位置和方向。\n\n霍夫变换的优势在于其对图像中的噪声和不完整特征的高容忍度，使其成为直线检测的理想选择。通过适当调整阈值参数，霍夫变换可以从一组杂乱无章的边缘点中准确地识别出直线，为双线墙识别提供了可靠的基础。\n\n综上所述，墙体检测是双线墙识别中的一个复杂而关键的步骤。通过结合Canny边缘检测和霍夫变换直线检测两种算法，可以有效地从图像中识别出墙体的边缘和直线，为双线墙的进一步分析和识别奠定坚实的基础。这些技术的应用不仅提高了墙体检测的准确度，也极大地扩展了图像处理在数字孪生领域中的应用范围。\n\n# 3.1.2.3 双线墙过滤\n\n在墙体检测阶段之后，得到了大量的边缘信息，其中既包括代表墙体的边缘，也包括其他对象产生的边缘。为了准确地识别出双线墙，必须从这些边缘中筛选出真正代表墙体的边缘。这一过程称为墙体过滤，它是确保双线墙识别准确性的关键步骤。\n\n墙体过滤通常涉及以下几个筛选准则：\n\n直线长度：一条代表墙体的直线通常会比较长。因此可以根据直线的长度来过滤边缘，移除那些长度明显短于一定阈值的直线。这个阈值可以根据具体的应用场景和图像尺度进行调整。\n\n角度：在大多数室内环境中，墙体的边缘与图像的水平或垂直方向大致平行。因此通过检测直线的角度，可以过滤掉那些与水平或垂直方向角度偏差较大的直线。\n\n间隔：在进行双线墙匹配之前，需要识别出可能成对出现的直线。这需要检查直线间的间隔是否一致。直线间的一致间隔是判断它们是否属于同一面墙的重要依据。\n\n# 3.1.2.4 双线墙匹配\n\n在完成了墙体过滤之后，下一步是双线墙匹配，这是双线墙识别过程中至关重要的一步。双线墙匹配的目的是从过滤后的直线中找出那些成对且平行的直线，这些直线代表了室内环境中的墙体边缘。实现这一目标需要对直线间的空间关系进行细致的分析，包括它们之间的角度差和距离。\n\n双线墙匹配过程通常包括以下几个步骤：\n\n计算角度差：首先需要计算每对直线之间的角度差。在理想情况下，成对的墙体边缘应该是完全平行的，即它们的角度差应该为0。然而在实际应用中，由于图像采集的角度、墙体的微小倾斜等因素，完全平行的情况较为罕见。因此通常会设定一个较小的角度差阈值，允许一定程度的偏差。\n\n测量间距：对于角度差满足条件的直线对，进一步测量它们之间的间距。双线墙的特征之一是两条边缘的间距相对一致。通过设定间距的阈值，可以过滤掉那些间距过大或过小的直线对。\n\n# 3.1.2.5 双线墙属性提取\n\n成功匹配双线墙后，接下来的关键步骤是提取这些墙体的属性，包括它们的长度、宽度、方向等信息。这一过程不仅是双线墙识别任务的最终目的，也为后续的建模提供了重要数据。\n\n长度和宽度的计算，双线墙的长度可以通过计算两个边界直线的共同端点之间的距离来确定。具体而言，如果将两条匹配的直线视为几何线段，则这些线段的长度可以通过简单的欧几里得距离公式计算得出。假设线段的两个端点分别是  $P_{1}(x_{1},y_{1})$  和  $P_{2}(x_{2},y_{2})$  ，则线段L可以表示为：\n\n$$\nL = \\sqrt {\\left(x _ {2} - x _ {I}\\right) ^ {2} + \\left(y _ {2} - y _ {I}\\right) ^ {2}} \\tag {3-4}\n$$\n\n对于墙体的宽度，由于已经知道了双线墙的间距，这个间距实际上就代表了墙体的厚度或宽度，因此可以直接采用双线间的测量值。\n\n双线墙的方向可以通过计算直线的角度来确定。在极坐标系中，直线的方向角描述了直线相对于水平方向的倾斜程度，这个角度可以直接从霍夫变换的结果中获得。对于模型建立和建筑元素分析来说，了解墙体的精确方向非常重要，它有助于确定门窗位置等。\n\n# 3.1.3 数据输出\n\n双线墙识别算法主要用于检测和分析室内建筑中双线墙元素，其输出对于建筑元素分析和模型建立至关重要。识别结果信息应当包括双线墙的位置和尺寸，这些详细信息对于建筑模型建立十分重要。通过将检测到的双线墙信息转换为JSON数据文件，可以详细描述一个几何或建筑配置，如图3-1所示。这种数据结构提供了双线墙布局的详细信息。\n\n![](images/f3128b152b4ecda85080c4fd1f370ca58345a6a3c582475ebfddfed16c7f8f53.jpg)  \n图3-1 双线墙数据输出数据\n\nDoubleLineWallNum（双线墙数）：值为586，表示配置中包含的“双线墙”的数量。\n\nDoubleLineWalls（双线墙）：此数组通过一系列嵌套的像素坐标点定义了每一面墙，说明了双线墙的布局。坐标的变化揭示了墙的尺寸或长度，提供了关于空间结构的具体信息。\n\n其中第一组数据654和-3137代表墙体的四个顶点中左上顶点的像素位置，然后通过顺时针方向表达出四个点的像素位置，准确表达出墙体的位置。\n\n结合对数据结构的理解和不同的展示方式，这种灵活的处理方法使得上述JSON数据结构不仅能够详细描述空间配置，还能够适应各种不同的应用需求，从技术分析到用户展示，提供了一种全面而有效的数据利用途径。\n\n# 3.1.4 实验结果与分析\n\n在双线墙识别研究中，设计了一种高效的双线墙识别算法，使其能自动化地从CAD建筑图纸中检测和识别双线墙的位置和尺寸属性。此算法的开发是基于对现有图像处理技术和计算机视觉理论的深入理解，特别是对于图像预处理、边缘检测、直线识别、以及高级特征提取技术的应用。实验的目的是验证双线墙识别算法的有效性和准确性，以及其在实际建筑图纸分析和室内建模中的应用潜力。\n\n实验过程包括从原始 CAD 图像的预处理开始，经过一系列复杂的图像处理步骤，最终实现双线墙的精确识别。实验的核心部分是双线墙过滤和匹配过程，通过这一步骤，能够从大量的边缘信息中筛选出真正代表墙体的边缘，并将它们配对为代表双线墙的直线对。这一过程不仅考虑了直线的长度和角度，还涉及直线间的空间关系，如它们之间的间隔一致性。通过细致的分析和筛选，算法成功地识别出了代表双线墙的直线对。\n\n为了直观展示算法的效果，选取了一张原始的CAD建筑图纸作为实验对象。该图纸详细描述了一个室内环境的布局，包含了墙体、门和窗等多种元素。在经过识别算法处理之后，生成了一张新的图像，其中成功识别的双线墙以红线标出。这种视觉对比不仅展示了算法在实际应用中的准确性和有效性，还直观地证明了算法对于复杂室内环境中双线墙识别的能力，如图3-2所示。\n\n![](images/a7184fdc8ad90242dcac3ecf378fe58ae97ab8ac6018192d86a5f8d6211c7445.jpg)  \n图3-2左图为栅格化后原始图像图，右图为双线墙识别结果图像\n\n![](images/2a9090a42b96964fe4b5956a93791dae8ff63e46858e69a744ea5bf9bebf9213.jpg)\n\n实验结果表明：在上述CAD建筑图纸中，共有192个双线墙元素。通过应用双线墙识别算法，成功识别并标记了其中的170个，实现了  $88.6\\%$  的识别准确度。这一结果不仅表明了算法的高效率，也验证了其在处理复杂建筑图纸时的可靠性。此外算法的输出结果以JSON格式呈现，为每个识别出的双线墙提供了位置属性信息。这些数据的精确性和详细性为后续数字孪生中建筑模型设计提供了数据基础。\n\n# 3.2 多线墙识别\n\n# 3.2.1 算法概述\n\n在当今数字化时代，图像处理和计算机视觉技术已经成为自动化系统和智能分析的核心。本小节详细介绍了一个创新的多线墙识别系统，它集成了OpenCV库和定制化的数据处理逻辑，专门设计用于高效地识别和分析图像中的砖墙结构。\n\n# 3.2.2 算法工作流程\n\n在图像预处理阶段，处理多线墙的方法大致与双线墙的处理方式相似，包括通过灰度值转换、噪声去除和图像增强来提高识别准确度。然而由于多线墙比双线墙在墙体线条上更加复杂，检测方法需要具备灵活性、扩展性和高效性。这些原则是多线墙识别方法架构设计的核心，从而确保系统能够高效处理复杂的多线墙识别任务。\n\n# 3.2.2.1 数据阈值的扩充\n\n数据阈值的扩充是多线墙识别的一个显著特点，它扩展墙体识别的功能和算法。这一点对于墙体识别的整体来说尤其重要，因为它意味着可以根据墙体线条数目的具体需求快速集成新的多线墙识别处理算法或优化现有算法。数据阈值设计是一个灵活的插件，可以通过实现特定的接口和遵循约定的协议，将新的图像处理模块作为插件加入系统。\n\n# 3.2.2.2 线条识别\n\n结合Canny边缘检测结果，多线墙识别进一步应用Hough变换来识别图像中的直线。Hough变换是一种强大的线条检测算法，它通过变换空间来识别具有特定形状的特征。对于直线检测，Hough变换的公式可以表示为：\n\n$$\nH (\\rho , \\theta) = \\sum f (x, y) \\cdot \\delta (\\rho - x \\cos \\theta - y \\sin \\theta) \\tag {3-5}\n$$\n\n其中， $\\rho$  是直线到原点的距离， $\\theta$  是直线的倾斜角度， $\\delta$  是狄拉克函数。通过这种变换，可以从图像中检测出直线并确定其位置和方向。\n\n# 3.2.2.3 图像分割\n\n利用边缘检测和直线识别的结果，系统可以将图像分割成多个区域，每个区域包含特定的线条结构。这种分割对于进一步的图像分析至关重要，因为它允许系统集中处理特定区域，从而提高分析的准确性和效率。\n\n# 3.2.2.4 特征提取\n\n在图像分割的基础上，系统进一步对每个区域进行特征提取，包括线条的数量、长度、角度等。这些特征对于理解图像内容和执行后续的图像识别任务至关重要。特征提取不仅帮助系统识别出墙壁边缘等关键线条结构，还能够为多线墙识别任务提供基础等。\n\n通过上述边缘检测算法和线条识别技术，多线墙识别系统能够有效地处理和分析图像数据，识别出图像中的重要线条结构。这一过程不仅涉及图像处理技术，还包括复数学运算和算法逻辑。通过精确地分割图像并提取关键特征，算法能够提供有价值的数据信息，拓展墙体识别应用场景。\n\n# 3.2.3 数据输出\n\n处理完成后，系统会生成包含多线墙信息的JSON数据和处理后的图像，如图3-4所示。多线墙的数据结构跟上述3.1.3小节中双线墙数据结构相似。\n\n![](images/b28c751cadb2f85ab06eb2d2b84f613476da260f2a33b24df393bdc3c9f7af92.jpg)  \n图3-3多线墙数据输出数据\n\nLinesWallNum（多线墙数）：值为152，表示配置中包含的“多线墙”的数量。\n\nLinesWalls（多线墙）：此数组通过一系列嵌套的像素坐标点定义了每一面多线墙，说明了多线墙的布局。\n\n其中第一组数据372和-1580代表墙体的四个顶点中左上顶点的像素位置，然后通过顺时针方向表达出四个点的像素位置，准确表达出墙体的位置。\n\n多线墙识别算法的设计与实现展示了计算机视觉技术的广泛应用。通过选择合适的算法、特定的数据结构、高效的图像处理流程，系统不仅能够准确地识别出图像中的多线墙信息，还能够确保这些数据准确地的输出。\n\n# 3.2.4 实验结果与分析\n\n在数字孪生中建筑模型设计领域，多线墙识别显得尤为关键。这一算法能够从复杂的 CAD 图纸中自动化地识别多线墙，不仅拓展了墙体识别范围，还极大地提高了设计和分析的效率。多线墙识别算法采用了 OpenCV 库和定制化的数据处理逻辑，集成了图像预处理、边缘检测、线条识别等一系列复杂的图像处理步骤，最终实现对多线墙的精确识别。\n\n实验采用跟3.1.4相同的测试图纸，其中包含了墙体、门窗等元素。实验的目标是通过多线墙识别算法，自动地从图纸中识别并提取出多线墙信息。在经过多线墙识别处理后，所有成功识别的多线墙以红线标注。\n\n![](images/aeb6d78f61849bc0e1b2be845098c78d70809c77f4ea1d74d34948e17369b52d.jpg)  \n图3-4左图为栅格化后原始图像图，右图为多线墙识别结果图像\n\n![](images/166f0d559b3ffdebdf99407c84c8b58dcb148431b228d9a7a4e88bd3c0f6efb5.jpg)\n\n实验结果表明，如图3-4所示，在CAD图纸中共有75个多线墙元素。通过应用多线墙识别算法，共识别出了84个多线墙，这表明算法在识别过程中出现了9个误识别的情况。这一现象主要归因于图纸中双线墙拐角处线条较为复杂，导致算法在这些区域的识别上产生了误差，错误地将一些双线墙判断为多线墙。尽管如此，算法的识别准确率依然达到了  $89.3\\%$  ，这证明了算法在大多数情况下的可靠性和有效性。\n\n此外算法输出的识别结果以JSON格式详细记录了每一面墙的位置和尺寸信息，为后续的数字孪生中建筑模型设计提供了宝贵的数据支持。\n\n# 3.3 实心墙识别\n\n# 3.3.1 算法概述\n\n在现代建筑设计和分析中，实心墙代表钢筋混凝土墙，所以准确识别建筑图像中的实心墙至关重要。实心墙识别算法的开发旨在自动化这一过程，减少手动绘图和分析的时间消耗，提高设计精度和效率。通过应用先进的图像处理技术，能够识别并提取出实心墙的位置和尺寸信息，为后续的结构分析和优化提供可靠的数据基础。\n\n# 3.3.2 算法工作流程\n\n实心墙识别在与上述双线墙识别和多线墙识别都有图像预处理和线条识别的共性基础上，添加了墙体间灰白色填充物的识别，确保了高效的数据处理和良好的可扩展性。主要模块包括图像预处理、实心墙检测和数据转换，它们协同工作，从输入的CAD建筑图像中自动识别实心墙。\n\n# 3.3.2.1 图像预处理\n\n在图像预处理阶段，实心墙的处理方法与3.1节的双线墙图像预处理方式和3.2节多线墙图像预处理大致相同，通过灰度值转换、噪声去除和图像增强，来提高识别的准确度。\n\n# 3.3.2.2 实心墙检测\n\n由于实心墙比双线墙多了墙体间灰白色填充表示，所以检测方法必须增加填充体检测。实心墙检测模块首先利用自定义的MyHRect1和MyVRect1结构体实现对水平和垂直墙面线条的识别。然后根据线条的方向和长度，确定实心墙水平和垂直墙面。\n\n实心墙灰白色填充物识别中的关键部分包括两个主要函数：ishorizonline1和isverticalline1，它们共同构成了识别图像中墙体间灰白色填充物的核心逻辑。这两个函数专门负责检测图像中的水平和垂直线段，进而帮助识别和处理图像中的实心墙。\n\nishorizonline1函数专注于水平线段的识别。通过遍历图像从指定起点开始的像素行，该函数检查每个像素点的颜色值是否低于预设的阈值，这个阈值通常对应于墙体间填充物的灰白色。函数参数包括图像矩阵、起始坐标、输出参数（标记线段的起始和结束位置）以及线段的最小长度。当发现长度超过最小长度要求的线段时，函数返回真值，表明成功检测到水平线段。这种检测方式有效地辨识出图像中的灰白色填充物，为进一步的图像处理提供了基础。\n\n与此同时，isverticalline1函数采取了类似的方法来识别垂直线段。它沿着给定起点的像素列向下遍历，同样检查像素值是否低于设定的阈值。除此之外，该函数还考虑线段宽度和RGB颜色误差阈值，以增强识别的精确性。通过这样的机制，函数能够有效识别图像中的垂直墙体或填充物。\n\n上述两个函数的工作都是通过检测满足特定颜色和长度要求的连续像素行或列来实现实心墙体识别。ishorizonline1负责水平方向的检测，而isverticalline1则负责垂直方向的检测。这种分工合作的方式不仅提高了识别的准确性，还增强了处理图像的能力，使得算法能够有效地识别并处理图像中的实心墙及其间的灰白色填充物。\n\n# 3.3.3 数据输出\n\n墙面检测完成后，将结果转换为JSON格式，如图3-5所示。\n\n![](images/fb0c3a16db894e2bc1a4dbf1bbb6727c9cf743d1a25037c5eecf30bcd14d9963.jpg)  \n图3-5 实心墙数据输出数据\n\nSolidWallNum（实心墙数）：值为56，表示配置中包含的“实心墙”的数量。\n\nSolidWalls（实心墙）：此数组通过一系列嵌套的像素坐标点定义了每一面实心墙，说明了实心墙的布局。\n\n其中第一组数据72和-725代表墙体的四个顶点中左上顶点的像素位置，然后通过顺时针方向表达出四个点的像素位置，准确表达出墙体的位置。\n\n# 3.3.4 实验结果与分析\n\n实心墙在建筑结构中起着不可或缺的作用，因此准确识别出实心墙对于数字孪生中建筑模型设计具有重要意义。在本次实心墙识别实验中，系统经过预处理、检测和识别三个阶段，最终成功地将实心墙从图纸中分离出来。在CAD图纸中，实际存在的实心墙数量为61个。系统识别出的结果为68个实心墙，这意味着有7个误识别的案例。这一误识别的主要原因在于，部分多线墙的线条过于密集，与实心墙的灰白色填充物在视觉上相似，导致系统将其错误地判断为实心墙。尽管如此，系统的识别准确率依然达到了  $91.1\\%$  ，显示出了较高的准确性。\n\n识别结果的可视化展示，如图3-6所示，清晰地标出了每一面实心墙的位置。此外算法将识别结果转换成了JSON格式，这不仅方便了后续的数据分析，而且使得实心墙的位置和几何信息得到了详尽的记录和整理，为进一步的模型设计、结构优化和数据应用提供了数据支持。\n\n![](images/532b894294f1b369bfcfd6340be9982841c07c4297e3b88a2608e77585d6a385.jpg)  \n图3-6左图为栅格化后原始图像图，右图为实心墙识别结果图像\n\n![](images/691272e49d8446463a54fca409d8e6770d73aae76733f2162e8f6b9d2fb6b901.jpg)\n\n# 3.4 窗户识别\n\n# 3.4.1 算法概述\n\n在本章中将深入探讨一种窗户检测算法，本算法的核心目的是识别和定位图像中的窗户结构，它利用图像中的线条和形状，结合先进的图像处理技术，实现了窗户的准确检测。本小节将详细介绍算法的实现细节，包括算法原理、关键数据结构的定义、图像处理方法以及窗户的定位和识别策略。\n\n# 3.4.2 算法工作流程\n\n# 3.4.2.1 图像预处理\n\n在图像预处理阶段，窗户识别的灰度值转换和图像增强处理方法与上述墙体识别图像预处理方式相同。但在噪声去除过程中，添加了中值滤波器，通过对图像中每个像素的邻域内的像素值进行排序，并取中值作为该像素的新值。数学表示为：\n\n$$\nP _ {n e w} = \\text {m e d i a n} \\left\\{P _ {\\text {n e i g h b o r h o o d}} \\right\\} \\tag {3-6}\n$$\n\n# 3.4.2.2 窗户检测\n\n在 CAD 建筑图纸中，窗户的描述形式与双线墙的描述有很多类似的地方，都是通过矩形进行表示，但是窗户是由三个扁平细长的矩形根据最长边拼接在一起组成。所以窗户识别在双线墙识别的基础上添加矩形数量识别和矩形紧邻状态检测，保证识别方法的灵活性、扩展性和高效性。\n\n# 3.4.2.3 直线检测\n\n在检测到直线后，需要添加直线间距计算、直线角度计算和直线几何关系，来判断哪些直线可以组成窗户的边缘。因此在窗户识别检测中设计了两个关键的函数：calculateAngle()和calculateDistance()，它们分别用于处理直线间的角度问题和点间的距离问题。\n\n计算两直线夹角的原理：\n\ncalculateAngle()函数通过向量点乘公式来计算两条直线间的夹角余弦值。如果有两条直线，它们的方向向量分别为  $l1$  和  $l2$ ， $l1$  的方向向量为  $(l1.dx, l1.dy)$ ， $l2$  的方向向量为  $(l2.dx, l2.dy)$ ，则这两个向量的点乘（内积）可以表示为：\n\n$$\n\\text {d o t P r o d u c t} = l 1. d x \\times l 2. d x + l 1. d y \\times l 2. d y \\tag {3-7}\n$$\n\n两个向量的模乘（即两个向量的长度之积）则为：\n\n$$\n\\text {n o r m s} = \\sqrt {\\left(\\mathrm {l} 1 . \\mathrm {d x} ^ {2} + \\mathrm {l} 1 . \\mathrm {d y} ^ {2}\\right) \\times \\left(\\mathrm {l} 2 . \\mathrm {d x} ^ {2} + \\mathrm {l} 2 . \\mathrm {d y} ^ {2}\\right)} \\tag {3-8}\n$$\n\n两条直线间的夹角  $\\theta$  的余弦值可以通过它们的点乘除以模乘得到：\n\n$$\n\\cos (\\theta) = \\frac {\\text {d o t P r o d u c t}}{\\text {n o r m s}} \\tag {3-9}\n$$\n\n因此，夹角  $\\theta$  可以通过计算余弦值的反余弦(arccos)得到：\n\n$$\n\\theta = \\arccos  \\left(\\frac {\\text {d o t P r o d u c t}}{\\text {n o r m s}}\\right) \\tag {3-10}\n$$\n\n计算两点之间距离的原理：\n\ncalculateDistance()函数利用欧几里得距离公式来计算两点  $p1$  和  $p2$  之间的距离。如果  $p1$  的坐标为  $(p1.x,p1.y)$ ， $p2$  的坐标为  $(p2.x,p2.y)$ ，则两点间的距离可以通过以下公式计算得出：\n\n$$\nd = \\sqrt {\\left(p 1 . x - p 2 . x\\right) ^ {2} + \\left(p 1 . y - p 2 . y\\right) ^ {2}} \\tag {3-11}\n$$\n\n这里  $(p1.x - p2.x)^{2}$  和  $(p1.y - p2.y)^{2}$  分别是两点在水平和垂直方向上距离的平方，将它们相加后开方即得到两点间的欧氏距离。\n\n# 3.4.2.4 容错值\n\n在窗户的识别过程中，容错值的设置对于识别误差至关重要。容错值允许算法在判断点是否位于矩形内部或两个矩形是否紧邻时，有一定的容错范围。在本窗户识别方法中，通过大量测试工作，将矩形紧邻的容错值设置为5。这增强了算法的鲁棒性，使其能够在不同的图像质量和环境条件下稳定工作。\n\n# 3.4.2.5 矩形构造\n\n矩形构造算法采用聚类和合并来构造窗户。这一步骤关键在于分析哪些直线可以组合成窗户的边缘。聚类或合并的策略主要依赖于距离准则和角度准则，其中距离准则要求两条直线之间的距离在一定范围内，而角度准则要求两条直线的夹角接近90度或者平行。通过应用这两条准则，算法能够从检测到的直线中识别出可能构成窗户边缘的直线，并将它们组合成矩形。\n\n通过结合直线检测、容错值设定和矩形构造方法，算法能够在给定的图像中有效识别和定位窗户。首先利用图像处理技术识别出图像中的直线；然后基于这些直线按照距离和角度准则构建窗户的矩形表示；最后通过分析这些矩形的大小、形状和相对位置，确定它们是否代表窗户。\n\n# 3.4.3 数据输出\n\n窗户检测完成后，将结果转换为JSON格式，如图3-7所示。\n\n![](images/24c25bc861e1cdab4b83a92963045a63f7825d7e2375cae6f2e75ad97af3e09b.jpg)  \n图3-7 窗户数据输出数据\n\nWindowNum（窗户数）：值为14，表示配置中包含的“窗户”的数量。\n\nWindows（窗户）：此数组通过一系列嵌套的像素坐标点定义了每一扇窗户，说明了窗户的布局。\n\n其中第一组数据653和-605代表窗户的四个顶点中左上顶点的像素位置，然后通过顺时针方向表达出四个点的像素位置，准确表达出窗户的位置。\n\n# 3.4.4 实验结果与分析\n\n在本次实验中，模型采用了跟上述三小节一样的 CAD 图纸作为分析对象。实验的目的是通过窗户识别算法，自动从图纸中识别出所有窗户的结构，并以红线在处理后的图像上进行标注，从而清晰展示窗户的位置和尺寸。\n\n实验结果显示，CAD图纸中实际存在的窗户数量为12个，而窗户识别算法识别出了14个窗户。这表明算法在处理过程中产生了2个误识别的案例。误识别的主要原因在于，部分多线墙的线条数量与窗户的线条数量相等，导致算法错误地将这些多线墙判断为窗户。尽管如此，算法的识别准确率仍然达到了  $85.7\\%$  这证明了算法在大多数情况下的可靠性。\n\n识别结果的可视化展示，如图3-8所示。此外窗户识别的结果还被转换成了JSON格式，详细记录了每扇窗户的具体信息，包括它们的位置、尺寸信息。这种数据输出方式不仅增强了窗户识别算法的实用性，而且为数字孪生中建筑模型设计提供了坚实的数据基础。\n\n![](images/ad858b1443b7867b1a98d905c0f569e65bd94bfb50eaf5edfed7716de7aedf78.jpg)  \n图3-8左图为栅格化后原始图像图，右图为窗户识别结果图像\n\n![](images/4fff1a8975e614fb84ebc5858fb3fb3ed7b2d753278f7ecae3f806247fd81c16.jpg)\n\n# 3.5 本章小节\n\n本章深入探讨了基于OpenCV的CAD建筑图纸图像处理与分析，尤其聚焦于墙体和窗户的自动识别，并且在双线墙的基础上，通过阈值设定从无到有拓展了多线墙和实心墙识别，识别性能如表3-1所示。此外算法在各类建筑元素识别任务中均展现出优异其他识别算法的性能，其中双线墙、多线墙、实心墙和窗户的识别率分别达到了0.886、0.893、0.911和0.857。\n\n表 3-1 建筑元素识别任务的性能评估表  \n\n<table><tr><td>类别</td><td>双线墙</td><td>多线墙</td><td>实心墙</td><td>窗户</td></tr><tr><td>HRNetsV2 W18 [48]</td><td>0.620</td><td>None</td><td>None</td><td>0.620</td></tr><tr><td>HRNetsV2 W48 [48]</td><td>0.624</td><td>None</td><td>None</td><td>0.640</td></tr><tr><td>DeepLabv3+ R50 [49]</td><td>0.630</td><td>None</td><td>None</td><td>0.659</td></tr><tr><td>DeepLabv3+ R101 [49]</td><td>0.634</td><td>None</td><td>None</td><td>0.666</td></tr><tr><td>CNN-GCN[33]</td><td>0.814</td><td>None</td><td>None</td><td>0.709</td></tr><tr><td>本模型方法</td><td>0.886</td><td>0.893</td><td>0.911</td><td>0.857</td></tr></table>\n\n在双线墙识别的处理过程中，采用了图像预处理、边缘检测、直线检测和特征提取等关键步骤，有效地从复杂的 CAD 图纸中提取了墙体的边缘和直线信息。预处理阶段包括灰度转换、噪声去除和图像增强，这为后续算法的最佳运行状态奠定了基础。利用 Canny 边缘检测和霍夫变换直线检测等算法，能够从图像中精确地识别出直线，进而通过特定的过滤和匹配算法识别出代表双线墙的直线对。\n\n多线墙识别算法在此基础上进一步发展，通过增强的预处理方法和更复杂的直线检测策略，有效地处理并识别出更复杂的墙体结构，为建筑图纸中复杂环境的深入分析提供了详尽的墙体信息。\n\n实心墙识别算法通过识别墙体间的灰白色填充物，准确地从CAD图纸中提取出实心墙的位置和尺寸信息，这对于深入理解建筑图纸的结构布局至关重要。\n\n实心墙在建筑设计中具有重要的结构和隔断功能。\n\n窗户识别技术则通过直线检测和矩形构造的步骤，有效地识别出图纸中的窗户位置和尺寸，进一步增强了对建筑环境的理解和分析能力。\n\n# 第四章 基于优化的YOLOv8的门检测方法\n\n# 4.1 引言\n\n在互联网时代，随着数字孪生、室内导航和室内位置追踪等业务的进一步发展，3D建筑模型的重要性越来越明显。传统的门建模方法是人工处理和标识，标识出图纸中的门元素，再进行拉伸建模，这种方法需要大量的人工干预和专业知识和技术，且容易受到建筑图纸质量和复杂性限制。因此自动识别分析建筑平面图纸中的门，成了解决上述问题的关键技术之一。在诸多门识别方法中，根据CAD建筑平面图进行门识别与三维点云、激光扫描进行建模相比，推广更加广泛，因为平面图易于获取并且人力物力相对便宜。\n\nYOLO v8 虽然对于大中型、显著性目标的检测具有良好的效果，但对于小目标且复杂的 CAD 建筑图纸图像，YOLO v8 的检测识别性能并不理想。为进一步弥补YOLO v8算法的缺陷，提高网络性能，采用以下方法进行改进：\n\n（1）从无到有构建一个完善的门识别数据集，该数据集取材全面、样本多样性强，共包含15663张原始图纸图像。  \n（2）为了减少YOLOv8模型学习时间和提升模型识别准确度，进行参数优化，调整学习率和训练轮数等。  \n（3）在应用层的图片预处理方面优化提取策略，确保后续决策层的处理需求，提高识别准确度和处理效率。\n\n# 4.2 数据收集与预处理\n\n为了适应实际工程需求和自动提取大规模复杂建筑平面图纸中的门信息，采用基于YOLOv8的方法来识别建筑平面图中的门，以提高模型对门种类的泛化能力。目前适用于识别建筑平面图中门的机器学习数据集大部分格式是相对简单并且统一的，这与实际图纸差距过大，比如：这些数据集图纸中的门都为简单弧线，弧度相对统一，而实际工程中门描述可能是两条斜线、三条斜线、四条斜线和甚至边际为单线和大小不等的门等，种类繁多，如图4-1、图4-2和图4-3所示。因此需要实现对于这些多样化门类型的准确识别，以便于从建筑平面图中提取更全面的门信息。\n\n![](images/9bc41a4a98f2be59add72fcf391350004466459f28c208029a96993cf1715866.jpg)  \n图4-1左图为具有一定弧度的单开门图，右图为具有一定弧度的双开门\n\n![](images/5f8e6d2234f0cde76cbdb530065e8b537453aabf66661406f9a1f918821413a7.jpg)\n\n![](images/7a9b4e65e7713b87e0f97fc0c4e3ac65732a153ce869034d60c6953f81bc78fc.jpg)\n\n![](images/343151fb3620d7ca3d153e9d9bb2c08fe224d6bc8f9bde886bc66f264f42f325.jpg)\n\n![](images/a1241502d79ab036fe36bae4460f748fca04780f55d9ff9f0ae347093614a618.jpg)  \n图4-2左图为斜线表示并且为单线大小门，右图为斜线表示的门  \n图4-3 三斜线表示的门\n\n为了实现这一目标，采用YOLOv8模型进行门的识别。YOLOv8模型是一种流行的目标检测算法，能够在图像中同时检测多个目标，并输出它们的边界框和类别信息。门的标注采用特定格式，即一系列包含门位置和类别信息的坐标，这些信息被记录在TXT文本中。考虑到原始图像尺寸较大，直接输入模型训练将导致巨大的计算资源浪费和训练效率低下，本研究采用了一种输入层优化策略，通过精心设计切割方案，将原始图像划分成  $640\\times 640$  像素的小块图像。该方法在切割过程中特别考虑了图纸中的门元素，确保了这些元素的完整性，避免了在图像划分过程中被切割，从而保证了图像分析的准确性和建筑元素的完整性。这样不仅可以减少图像尺寸，加快训练速度，还能保留足够的图像细节和信息，确保识别任务的准确性。\n\n本研究的数据集由15663张原始CAD建筑图纸组成，涵盖了许多现代建筑设计的特征和细节。整个数据集被划分为训练集、测试集和验证集，比例为6:2:2。训练集用于模型训练，测试集和验证集用于评估模型的泛化能力和准确性。模型性能的评估指标包括准确率、召回率和F1分数，以便根据需要对模型进行调整和优化。通过这种方法，本研究旨在准确识别和提取建筑平面图中的门信息，以支持工程的需要并促进建筑信息分析和提取的自动化。\n\n# 4.3 参数优化\n\n在YOLO v8模型训练中，学习率和迭代次数是两个关键参数，它们对模型性能和训练效率产生直接影响。学习率决定了模型参数更新的幅度，而迭代次数则决定了模型训练的深度。通过多次实验，确定了初始学习率设置为0.01，这一设置有助于模型在训练过程中快速而稳定地收敛至最优解，避免了过高学习率带来的不稳定性或过低学习率导致的缓慢收敛。迭代次数的确定同样经过了仔细的考量，最终设置为500次迭代，这一数字旨在平衡模型对训练数据的拟合度与过拟合的风险。过多的迭代可能导致模型对训练数据过度拟合，而迭代次数不足则可能导致模型未能充分学习数据特征，从而产生欠拟合。根据模型的复杂性、训练数据的规模和质量，以及训练过程中的观察，确定了这一迭代次数。\n\n因此合适的迭代次数选择需平衡模型性能和训练时间，以达到最优训练效果。为进一步提升训练效率和模型的识别准确性，模型采用了图像切割技术，将原始图像划分为  $640 \\times 640$  像素的小块。这一技术不仅减少了计算资源的消耗，而且通过保持门元素的完整性，优化了模型的识别能力。此外还将数据集划分为训练集、测试集和验证集，全面评估了模型的泛化能力和准确性。在模型训练完成后，通过在验证集上的性能评估，进一步验证了模型的效果。评估指标包括准确率、召回率和F1分数等。通过精心选择的学习率和迭代次数，以及图像切割技术和数据集的合理划分，YOLO v8模型在自动识别和提取建筑平面图中的门信息方面表现出色。\n\n# 4.4 模型应用层优化\n\n门识别在很多实际应用中具有重要价值，例如在数字孪生模型和室内信号覆盖预测等。然而门的种类和外观因素复杂多样，对于传统方法来说，设计有效的特征和规则变得十分困难。深度学习模型通过学习大量数据中的特征来解决这个问题，因此在门识别任务中表现出色。\n\n在模型方面，选用了最新版YOLOv8模型作为门识别的深度学习模型，并使用标注好的数据集对模型进行训练。YOLOv8是一种基于卷积神经网络的端到端实时目标检测算法，其特点是通过一个神经网络同时进行目标检测和分类，具有较快的速度和较高的准确性。然而在实验中遇到了一个挑战：当训练完模型后，将模型在原始高分辨率（ $9599 \\times 7199$  像素）图像中进行测试时，虽然训练过程中数据表现很好，但是在最终图像上出现识别不到门的情况。这主要是因为较大的图像尺寸增加了计算负担，并使得门目标在图像中表现较模糊，导致模型难以准确捕捉。\n\n为了克服这一问题，提出了一个创新的优化方法，即对应用层进行模型优化，引入切割整合算法。这个算法的原理是：\n\n1）训练模型选取。初始化一个YOLOv8模型，然后将经过训练后包含了模型参数的权重文件导入，这些参数经过大量数据训练得到，能够实现高精度的目标检测。  \n2）图像读取与预处理。从文件系统中加载一张需要测试的高分辨率图像，通过OpenCV库读取图像。  \n3）图像切割。通过遍历高分辨率图像并按照设定的尺寸逐块切割，生成小图像块并记录其在原图中的坐标。  \n4）目标检测。将每个小图像块输入到上述加入了权重参数的YOLOv8模型中，模型返回检测到的目标的边界框和置信度得分。  \n5）结果处理与合并。根据小图像块在大图像中的实际位置调整边界框坐标，将从小图像上得到的检测结果映射回原始大尺寸图像，并在图像上标注目标的位置和置信度。\n\n经过切割整合算法对数据集进行预处理后，实验结果显示门识别模型的准确率明显提高，达到了0.877。这是一个显著的改进，证明了切割整合算法在门识别任务中的有效性。\n\n切割整合算法的改进带来了两个主要好处：首先通过将大图像切割成小图像，模型在测试过程中处理更小的输入图像，从而降低了计算复杂度，加快了测试速度；其次切割后的小图像中的门目标更为突出和清晰，有助于模型更准确地学习和预测门的位置和类型。\n\n# 4.5 实验设计与结果\n\n本节深入展示和讨论了利用切割整合算法识别建筑平面图中门的实验结果。实验的主要目标是全面评估模型在不同训练条件下的性能，特别是切割整合算法对模型性能的影响。为了更加全面地验证模型的性能和稳定性，本研究在原有的100轮训练基础上，进一步扩展了实验范围，包括额外的200轮、300轮、400轮和500轮训练。这种扩展实验设计能够对模型在长期训练过程中的准确度、稳定性和效率进行深入分析，从而更加全面地理解和评价所提出方法的有效性和实用性。\n\n# 4.5.1 未加入切割整合算法识别实验结果\n\n![](images/c0382d517cacc257589f58bbe3915f8fb82ebefc600a503b2138827186c5ecd8.jpg)\n\n![](images/427d463f4222d12b6490a824410b260fcd2c9dcc1544abce1453d4d826fc8310.jpg)\n\n![](images/cac91216802685aaef9fc8c3431ccc161d80f55bde06b3bcbaa8160b5afac487.jpg)\n\n![](images/c6a2c430b377728a37f9ea5fb2d4bc0d99d43886007ec9f5eae4c583c3d29ba2.jpg)\n\n![](images/95a9526f29508bb4b087b0707aac201e715d115287ba606c1e92f9f76ba23ab0.jpg)\n\n![](images/791f48985891dc06a8b6fe5b07abda87290f81749a0d6d624dddaf395c261555.jpg)  \n图4-4 训练100轮后的训练数据\n\n![](images/870122a608bcc914c3b74cff89ab90cce33438da925787e9ba6ed68bc1e962f4.jpg)\n\n![](images/9df7db17831f25cd016928659eabd494e3f9cdf73595b7d8a6473da4d499952c.jpg)\n\n![](images/29be9014fbcb6b7e43757442b0023c261fb8bdb7300c045fcd4c20a414c5f532.jpg)\n\n![](images/f31ccd4b8126a09f3f2d1ba4bdfab6148a09f1d1c0c8ba8eb51b157bd94b746f.jpg)\n\n![](images/2e777f3a33a7ffe350475b056baa90f68e4a9449d8c25b3c5640bfc094c3344c.jpg)  \n图4-5未加入切割整合算法，训练100轮后，图纸中65个门均未被识别\n\n实验首先考察了在不采用切割整合算法的情况下，YOLO v8 模型对原始高分辨率图像的识别准确度。虽然训练后的 metrics/precision(B)、metrics/recall(B)、\n\nmetrics/mAP50(B)和metrics/mAP50-95(B)分别达到了0.948、0.961、0.984和0.827，如图4-4所示，但是由于原始图像的尺寸较大，模型在捕捉门的位置和细节方面遭遇了挑战。这导致了在验证集上没有任何识别框显示，也没有置信度表达，意味着模型未能成功识别出任何门的实例，举例如图4-5所示。\n\n为了深入探究是否因为训练周期太短而导致无法识别情况，实验进一步扩展了迭代范围，除了初始的100轮训练外，还额外进行了200轮、300轮、400轮和500轮的训练，训练数据如表4-1、图4-6和图4-7所示。\n\n表 4-1 模型识别指标  \n\n<table><tr><td>epoch</td><td>metrics/precision(B)</td><td>metrics/reca11(B)</td><td>metrics/mAP50(B)</td><td>metrics/mAP50-95(B)</td></tr><tr><td>100</td><td>0.94764</td><td>0.96128</td><td>0.98378</td><td>0.82707</td></tr><tr><td>200</td><td>0.9655</td><td>0.98238</td><td>0.99016</td><td>0.87012</td></tr><tr><td>300</td><td>0.9773</td><td>0.98606</td><td>0.99333</td><td>0.89815</td></tr><tr><td>400</td><td>0.98545</td><td>0.97938</td><td>0.99383</td><td>0.91472</td></tr><tr><td>500</td><td>0.9825</td><td>0.98916</td><td>0.9944</td><td>0.92184</td></tr></table>\n\n![](images/0ae65c6c2f277aa8a27b3d6fd061124972002abbd84fb01b37a9108618e7153b.jpg)  \n图4-6左图为训练200轮的训练数据，右图为训练300轮的训练数据\n\n![](images/be9828d8d33374ca87f7451590b4ddb699094aa53155bfb70b9bbefc13cc30e2.jpg)\n\n![](images/0427fb5cc7a532258d55c1fe2941e4f7e4f547ded271c5cd38e46185aaaa0ab7.jpg)  \n图4-7左图为训练400轮的训练数据，右图为训练500轮的训练数据\n\n![](images/2bac083f1bd8513428bf4b88a223cc20ff4ed2fa07002b040e3bfe77c15c15bd.jpg)\n\n从实验数据中可以看出，虽然通过增加训练轮次在进一步改善了precision、recall及mAP（mean Average Precision）指标，但是在没有应用切割整合算法的情况下，模型在验证集的高分辨率大图像上仍然没有门识别的检测框和置信度表达，举例如图4-8所示。识别结果表明模型在大尺寸图像中识别门的位置和细节方面遇到困难。\n\n![](images/fa92bbee4e83c7a8c351a768d96ec459977a78689a2c13ed4c20c3f17aa3b964.jpg)  \n图4-8未加入切割整合算法，经过200、300、400和500轮训练后的模型均未识别出图中的75个门\n\n# 4.5.2 加入切割整合算法实验结果\n\n为了应对原始图像尺寸对模型性能可能产生的负面影响，引入了一种图像切割整合算法。该算法通过将大型原始图像分割成多个较小尺寸的图像，使得模型能够更加高效地处理和识别门目标，显著提升了整体的识别准确度。实验结果表明，采用该切割整合算法进行预处理后，模型的识别准确率得到了显著提升。\n\n为了全面评估模型在长期训练中的性能，模型在完成基础的100轮训练后，继续进行了200轮、300轮、400轮，乃至500轮的额外训练。然而当训练达到500轮时，模型开始出现过度拟合的迹象。这种现象发生的原因是，随着训练的深入，模型逐渐学习到了训练数据中的特定噪声和异常值，而非泛化的特征。\n\n为了避免过度拟合并增强模型的泛化能力，模型在完成500轮训练后不再增加训练轮数。具体到识别结果，100轮、200轮、300轮、400轮和500轮的训练分别识别出了41、43、43、50和57个门，如图4-9、图4-10和图4-11所示。这表明在未达到过拟合前，随着训练深度增加，模型识别准确度也得到了进一步提高。\n\n![](images/92c03d92d28e9dc2fe93b2e074037230df8de8f3445b449fb57ca453f1a61f2d.jpg)  \n图4-9加入切割整合算法后，左图为训练100轮后识别结果举例，右图为训练200轮后识别\n\n![](images/76a2d03ae5d9c620f2ac8d02d8d92c6cc723ad5686c7cc5f44bebc9bd955505c.jpg)  \n结果举例\n\n![](images/473b0d8f7bf07c70288cd22cf4a8444600d1e3c28154a4db42bfb14532b50fbc.jpg)  \n图4-10加入切割整合算法后，左图为训练300轮后识别结果举例，右图为训练400轮后识\n\n![](images/c29801e84786c1f7caf8465f00ed8917bafd2774ca78890fa164ccd925c5cab2.jpg)\n\n![](images/044c35d258991a365c0589a632ea423efee377ce1deb75e927c6efab19218023.jpg)  \n别结果举例  \n图4-11加入切割整合算法后，训练500轮后识别结果举例\n\n# 4.5.3 实验对比分析\n\n在本节中将通过对比实验来分析切割整合算法对门识别任务性能的影响。实验结果清晰地展示了算法优化前后的识别准确度差异，从而验证了切割整合算法的有效性。\n\n首先通过表4-2展示了不同训练轮数和图片切割像素下，未加入切割整合算法与加入切割整合算法的门识别准确度对比。从表中可以看出，在未加入切割整合算法的情况下，即使改变了切割像素，模型在100至500轮训练过程中的识别准确度也均为0。这表明原始YOLOv8模型在处理高分辨率图像时，由于计算负担的增加和目标细节的模糊，难以有效捕捉门的位置和特征。然而当引入切割整合算法后，模型的识别准确度显著提升。在切割像素为  $640 \\times 640$  情况下，经过100轮训练后，加入切割整合算法的模型召回率即达到了0.630，而随着训练的深入，准确度逐步提高，在500轮训练后达到了0.877。表4-2的结果直观地反映了切割整合算法在提升模型性能方面的重要作用。\n\n表 4-2 门识别算法召回率对比  \n\n<table><tr><td>门识别</td><td>未加入切割整合算法（切割像素640×640）</td><td>加入切割整合算法（切割像素640×640）</td><td>未加入切割整合算法（切割像素1080×1080）</td><td>加入切割整合算法（切割像素1080×1080）</td></tr><tr><td>100</td><td>0</td><td>0.630</td><td>0</td><td>0.613</td></tr><tr><td>200</td><td>0</td><td>0.662</td><td>0</td><td>0.637</td></tr><tr><td>300</td><td>0</td><td>0.677</td><td>0</td><td>0.658</td></tr><tr><td>400</td><td>0</td><td>0.769</td><td>0</td><td>0.745</td></tr><tr><td>500</td><td>0</td><td>0.877</td><td>0</td><td>0.851</td></tr></table>\n\n为了进一步描述识别结果对比，通过表4-3展示了不同模型在门识别任务上的准确度对比。相较于其他现有模型，本模型方法在门识别精确度、召回率和F1上都展现出了更高的准性能。\n\n表 4-3 不同模型在门识别任务上的准确度对比  \n\n<table><tr><td>类别</td><td>HRNetsV 2 W18 [48]</td><td>HRNetsV 2 W48 [48]</td><td>DeepLab v3+ R50 [49]</td><td>DeepLab v3+ R101 [49]</td><td>CNN-GC N[33]</td><td>本模型方法</td></tr><tr><td>门识别精确度 (Precision)</td><td>0.801</td><td>0.805</td><td>0.802</td><td>0.824</td><td>0.839</td><td>0.895</td></tr><tr><td>门识别召回率 (Recall)</td><td>0.842</td><td>0.817</td><td>0.854</td><td>0.850</td><td>0.857</td><td>0.877</td></tr><tr><td>门识别F1</td><td>0.821</td><td>0.811</td><td>0.828</td><td>0.837</td><td>0.848</td><td>0.882</td></tr></table>\n\n通过这些对比分析，切割整合算法不仅降低了模型训练的计算复杂度，还改善了门元素在图像中的清晰度和突出度。这使得模型能够更好地捕捉门的细节和特征，从而提升了识别性能。切割整合算法的引入是本研究的一个重要创新点，对于解决原始图像尺寸较大的问题具有显著效果。\n\n# 4.6 本章小结\n\n本章深入探讨了基于优化的YOLOv8算法在建筑平面图中门检测任务的应用，重点关注了数据收集与预处理、参数优化、模型应用层优化、实验设计与结果等关键方面。随着数字孪生、室内导航和室内位置追踪业务的发展，3D建筑模型的重要性不断增强，传统的门建模方法因其需要大量人工干预、专业知识和技术，以及受建筑图纸质量和复杂性的限制，逐渐显现出局限性。针对这一问题，本研究采用基于YOLOv8的方法，旨在自动识别并分析建筑平面图纸中的门，以提升模型对门种类的泛化能力和识别准确度。\n\n在数据收集与预处理阶段，本研究从无到有构建了一个包含550张原始复杂大型图纸的门识别数据集，该数据集以其全面的取材和样本多样性，为后续模型的训练提供了坚实的基础。此外针对原始图像尺寸较大导致的计算资源浪费和训练效率低下问题，本研究采用了应用层优化策略，通过将原始图像切割成  $640 \\times 640$  像素的小图像，不仅加快了模型的训练速度，还确保了识别任务的准确性。\n\n在参数优化方面，学习率和迭代次数被识别为影响模型性能和训练效率的两个关键参数。通过固定学习率和适当设置迭代次数，本研究成功提高了模型对训练数据的拟合度，同时控制了过拟合的风险，实现了模型性能和训练时间的平衡。\n\n模型应用层优化是本研究的一个重要创新点，针对原始数据集中图像分辨率高导致的YOLOv8模型预测门位置准确率不高的问题，本研究引入了图像切割整合算法，将大图像切割成多个小图像，从而使得门目标在小图像中更加突出和清晰。实验结果表明，这一改进显著提高了模型的识别准确率。\n\n最后，通过实验设计与结果分析，本研究全面评估了模型在不同训练条件下的性能。实验结果显示，通过引入切割整合算法，模型的识别准确率得到了显著提升，验证了切割整合算法在提高门识别任务中的有效性。此外，通过对比未使用切割整合算法和使用后的实验结果，本研究进一步证实了切割整合算法对于解决原始图像尺寸较大问题、提高模型识别性能的重要作用。\n\n# 第五章 总结与展望\n\n# 5.1 总结\n\n本论文深度分析了基于OpenCV和优化的YOLOv8算法在数字孪生中建筑模型设计的应用，重点关注了墙体、窗户和门的自动识别技术。随着数字孪生技术的快速发展，对于创建精确且高效的3D建筑模型的需求显著增加。面对这一需求，传统的手动建模方法由于耗时长、效率低且难以适应工程需求的快速变化，显得力不从心。因此自动化建筑元素识别技术是提升数字孪生中3D建筑模型构建效率的关键。\n\n针对现有 CAD 建筑图纸数据集普遍存在的问题，如缺乏统一的标准、数据年份陈旧以及样式单一，本论文使用了一个复杂性更高的 CAD 建筑图纸图像数据集。该数据集不仅包含了多样的现代建筑设计特点和细节，而且为后续的建筑元素自动识别提供了坚实的基础。在算法方面，本文选用了前沿的图像识别技术和 YOLO v8 深度学习模型作为研究的出发点，并在此基础上进行了改进，以提高识别的准确率和效率。\n\n本文的核心工作和结论主要包括：通过使用全面且复杂的15663张CAD建筑图纸图像数据集，满足数字孪生中建筑设计的分析需求；采用OpenCV和优化后的YOLOv8模型，重点提高了墙体、窗户和门的自动识别效果；通过对算法的一系列改进，显著提升了自动化建筑平面图分析的精确度和效率。本文的重点工作及结论主要有：\n\n1）复杂且全面的建筑图纸数据集。本论文使用了一个复杂且全面的数据集，该数据集包含15663张图像，全面覆盖了包括墙体、窗户和门在内的多种建筑元素。为了确保数据质量，本研究还对数据集中的图像进行了专业的预处理步骤，重点包括去除噪声。此外应用图像分割技术精确地从原始图纸中分离出门元素，并借助精确的标注系统对它们进行了详细的标注。这个经过细致处理的数据集为数字孪生中建筑模型设计的发展提供了一个坚实的基础。  \n2）墙体与窗户识别。在墙体与窗户识别方面，通过利用OpenCV进行图像预处理、边缘检测及特征提取，实现了从复杂CAD建筑图纸中精确识别双线墙、多线墙、实心墙及窗户元素，拓展了墙体识别的范围。预处理步骤包括灰度转换、噪声去除和图像增强，为后续算法的有效运行提供了良好基础。利用Canny边缘检测和霍夫变换直线检测技术，有效提取了墙体边缘和直线信息，通过特定的过滤和匹配算法，实现了对墙体的准确识别和分类。  \n3）门识别的优化。在门识别方面，本文基于优化后的YOLOv8模型，针对\n\nCAD图纸中门的自动识别进行了深入研究。通过使用包含多样化门类型的数据集，提高了模型的泛化能力。针对YOLOv8在处理高分辨率图纸时的性能瓶颈，引入了图像切割整合算法，将大图像切割成多个小图像进行训练和识别，显著提高了识别准确率，并减少了计算资源的消耗。\n\n4）实验结果。实验结果表明，通过图像预处理、算法优化和参数调整，基于OpenCV的墙体、窗户识别和基于优化YOLOv8的门识别方法能够有效提高数字孪生中建筑平面图纸分析的自动化水平和准确率，为3D建筑模型构建提供了可靠的数据支持。\n\n# 5.2 研究展望\n\n虽然本研究在数字孪生中建筑模型设计方面取得了一定的成果，但在该领域仍有广阔的发展空间。未来的研究可以从以下几个方向进一步扩展：\n\n1）算法的深入优化。尽管现有的识别算法已经能够处理多样化的建筑元素，但在遇到极其复杂或新颖的元素时，其性能仍然有待提高。未来的研究可以着重于引入更先进的深度学习模型和探索更高效的网络结构。例如通过实现深层次的特征学习和复杂的模式识别，提升算法对于更加新颖的建筑元素的识别准确率。此外通过大模型等新兴技术优化参数和结构，也可以为算法的鲁棒性和适应性提供新的思路。  \n2）更广泛的应用场景。目前的研究主要关注于门、墙体和窗户的识别。然而建筑平面图中还包含了许多其他重要元素，如楼梯、电梯和家具等，这些元素的自动识别对于完整的3D模型构建同样重要。未来研究可以扩展到这些元素的识别，以构建更加全面和精确的建筑模型。同时也可以将研究成果应用于数字孪生中其他场景，这些研究成果不仅可以提高技术的应用价值，还可以促进智能城市和数字孪生技术的发展。  \n3）数据集的扩充与标准化。数据集的质量和多样性是深度学习模型成功的关键。为了进一步提高模型的泛化能力，未来研究需更加关注大规模、高质量和多样化的建筑平面图纸数据集。这不仅包括增加数据量，还包括提高数据质量。同时推动建筑图纸数据标准化，建立统一的标注规范和数据格式，也更有利于模型训练和算法评估。  \n4）交互式设计与反馈机制。为了提高系统的实用性和准确性，引入交互式设计和智能反馈机制是一个值得探索的方向。这种机制允许用户直接参与识别结果的校验和修正，这样不仅可以提高用户满意度，还可以收集宝贵的反馈信息。此外可以建立一个动态的学习系统，根据用户反馈自动调整算法参数，从而为用户提供更加个性化和高效的服务。  \n5）跨学科的合作研究。建筑平面图纸分析与处理是一个典型的跨学科领域，\n\n它融合了人工智能、计算机科学和建筑学等多个学科的知识和技术。未来的发展需要这些领域的紧密合作，共同推动研究的进步。\n\n# 参考文献\n\n[1] Sun C, Hsiao C W, Sun M, et al. Horizonnet: Learning room layout with 1d representation and piano stretch data augmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 1047-1056.  \n[2] Yang S T, Wang F E, Peng C H, et al. Dula-net: A dual-projection network for estimating room layouts from a single rgb panorama[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 3363-3372.  \n[3] Zhang Y, Song S, Tan P, et al. Panocontext: A whole-room 3d context model for panoramic scene understanding[C]//Computer Vision - ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13. Springer International Publishing, 2014: 668-686.  \n[4] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.  \n[5] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.  \n[6] Dodge S, Xu J, Stenger B. Parsing floor plan images[C]//2017 Fifteenth IAPR international conference on machine vision applications (MVA). IEEE, 2017: 358-361.  \n[7] Ahmed S, Liwicki M, Weber M, et al. Improved automatic analysis of architectural floor plans[C]//2011 International conference on document analysis and recognition. IEEE, 2011: 864-869.  \n[8] Liu C, Wu J, Kohli P, et al. Raster-to-vector: Revisiting floorplan transformation[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2195-2203.  \n[9] Moradi M A, Mohammadrashidi O, Niazkar N, et al. Revealing connectivity in residential Architecture: An algorithmic approach to extracting adjacency matrices from floor plans[J]. Frontiers of Architectural Research, 2024.  \n[10] Adhikari S, Meadati P, Katragadda D. Identify Applications of AR in the Construction Industry[C]//Construction Research Congress 2024. 537-546.  \n[11] Lee C Y, Badrinarayanan V, Malisiewicz T, et al. Roomnet: End-to-end room layout estimation[C]/Proceedings of the IEEE international conference on computer vision. 2017:4865-4874.  \n[12] Ketan H S, Haleel A J. STEP-based assembly feature recognition using attribute adjacency graph for prismatic parts[J]. Eng Tech J, 2013, 31(10).  \n[13] Shen W, Wang L, Wang C, et al. Reconstruction of 3D building models based on LiDAR data [J]. Journal of Liaoning Technical University (Nature Science), 2011, 30(3): 373-377.  \n[14] Wei C, Gupta M, Czerniawski T. Automated Wall Detection in 2D CAD Drawings to Create Digital 3D Models[C]//ISARC. Proceedings of the International Symposium on Automation and Robotics in Construction. IAARC Publications, 2022, 39: 152-158.  \n[15] Rezvanifar A, Cote M, Albu A B. Symbol spotting on digital architectural floor plans using a deep learning-based framework[C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2020: 568-569.  \n[16] Kim H. Evaluation of deep learning-based automatic floor plan analysis technology: An AHP-based assessment[J]. Applied Sciences, 2021, 11(11): 4727.  \n[17] Eltarabishy S. Towards Data-Driven Design[D]. University College London, 2017.  \n[18] Or S, Wong K H, Yu Y, et al. Highly automatic approach to architectural floorplan image understanding & model generation[J]. Pattern Recognition, 2005: 25-32.  \n[19] Muammar D, Endah S N, Sasongko P S, et al. Garbage Image Segmentation Using Combination of Thresholding Algorithms and Pyramid Scene Parsing Network[C] //2020 4th International Conference on Informatics and Computational Sciences (ICICoS). IEEE, 2020: 1-6.  \n[20] Liu Y, Cheng M M, Hu X, et al. Richer convolutional features for edge detection[C] //Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3000-3009.  \n[21] Hou Q, Cheng M M, Hu X, et al. Deeply supervised salient object detection with short connections[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3203-3212.  \n[22] Noack R. Converting CAD drawings to product models[D]. Institutionen für fastigeter och byggande, 2001.  \n[23] Mahmoud M, Chen W, Yang Y, et al. Automated BIM generation for large-scale indoor complex environments based on deep learning[J]. Automation in Construction, 2024, 162: 105376.  \n[24] Hunter E, Enright J, Miller A. Feasibility assessments of a dynamical approach to compartmental modelling on graphs: Scaling limits and performance analysis[J]. Theoretical Computer Science, 2023, 980: 114247.\n[25] Jeong W S, Kong B C, Yum S G. Developing a Framework for Data-Driven Generation of Building Information Modeling from Sketches: Enhancing Efficiency in Space Configuration and Building Performance Analysis[J]. Applied Sciences, 2024, 14(7): 3013.  \n[26] Liu H, Gan V J L, Cheng J C P, et al. Automatic Fine-Grained BIM element classification using Multi-Modal deep learning (MMDL)[J]. Advanced Engineering Informatics, 2024, 61: 102458.  \n[27] Duan T, Nguyen T Q, Yeoh J K W. Deep Learning Method to Detect and Locate Signages from 2D Drawings for Semantic Enrichment of BIM[M]//Computing in Civil Engineering 2023. 535-543.  \n[28] 王腾，孟维亮，卢政达，等. RC-Net: 基于文本特征的行列约束平面图分割网络[J]. 计算机科学技术学报, 2023, 38(3): 526-539.  \n[29] Kalervo A, Ylioinas J, Häikiö M, et al. Cubicasa5k: A dataset and an improved multi-task model for floorplan image analysis[C]//Image Analysis: 21st Scandinavian Conference, SCIA 2019, Norrköping, Sweden, June 11 - 13, 2019, Proceedings 21. Springer International Publishing, 2019: 28-40.  \n[30] Seo J, Park H, Choo S. Inference of drawing elements and space usage on architectural drawings using semantic segmentation[J]. Applied Sciences, 2020, 10(20): 7347.  \n[31] Pizarro P N, Hitschfeld N, Sipiran I, et al. Automatic floor plan analysis and recognition[J]. Automation in Construction, 2022, 140: 104348.  \n[32] Liu C T, Chung H Y, Hwang C C. Design assessments of a magnetic-geared double-rotor permanent magnet generator[J]. IEEE Transactions on Magnetics, 2013, 50(1): 1-4.  \n[33] Fan Z, Zhu L, Li H, et al. Floorplancad: A large-scale cad drawing dataset for panoptic symbol spotting[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 10128-10137.  \n[34] Jing J, Liu S, Wang G, et al. Recent advances on image edge detection: A comprehensive review[J]. Neurocomputing, 2022, 503: 259-271.  \n[35] Xue H, Géraud T, Duret-Lutz A. Multiband segmentation using morphological clustering and fusion $ application to color image segmentation[C]//Proceedings 2003 International Conference on Image Processing (Cat. No. 03CH37429). IEEE, 2003, 1: I-353.  \n[36] Shaw D, Barnes N. Perspective rectangle detection[C]/Proceedings of the Workshop of the Application of Computer Vision, in conjunction with ECCV. 2006, 2006: 119-127.  \n[37] Galamhos C, Matas J, Kittler J. Progressive probabilistic Hough transform for line detection[C]//Proceedings. 1999 IEEE computer society conference on computer vision and pattern recognition (Cat. No PR00149). IEEE, 1999, 1: 554-560.\n[38] Bhattacharyya S. A brief survey of color image preprocessing and segmentation techniques[J]. Journal of Pattern Recognition Research, 2011, 1(1): 120-129.  \n[39] Minaee S, Boykov Y, Porikli F, et al. Image segmentation using deep learning: A survey[J]. IEEE transactions on pattern analysis and machine intelligence, 2021, 44(7): 3523-3542.  \n[40] Noble J. High frame rate image pre-processing system and method: U.S. Patent Application 17/253,006[P]. 2021-4-22.  \n[41] Zhang F, Xiao Z, Ni K, et al. Image restoration method based on the combination of heat conduction equation and anisotropic coupled diffusion equations[C]//2011 International Conference on Control, Automation and Systems Engineering (CASE). IEEE, 2011: 1-4.  \n[42] Chen D H, Zhang Z W, Chang T R. Leaf segmentation and 3D reconstruction of ARAFIDOPSIS based on MASK R-CNN[C]/2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI). IEEE, 2019: 1073-1074.  \n[43] Yu C, Shin Y. Ship Detection in Synthetic Aperture Radar Images with Improved YOLOv8[C]/2023 14th International Conference on Information and Communication Technology Convergence (ICTC). IEEE, 2023: 308-311.  \n[44] Wu R, Zhou F, Li N, et al. Enhanced You Only Look Once X for surface defect detection of strip steel[J]. Frontiers in Neurorobotics, 2022, 16: 1042780.  \n[45] Heneweer C, Zirk M, Safi A, et al. An innovative approach for preoperative perforator flap planning using contrast-enhanced B-Flow imaging[J]. Plastic and Reconstructive Surgery - Global Open, 2021, 9(5): e3547.  \n[46] Amiri Z, Hassanpour H, Beghdadi A. An Expanded MLP Neural Network for Fast Angiodysplasia Lesions Segmentation in Capsule Endoscopy Images[J]. International Journal on Artificial Intelligence Tools, 2022, 31(02): 2250006.  \n[47] Contributors, M, et al. YOLOv8 by MMYOLO, 2023. Available at: https://github.com/open-mmlab/mmyolo/tree/main/configs/yolov8 [Accessed: 24 April 2024].  \n[48] Wang J, Sun K, Cheng T, et al. Deep high-resolution representation learning for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2020, 43(10): 3349-3364.  \n[49] Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.",
    "chunked": true
  },
  {
    "id": "78141f69-093c-44ea-a286-ec2fe4535b73",
    "title": "可控文本生成图像的关键技术研究_陈卓为",
    "fileName": "可控文本生成图像的关键技术研究_陈卓为.pdf",
    "fileType": "pdf",
    "fileSize": 20473160,
    "uploadTime": "2025-12-29T19:06:45.506235",
    "parsed": true,
    "parseStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251229190645_可控文本生成图像的关键技术研究_陈卓为.pdf",
    "errorMessage": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
    "markdownContent": "# 中国科学技术大学\n\n# University of Science and Technology of China\n\n# 博士学位论文\n\n![](images/84835606a58de5e8083728d8bc182423b3a80bb406b8ad409d2a859df02bb714.jpg)\n\n论文题目 可控文本生成图像的关键技术研究\n\n作者姓名 陈卓为\n\n学科专业 网络空间安全\n\n导师姓名 毛震东教授\n\n完成时间 二〇二四年十一月\n\n# 中国科学技术大学\n\n# 博士学位论文\n\n![](images/70444d82f420a9da65311cc3bc7451661fa5c6d7eedd45b69d3afc632d1edb50.jpg)\n\n# 可控文本生成图像的关键技术研究\n\n作者姓名： 陈卓为\n\n学科专业： 网络空间安全\n\n导师姓名： 毛震东\n\n完成时间：二〇二四年十一月二十八日\n\n# University of Science and Technology of China A dissertation for doctor's degree\n\n![](images/d7b03be5d2f79f6616a5d27d7cf2a506868319b2244a925417c59a575e69eaa1.jpg)\n\n# Research on Key Technologies for Controllable Text-to-Image Generation\n\nAuthor: Zhuowei Chen\n\nSpeciality: Cyberspace Security\n\nSupervisor: Zhendong Mao\n\nFinished time: November 28, 2024\n\n# 摘要\n\n随着数字化进程的加速，网络空间已成为国家竞争的关键场域，既是信息传播的重要渠道，也是文化输出和国际形象塑造的核心战场。因此，一个国家在网络空间中的话语权和影响力已成为网络空间安全的重要组成部分。而目前我国在全球舆论场中的影响力相对有限，常因外媒的偏见而难以精准传递真实的中国形象和文化内涵。为增强中国在全球网络空间中的影响力，图像作为跨越语言障碍的直观传播工具，可在讲述中国故事时发挥独特作用。图像生成技术，尤其是文本生成图像技术，能够通过简单的文本描述生成复杂多样的图像，使信息更具表现力和吸引力，有助于更生动地传播中国故事，抵制各种恶意扭曲我国形象的谣言。\n\n然而由于文本自身的抽象、模糊的语义特征，仅通过文本描述往往难以满足用户对精细化控制的需求。这一可控性不足的现状导致文本生成图像系统的生产效率不高，用户满意度偏低。本研究从关键的位置可控性、属性可控性及身份可控性三方面展开研究，以克服文本生成图像的可控性瓶颈。\n\n首先，本文研究了布局结构可控的文本生成图像技术，以实现在文本生成图像中对实现单个或多个物体位置的精确控制。针对现有方法在生成复杂场景图像时容易出现物体变形或背景失真的问题，本文提出了一种基于引导的布局结构可控的文本生成图像方法。首先通过引入背景布局信息并采用Transformer模型对布局进行序列到序列的离散化生成，大幅提升了布局的完备性和语义一致性。进一步地，本文引入预训练的物体生成器作为布局到图像生成的知识先验，并设计了细粒度文本-布局交互模块，有效地将该模型的生成能力迁移到复杂场景图像的生成过程中，显著改善了图像的生成质量。\n\n然后，本文研究了属性可控的文本生成图像技术，重点关注了肤色这一属性的控制。现有的文本生成图像方法多通过非学习的方法进行肤色控制，如基于颜色描述词的粗略控制，然而这些方法常常导致生成图像中肤色一致性差等问题。本文提出了首个基于学习的细粒度肤色可控的文本生成图像方法，首先设计了自适应频率导向的颜色直方图以准确表示用户的肤色信息，然后引入颜色一致性匹配奖励策略，显式地提升了生成图像与输入图像之间的肤色一致性。最终通过实验验证该方法能够大幅提升了肤色控制的精度。此外，该方法是一种通用的颜色控制方法，可以简单地扩展到发色属性控制任务中，从而进一步增强文本生成图像模型的属性可控性。\n\n最后，本文研究了身份可控的文本生成图像技术。现有基于微调的方法通常需要进行长时间训练，而基于编码器的方法则存在生成的人脸相似性较低及难\n\n以保持文本描述的一致性的问题。本文提出了基于人脸特征精确表达的人脸身份可控的文生图方法，从相似性和编辑性两个层面提升了人脸表达的精确性。首先通过利用人脸特征提取器中的多尺度特征来表示人脸身份图像中的底层细节与高层语义信息，显著提升了生成图像的人脸相似性。然后本文进一步设计了自增强的编辑学习方法，蒸馏模型对名人身份的编辑能力，有效提高了身份表示的编辑性，解决了图像生成过程中易于忽略文本信息的问题。\n\n关键词：可控文本生成图像，跨模态生成，图像生成，深度学习\n\n# ABSTRACT\n\nWith the acceleration of digitalization, cyberspace has become a crucial arena for national competition, serving as both a key channel for information dissemination and a core battleground for cultural outreach and international image building. Consequently, a nation's discourse power and influence in cyberspace have become essential components of cyber security. Currently, however, China's influence in global public opinion remains relatively limited, often hindered by foreign media biases that prevent an accurate portrayal of China's true image and cultural depth. To strengthen China's impact in the global digital sphere, images—as a visual medium that transcends language barriers—can play a unique role in telling China's story. Image generation technology, especially text-to-image generation, allows complex and diverse images to be created from simple text descriptions. This ability can enhance the appeal and expressiveness of the message, helping to vividly spread the Chinese story and resist various malicious rumors that distort the image of our country.\n\nHowever, due to the abstract and ambiguous semantic characteristics of text, it is often difficult to meet users' demands for fine-grained control using only text descriptions. This lack of controllability leads to low production efficiency in text-to-image generation systems and poor user satisfaction. This study focuses on overcoming the controllability bottlenecks in text-to-image generation by investigating three key aspects: layout controllability, attribute controllability, and identity controllability.\n\nFirstly, this thesis investigates layout-controllable text-to-image generation technology, aiming to achieve precise control over the positions of single or multiple objects in generated images. This thesis proposes a guided layout-controllable text-to-image generation method to address object deformation or background distortion in complex scene generation with existing methods. By introducing background layout information and using a Transformer model for sequence-to-sequence discrete layout generation, the method significantly improves the completeness and semantic consistency of the layout. Furthermore, this thesis introduces a pre-trained object generator as a knowledge prior for layout-to-image generation and designs a fine-grained text-Layout interaction module, effectively transferring the model's generation capabilities to the generation of complex scene images, significantly improving image quality.\n\nSecondly, this thesis explores attribute-controllable text-to-image generation technology, focusing on the control of skin color. Existing text-to-image generation methods\n\noften use non-learning-based approaches for skin color control, such as coarse control based on color description words. However, these methods often result in poor skin color consistency in generated images. This thesis proposes the first learning-based fine-grained skin color controllable text-to-image generation method. It first designs an adaptive frequency-based color histogram to represent the user's skin color accurately and then introduces a color consistency matching reward strategy to explicitly enhance the consistency of skin color between the generated image and the input image. Experimental results demonstrate that this method significantly improves the accuracy of skin color control. Additionally, this method is a general color control approach that can be easily extended to tasks such as hair color control, further enhancing the attribute controllability of text-to-image generation models.\n\nFinally, this thesis studies identity-controllable text-to-image generation technology. Existing fine-tuning-based methods usually require long training times, while encoder-based methods often suffer from low facial similarity and difficulty maintaining consistency with textual descriptions in the generated images. This thesis proposes an identity-controllable text-to-image generation method based on precise facial feature representation, improving facial representation's accuracy in terms of similarity and editability. First, by using multi-scale features from a facial feature extractor to represent the low-level details and high-level semantic information in face identity images, the method significantly enhances facial similarity in generated images. Additionally, this thesis further designs a self-augmented editing learning method that distills the model's editing capabilities on celebrity identities, effectively improving the editability of identity representation and addressing the issue of neglecting textual information during the image generation process.\n\nKey Words: Controllable Text-to-Image Generation, Cross-Modal Generation, Image Generation, Deep Learning\n\n# 目录\n\n# 第1章绪论 1\n\n1.1 研究背景与意义 1  \n1.2 研究现状与挑战 3  \n1.3 论文的研究内容与创新点 6\n\n1.3.1 研究内容之间的关联性 ..... 8  \n1.4 论文的结构安排 9\n\n# 第2章可控文本生成图像综述 11\n\n2.1 引言 11  \n2.2 图像生成模型建模方法 11\n\n2.2.1 变分自编码器 11  \n2.2.2 生成对抗网络 ..... 13  \n2.2.3 自回归模型 14  \n2.2.4 扩散模型 ..... 15  \n2.2.5 其他生成方法 ..... 16\n\n# 2.3 文本生成图像模型演进 17\n\n2.3.1 生成方法层面 ..... 18  \n2.3.2 文本表征层面 21  \n2.3.3 模型结构层面 22  \n2.3.4 数据增强 22\n\n# 2.4可控文木生成图像 23\n\n2.4.1 位置可控性 ..... 23  \n2.4.2 属性可控性 26  \n2.4.3 身份可控性 27\n\n# 2.5 评价指标 30\n\n2.6 数据集 34  \n2.7 本章小结 35\n\n# 第3章 基于引导的布局结构可控文生图 37\n\n3.1 引言 37  \n3.2相关工作 40\n\n3.2.1 文本生成图像 40  \n3.2.2 位置可控的文本生成图像 ..... 40  \n3.2.3 文本到布局生成 41  \n3.2.4 布局到图像生成 41\n\n# 3.3 算法设计 41\n\n3.3.1 基于Transformer的序列到序列的背景布局生成……42  \n3.3.2 基于物体知识迁移与细粒度文本-布局交互的图像生成……44  \n3.3.3 模型训练 47\n\n3.4 实验结果与分析 50\n\n3.4.1 实验设置 50  \n3.4.2 主要结果 ..... 51  \n3.4.3 消融实验 53\n\n3.5 本章小结 ..... 57\n\n第4章 基于学习的属性可控文生图 59\n\n4.1 引言 60  \n4.2相关工作 62\n\n4.2.1 可控文本生成图像 62  \n4.2.2 肤色表示 62  \n4.2.3 基于扩散模型的文本生成图像的奖励方法 63\n\n4.3 算法设计 63\n\n4.3.1 预备知识 64  \n4.3.2 基于自适应频率导向的颜色直方图表示 65  \n4.3.3 基于颜色分布匹配的奖励学习 68  \n4.3.4 模型训练和推理 69\n\n4.4 实验结果与分析 70\n\n4.4.1 实验设置 70  \n4.4.2 主要实验结果 71  \n4.4.3 消融实验结果 ..... 72\n\n4.5 本章小结 ..... 76\n\n第5章 基于人脸特征精确表达的人脸身份可控文生图 78\n\n5.1 引言 78  \n5.2相关工作 80\n\n5.2.1 个性化图像合成 ..... 80\n\n5.3 算法设计 ..... 81\n\n5.3.1 预备知识 ..... 82  \n5.3.2 基于多尺度多令牌的身份表示 ..... 84  \n5.3.3 基于自增强的编辑性表示学习 ..... 85  \n5.3.4 模型训练 ..... 87\n\n5.4 实验结果与分析 88\n\n5.4.1 实验设置 88  \n5.4.2 主要实验结果 90  \n5.4.3 消融实验结果 91\n\n5.5 本章小结 ..... 94\n\n第6章 总结与展望 97\n\n6.1 全文总结 97  \n6.2 研究展望 98\n\n参考文献. 101\n\n# 插图清单\n\n图1.1 图像可以显著提升舆论内容的影响力 1  \n图1.2 可控文本生成图像技术介绍 2  \n图1.3 本学位论文研究内容示意图  \n图2.1 变分自编码器的基本结构 11  \n图2.2 向量量化变分自编码器技术的基本结构 12  \n图2.3 生成对抗网络的基本结构 13  \n图2.4 自回归模型的基本结构 14  \n图2.5 扩散模型的基本结构 16  \n图2.6 非自回归模型的基本结构 17  \n图2.7 文本生成图像的效果进展 17  \n图2.8 第一个基于生成对抗网络的文本生成图像方法GAN-INT-CLS介绍·18  \n图2.9 基于自回归模型的文本生成图像方法Cogview介绍 19  \n图2.10 基于扩散模型的文本生成图像方法Imagen介绍 20  \n图2.11 pix2pix方法介绍 23  \n图2.12 ControlNet方法介绍 24  \n图2.13 基于微调的身份控制方法DreamBooth介绍 27  \n图2.14 基于编码器的身份控制方法Face0介绍 30  \n图3.1 当前位置可控的文本生成图像的相关方法生成图像存在的问题 38  \n图3.2 BO-GAN的整体流程 42  \n图3.3 基于Transformer的序列到序列的布局补全生成 43  \n图3.4 BO-GAN的生成器结构 47  \n图3.5 BO-GAN中生成器和判别器中ResBlock的具体组成 48  \n图3.6 BO-GAN的判别器结构 48  \n图3.7 BO-GAN和其他方法的定性结果比较 52  \n图3.8 BO-GAN和其他方法的定性结果比较的更多示例 53  \n图3.9 BO-GAN生成的中间背景和物体布局及相应的最终图像 54  \n图3.10 背景布局和TL-Norm的消融实验结果 55  \n图3.11 利用Transformer建模布局生成的优势 56  \n图3.12 利用预训练类别生成物体模型的优势 57  \n图3.13 使用预训练类别生成物体模型后对收敛速度的影响 57\n\n图4.1 谷歌Gemini文生图模型存在无法保持人种肤色的现象 59  \n图4.2 Skin-Adapter生成结果及与肤色提示词效果对比 60  \n图4.3 文本描述的模糊性举例 60  \n图4.4 Skin-Adapter模型结构和训练方法 63  \n图4.5 Preceiver Resampler模型结构 65  \n图4.6 不同颜色表示方法的重建结果 68  \n图4.7 肤色保持不同方法的定量结果比较 71  \n图4.8 Skin-Adapter生成结果样例 72  \n图4.9 自适应频率导向的颜色直方图算法与不同颜色编码算法的比较 73  \n图4.10 颜色分布匹配奖励的消融实验 74  \n图4.11 通过后处理进行肤色一致性调整存在的问题 75  \n图4.12 Skin-Adapter的SD-XL上的效果 75  \n图4.13 Hair-colorAdapter的效果 76  \n图5.1 DreamIdentity生成结果展示 79  \n图5.2 DreamIdentity的框架图 82  \n图5.3 自增强数据集示例 87  \n图5.4 DreamIdentity与其他方法的定性对比 89  \n图5.5 自增强编辑学习的作用 90  \n图5.6 ID编码器和多尺度特征的有效性 91  \n图5.7 编码令牌数量对生成结果的影响 93  \n图5.8 身份保持的场景切换与其他方案进行比较 94  \n图5.9 同一身份在不同场景下的生成结果 95  \n图6.1 可控文本图像生成统一建模 98  \n图6.2 从单模态文生图模型向多模态理解-生成图像演进 99\n\n# 表格清单\n\n表 3.1 BO-GAN 与其他相关方法在 MS COCO-14 数据集上的定量结果对比 50  \n表 3.2 BO-GAN 与其他方法的在人工评价指标上的对比结果 ..... 51  \n表3.3 背景生成和TL-Norm的消融实验 53  \n表3.4 物体知识迁移的消融实验 55  \n表3.5 使用Tranformer建模布局生成的优势 56  \n表 4.1 用于肤色一致性测试的文本描述 ..... 70  \n表 4.2 关于不同颜色表示方法的消融研究 ………………………………………… 73  \n表 4.3 颜色分布匹配奖励的有效性 ………………………………………… 73  \n表5.1 用于训练DreamIdentity模型的文本描述集 86  \n表 5.2 DreamIdentity 与其他人脸身份控制方法进行比较的结果 …… 90  \n表 5.3 自增强编辑性学习的消融实验 92  \n表 5.4 多尺度多令牌编码器的消融实验 93  \n表 5.5 编码令牌数量对生成结果的定量影响 …………………… 93\n\n# 算法清单\n\n3.1 BO-GAN 训练的基本流程 49  \n4.1 基于自适应频率导向的颜色直方图算法流程 67  \n5.1 DreamIdentity算法训练流程 87\n\n# 第1章绪论\n\n# 1.1 研究背景与意义\n\n随着互联网的广泛普及，截至2023年12月，我国网民数量已攀升至10.92亿。在自媒体盛行的当下，网络空间安全不仅涉及保护国家的网络基础设施和数据安全，更关系到国家在全球信息数字化中的竞争力，即各国在网络空间中的话语权。话语权，本质上是一种意识形态的主导权[1]。掌握话语权，意味着能够在全球范围内有效引导网络舆论的传播方向和发展趋势，有力抵制各种恶意诋毁和扭曲我国形象的谣言，进而维护网络社会稳定与国家安全。然而，当前我国在全球网络舆论场中的话语权相对有限，导致真实的中国形象和文化内涵难以得到准确且高效的传播，也使得消除外界对我国的误解成为一项严峻的挑战。\n\n![](images/46103ccd4f5908ccc0dc7a3775bf2cf8454df44eec94b069d1077996fb2aa4db.jpg)  \n颜色革命\n\n![](images/3aebf8e98f7ee461dbd260653b9dad25f7c88188bc9e234adc4d53686cb43e64.jpg)  \n特朗普枪击事件\n\n![](images/d199caf3eebae67257b4e8dd6de3385011439f331b291baa02c7e2ee387386d5.jpg)  \n巴以冲突  \n图1.1 图像可以显著提升舆论内容的影响力\n\n人工智能生成内容（AIGC）技术是提升我国在国际话语权博弈中的重要手段。近年来，AIGC技术发展迅速，通过机器自动生成逼真的舆论内容已逐渐成为现实。这一技术可以显著提高内容创作的效率和质量，使得我国能够迅速反应并发布有力的舆论内容，提升传播的时效性和灵活性，进而增强我国在国际话语权博弈中的地位，维护我国网络安全。其中，图像作为一种具有强大视觉冲击力的传播媒介，生动形象且贴合人类感知，能够跨越语言障碍，吸引全球受众。如图1.1所示，在颜色革命、特朗普枪击事件、巴以冲突中，代表性的图像显著提升了传播效果。基于AIGC的文本生成图像技术使用户能够通过简单的描述生成复杂多样的符合文本语义的图像。该技术可以将中国价值观内核从文字变化成生动形象的图像，在国际舆论宣传中，跨越种族、国籍、文化背景等差异，显著提升宣传效果，获取更大的话语权。此外，图片的真伪性判定也成为网络空间安全中的重要问题[2-3]。文本生成图像技术可以通过针对性地生成当前伪造图像检测器识别不正确的样本，来提升鉴伪模型的分类准确性。因此，文本生成图像技术是可以提升我国网络空间安全的关键技术。\n\n自文本生成图像任务提出以来，研究者们从文本特征的表示[4-6]，文本-图像特征的融合[4,7-9]，损失函数的设计[4,10]以及生成模型的改进[11-12]等角度大幅提\n\n升了图像生成的质量和对文本的语义理解能力。然而，由于文本自身的抽象、模糊的语义特征，仅通过文本描述往往不能满足用户进一步精细化控制的需求。这一现状迫使用户只能不断修改文本描述和生成的随机数，并进行额外的手工处理才能获得符合预期的图像。文本生成图像模型的可控性不足导致了其在实际场景中使用门槛较高及生产效率较低等问题，最终制约了信息传播效果。\n\n# 可控文本生成图像技术\n\n![](images/2d79252c8305b516ea837c43b9ccb178e5674d6a766a7e5eb0994a84ae7c4e32.jpg)  \n图1.2 可控文本生成图像技术介绍\n\n在实际使用场景中，文本生成图像系统需要从多个层次来进行精细化的控制。首先，底层的几何布局，即物体的位置和分布，直接影响观众的视觉焦点和信息传达的效果。通过精确控制图像中各个元素的布局，可以突出关键信息，将观众的注意力引导至特定内容。其次，中层的属性信息，如肤色、发色等也扮演着关键角色。细粒度的属性控制不仅能够传递特定种族、文化背景等关键信息，还可以帮助建立和维持目标受众的情感共鸣，体现对多元文化的尊重。最后，高层的人物身份控制涉及到图像中人物身份的真实性和一致性，这对于公众认知效果至关重要。通过控制人物身份，生成符合特定公众人物形象的图像，可以在不同场景中有效传播，并增强信息的可信度和说服力。\n\n本学位论文围绕可控文本生成图像技术，即文本信息和用户指定的精细化信息进行协同控制的图像生成技术进行相应的研究，并按照从底层到高层可控信号的研究内容进行展开。如图1.2所示，首先研究底层的几何布局，以支持对单个或多个物体进行位置控制的需求。然后研究中层的属性表现，实现对关键属性，如人物肤色/发色实现细粒度颜色控制。最后探索高层的身份控制，以生成指定人脸身份在文本描述的不同场景中的形象。相关研究成果已应用于国家重点研发计划《面向全员媒体的内容跨媒体解析与动态组合生产》中的图像生产系统。\n\n综上所述，为了维护我国网络空间安全，本文从增强网络舆论话语权角度出发，开展由底层到高层的可控文本生成图像研究，以提升文本生成图像系统在实际场景中的效率和质量。并且，可控文本生成图像技术作为文本和图像之间的桥梁，有助于自然语言处理和计算机视觉领域的协同发展。因此，本课题不仅在网络空间安全中有着广阔的应用前景，也在学术研究方面具有重要的意义。\n\n# 1.2 研究现状与挑战\n\n可控文本生成图像技术旨在根据自然语言描述和用户指定的精细化信息联合生成真实且语义一致的图像。早期工作主要关注如何构造先进的文本生成图像系统。GAN-INT-CLS[13]是第一个提出文本生成图像任务的工作，该方法采用了当时最先进的生成对抗网络作为建模方法，并将文本作为额外条件输入。经过训练，该方法当时仅能生成低分辨率下的单一种类的物体图像，如花/鸟等等，但也代表着学术界正式进入到文本生成图像领域中。后续基于GAN的工作通过精心设计的文本表示、文本图像交互方法和有效的损失函数进一步大幅改进了文本生成图像的质量。具体来说，StackGAN和StackGAN++[14-15]通过分层架构逐步优化先前生成的低分辨率图像或特征，提高生成图像的分辨率。AttnGAN[4]引入了网格-单词级注意力机制，可以完成更加细粒度图像的生成，并提出了一种图像-文本匹配损失函数（DAMSM损失），以提升生成图像与输入文本之间的语义一致性。DM-GAN[7]提出动态记忆模块去自适应调整单词级特征。为更好地融合文本特征，SD-GAN[16]引入了语义条件批量归一化模块。DAE-GAN[17]设计了文本方面信息，以生成更丰富的图像细节。SSA-GAN[18]以弱监督方式学习软掩码图，以便在不同位置自适应地注入不同的文本信息。然而，基于GAN的模型通常会受到训练不稳定和模型崩溃的困扰，这使得模型难以利用大规模数据集的训练[19-21]。由于大型自然语言模型的可扩展性已被证实[22]，DALL-E[23]和Parti[24]等工作提出了采用基于自回归模型的方法进行文本生成图像的方法，首先对图像特征进行离散量化，然后基于语言模型的方法学习生成这些离散编码。该方法有效利用了大规模图文数据对，大幅提升了模型在通用文本生成图像任务中的能力。最近，随着扩散模型的进展，诸如GLIDE[11]、Imagen[25]、DALL-E2[26]和LDM[27]等扩散模型也展示了前所未有的高质量和多样化的图像生成能力。但是随着文本生成图像的发展，其可控性逐渐成为其进一步实用化的瓶颈，本课题主要针对文本生成图像过程的位置可控性、属性可控性以及身份可控性，希望通过赋予文本生成图像模型对物体、属性和身份的控制能力，使得文本生成图像模型更加实用。\n\n在物体位置可控的文本生成图像方向上，一些工作将布局结构引入到文本生\n\n成图像模型中以实现精确的物体位置控制，其中具体布局结构由一系列的物体类别及其在图像中的边界框（物体左上角坐标和宽度、高度）组成。用户提供的布局结构往往是不完整的，或者用户希望在参考布局的基础上进行修改，因此，如何生成完备的语义布局，是生成高质量图像的前提条件。之前的工作[28-30]首先使用LSTM模型补全前景物体对象布局，其中类别概率由分类分布建模，位置和尺寸由多元高斯混合模型建模。该模型首先预测边界框标签，然后预测左上角的位置和尺寸。为了实现文本与布局的协同控制，Obj-GAN[29]在文本-图像损失的基础上通过探索物体注意力生成网络物体判别器，提高了图像质量。R-GAN[30]进一步引入了形状记忆库去生成更加完备的物体形状。同时也有专注于布局到图像生成的相关工作被提出，这些方法首先根据布局生成语义掩码，然后从语义掩码生成图像。Layout2Im[31]通过卷积LSTM生成掩码，然后利用类似变分自编码[32]的损失来建模每个类别的外观。LostGAN-V1[33]首先分别预测每个边界框掩码，然后形成完整的语义掩码，最后通过ISLA-Norm层，语义掩码被注入掩码到图像生成器中。LostGAN-V2[34]通过整合不同阶段特征图上学习的掩码，进一步逐步优化掩码。总的来说，研究者们从文本布局交互角度、结构化的布局建模以及布局精细化等角度进行了改进，但从引导角度出发，降低图像生成学习难度并提升生成质量的研究仍然较少。\n\n在属性细粒度可控的文本到图像生成方向上，当前模型对颜色等属性仍然不够敏感，在生成过程中主要通过将颜色信息转换为文本信息来完成属性粗略控制，如将具体颜色像素值近似为某种常见的颜色，因此，这种方式往往导致显著的信息损失。文献[35]在后处理中，将人脸区域的颜色均值调整至与参考图像一致来完成精细化的肤色控制，文献[36]通过在生成过程中引入RGB颜色损失来实现颜色控制。总的来说，目前方案均通过不同非学习的方法[37-39]去完成属性的细粒度控制，基于学习的方法尚需进一步探索。\n\n在人物身份可控的文本到图像生成方向上，当前的通用文本生成图像模型仅能生成如“奥巴马”、“迈克尔·乔丹”等名人身份的可控图像，如何基于给定人物图像进行图像生成是当前研究的热点问题。Textual-Inversion[40]和DreamArtist[41]直接使用用户提供的少量图片，通过优化的方法学习新的词嵌入，但是由于仅对词嵌入进行优化，自由度较小，往往需要较长优化时间。DreamBooth[42]对整个预训练的文本生成图像模型进行微调，以独特的标识符学习高保真度新概念，同时提出了先验信息保持损失，避免了模型过拟合到主体图像和背景图像上。为了实现更快速的微调，Custom Diffusion[43]仅更新网络中交叉注意层中的键和值映射参数，以获得更好的性能。尽管这些基于优化的方法已经获得了高保真度和可编辑性，但它们的学习仍然需要多轮迭代和用户提供的多个图像。为了高效地进行人物身份的控制，近来有一些相关的研究采用了编码器的方式直接将输入图像映\n\n射到词嵌入空间或文本表征空间中。具体而言， $\\mathsf{ELITE}^{[44]}$ 在第一阶段使用CLIP编码器的最后一层特征将常见物体编码为词嵌入，同时参考Custom Diffusion打开了网络的键和值映射参数进行学习，然后在第二阶段设计了局部编码器，使得生成网络可以获取参考图像的局部区域的细节信息。UMM-Diffusion[45]只对词嵌入编码器进行学习，同时在推理时设计了混合文本推理策略，改善了文本的控制能力。InstantBooth[46]直接将输入主体映射到文本表征空间并通过在预训练的生成模型中引入一个适配器层，来提升生成图像与输入图像之间的保真度。然而目前的方法仍然面临明显的文本一致性与人脸相似性的显著折中，如何设计面向文本生成图像场景的精准人脸表示，使得其同时包含人脸的高层语义特性和底层细节信息，而且能够与文本信息有机结合，仍然需要进一步研究。\n\n总的来说，在舆论引导场景中应用当前可控文本生成图像技术时，仍然存在显著的技术瓶颈。具体体现在：1）在物体位置可控的文本生成图像方面，现有方法在图像生成过程中欠缺适当引导，导致模型学习困难，进而容易出现物体变形或背景失真的现象。2）在属性可控的文本生成图像方面，当前基于非学习的方法，仍然无法实现高质量精细化控制。3）在人物实体可控的文本生成图像方面，目前基于文本生成图像场景的人脸身份表达不够精确，导致生成的图像无法准确反映文本内容描述，且与指定人脸的相似度较低。基于这三方面的瓶颈，本课题拟针对以下科学问题展开研究：\n\n1）如何引入合理的引导信息，降低布局可控文本生成图像模型的学习难度，提升复杂场景图像的生成质量。用户提供物体布局往往不够完整，在生成图像前需要先生成图像其他区域的布局，进行布局结构的补全生成。目前相关研究主要局限于前景布局补全生成，忽略了占据了图像大部分区域的背景布局生成，导致补全后的布局空间中仍然存在大量语义空白的区域，因此模型生成图像时还需要推理背景区域对应的语义信息，增加了模型的学习难度。同时目前方法在图像生成的过程中需要同时学习物体本身和物体之间的外观信息，存在训练时间较长，图像生成质量较差的问题。如何在布局可控文本生成图像模型中引入更加合理，充分的引导信息，降低图像生成学习的难度，仍需要进一步探索。  \n2）如何设计可学习的细粒度属性控制方法，提升文本生成图像过程中属性细粒度的可控性。当前基于属性可控的文本生成图像模型主要是基于非学习的方法，如在生成的过程中引入额外的损失或对生成图像进行肤色/颜色的归一化。这些方法没有对生成模型权重或者结构进行针对性调整，因此难以实现高质量细粒度属性控制。如何设计可学习的细粒度属性控制方法，是需要解决的关键问题。  \n3）如何设计针对文本生成图像场景的精准人脸身份表示方式，提升文本生成图像过程中人脸身份可控性。为了进一步拓展现有文本生成图像模型的控制\n\n能力，实现对人脸身份的有效控制，人脸身份表示不仅需要包含丰富准确的人脸信息来保证重构的相似性，而且需要确保该特征不影响文本语义表达。现有方法欠缺对人脸身份编码进行针对性的建模，导致生成的人脸相似度较低，且基于人脸重构为学习目标的学习方式，容易导致生成过程中忽略文本信息。如何设计针对文本生成图像场景的精准人脸身份编码学习方式是具有挑战性的问题。\n\n# 1.3 论文的研究内容与创新点\n\n![](images/55d871963e6cab2523022641d3f94d839bc52875259f3fc153e0e3baf053a174.jpg)  \n图1.3 本学位论文研究内容示意图\n\n如图1.3所示，本课题围绕可控文本生成图像的技术体系，研究位置可控性、属性可控性以及身份可控性的文本生成图像技术，针对前文提出的关键科学问题，进一步提出了以下研究路线：基于引导的布局结构可控文本生成图像方法，基于学习的细粒度颜色可控文本生成图像方法以及基于人脸特征精确表达的人脸身份可控文本生成图像方法。整体研究框架如图所示，具体介绍如下：\n\n# 1. 基于引导的布局结构可控文本生成图像\n\n位置可控性是可控文本生成图像中的基础环节。在图像创作中，用户往往希望能够通过方框的形式指定目标物体所在的大致位置，然后再通过文本生成图像模型生成对应的图像。由于用户提供物体布局往往不够完整，在生成图像前往往需要补全其他区域的布局信息。因此现有方法往往按照布局补全和布局到图像生成两阶段的生成范式进行的。然而目前方法在布局补全中主要局限于前景布局的补全，而忽视了占据了图像的  $70\\%$  以上区域的背景补全，因此图像布局空间中存在大片语义空白的区域，这一现状导致图像生成过程中仍需要对未知背景语义区域进行推理，此外，现有方法通常从零开始学习布局到复杂场景图像\n\n的生成，在有限数据集上学习图像中多样的物体的生成存在极大的挑战。导致生成图像中容易出现背景失真和物体变形等问题。\n\n本文针对以上问题，从引导的视角出发，降低该任务在图像生成阶段的建模难度，首先将背景布局信息引入布局生成中，使得在图像生成前获取完备的空间语义信息，并针对引入背景建模后更加复杂多样的布局，采用Transformer[47]进行序列到序列的离散化布局补全生成。同时本研究注意到人类学习绘画的过程是先学会绘制单个物体，再学习绘制包含多个物体的复杂场景。因此本工作引入了在大规模物体数据集预训练后的图像生成模型来提供物体生成的先验知识，同时设计细粒度文本-布局交互模块，有效地将预训练物体生成器的单个物体生成能力迁移到包含多个物体的场景图像生成过程中，最终改善了图像的整体生成质量。\n\n# 2. 基于学习的细粒度颜色可控文本生成图像方法\n\n除了位置控制外，用户通常还希望对物体的属性进行细粒度的控制。本文以人物肤色为例进行探讨。在当前的文本生成图像方法中，为了实现肤色的控制，通常采用非学习的颜色控制方法，例如通过颜色描述词来体现肤色信息，然后将这些颜色描述词与用户的原始文本结合进行图像生成。然而，这种非学习的方法在生成结果中往往存在颜色一致性差的问题。其原因在于，肤色这一属性的信息分布是多样的，无法通过简单的颜色描述（如黑色/白色）来精准刻画。\n\n本文针对以上问题，从学习的属性控制出发，提出了首个基于学习的细粒度肤色可控文本生成图像方法。首先，本研究设计了一种基于自适应频率导向的颜色直方图，将图像中出现频率较高的颜色视为量化的参考值，并根据不同输入图像自适应调整量化参考值。该方法克服了传统颜色直方图存在的量化误差问题，更加准确地表示用户的颜色信息，并排除了无关区域的干扰。此外，针对颜色统计量信息在文本生成图像模型（尤其是扩散模型）中难以学习的问题，本工作设计了一种颜色一致性匹配奖励策略。通过对多次迭代生成的图像与输入图像进行颜色一致性损失计算，显式提升了输入图像和输出图像之间的肤色一致性。\n\n# 3. 基于人脸特征精确表达的人脸身份可控文本生成图像\n\n人物生成信息也是非常重要的控制元素。用户往往希望能够创作特定人物形象的图像。生成形象不仅要与参考形象相似，而且还需要符合文本描述。现有基于训练的方法对用户输入的图像要求较高，需要同一身份下的多张图片，并且每个身份都需要较长的训练时间。而基于编码器的方法仍然面临明显的文本一致性与人脸相似性的折中，往往使用通用的特征编码器进行人脸特征表示，欠缺对人脸身份编码进行针对性的建模，导致生成的人脸相似度较低，且当前是人脸重构为学习目标的学习方式，因此学习到的特征只考虑重构出输入的人脸这单一的目标，完全忽视了生成图像应该满足文本描述这一要求，因此导致训练得到\n\n的表示往往易于忽略文本信息。\n\n本文针对以上问题，从人脸特征精确表达的角度出发，提出一种基于人脸特征提取器的多尺度多令牌的人脸身份表示方法，首先利用人脸特征提取器而非通用物体编码作为主要人脸表示，并且采用了人脸编码器中的多尺度特征去表示人脸身份图像中的底层细节与高层语义信息，提供足够的重构信息。并将该信息映射到词嵌入空间，进行人脸信息与文本信息的有机融合。进一步地，设计自增强的编辑学习方法，学习模型已有的对名人身份保持的文本生成图像能力，通过生成成对的名人编辑性数据，提升表示的编辑性。解决生成过程中因为身份表示偏向于重构导致的文本信息容易被忽略的问题。\n\n# 1.3.1 研究内容之间的关联性\n\n从应用角度出发，在实际使用场景中，文本生成图像系统需要从多个层次来进行精细化的控制。这种多层次控制包含了逐步递进的逻辑关系，从底层的几何布局到高层的身份表达，使生成的图像在结构、细节和内容上更贴合用户的预期需求。\n\n首先，底层的几何布局是文本生成图像的基础控制环节。这一层级主要决定了图像中物体的空间位置和分布，它直接影响到观众的视觉焦点和图像信息传达的效果。通过精确控制图像中各个元素的位置布局，不仅可以突出关键信息，更能在视觉上引导观众的注意力聚焦于特定内容。这种底层的布局设计，为图像生成奠定了空间框架，减少了后续生成过程中的不确定性，提升了生成内容在空间层次上的稳定性。\n\n其次，中层的属性信息控制（如肤色、发色等）在底层布局的基础上进一步丰富了图像的表达。属性信息控制带来了细节的精细化，它不仅有助于传递特定种族、文化背景等关键信息，还能够塑造图像的情感氛围，帮助观众在视觉和心理层面产生共鸣。通过细粒度的属性控制，系统可以在布局的框架上增强图像的真实感和情感表达，使生成内容更符合用户预期。该层级的信息控制填补了布局控制的细节空白，为高层的人物身份控制打下了坚实基础。\n\n最后，高层的人物身份控制进一步提升了生成图像的真实性和一致性，使其不仅停留在基础布局和属性层次，更具备特定人物形象的表达能力。高层的身份控制旨在对生成图像中的人物特征进行精确的表达，确保生成的内容符合观众对特定公众人物的认知和预期。这种控制对增强图像的传播效果和说服力至关重要。通过对人物身份的控制，文本生成图像系统可以生成高度一致且具有辨识度的图像内容，从而在宣传、教育和媒体传播等场景中有效地建立公众信任。\n\n从技术角度出发，可控文本图像生成模型需同时满足三个核心要求：图像生成的质量、控制信号的一致性，以及控制信号的协同性。\n\n首先，图像生成的质量是生成模型的基础要求，指图像的清晰度、真实性及丰富的细节表现。若生成质量欠佳，即使模型能识别一些基本结构，也难以实现应用。本研究在图像生成质量方面，设计了背景布局生成和物体知识蒸馏的方法，从引导角度出发，显著提升了图像的视觉质量。\n\n其次，模型需确保生成图像准确反映输入的控制信号。在控制信号较弱的情况下，图像生成的一致性会面临严峻挑战。本研究以肤色控制为切入点，提出精确的颜色表示方法和一致性学习策略，有效提升了在一致性要求下的生成质量。\n\n最后，多重控制信号需在生成过程中协同作用以确保图像的整体表现。基于文本输入，控制信号应尽量保留未被文本指定的内容，且对文本中互斥的内容进行合适的调整。本研究围绕身份可控性设计了自增强的编辑学习策略与专家编码模型，以加强控制信号与文本信号的协同。\n\n综上所述，本文研究从可控文本生成图像的技术出发，逐层推进从底层控制到高层控制的探索，针对图像质量、一致性及协同性这三大技术挑战展开了具体的解决方案。\n\n# 1.4 论文的结构安排\n\n本学位论文围绕可控文本生成图像关键技术这一研究内容，共分为六章进行展开，各章内容组织概述如下：\n\n第一章是绪论。介绍可控文本生成图像技术的定义及其与网络空间安全相关的研究背景与意义。之后本章简要回顾了可控文本生成图像的研究现状，总结了当前存在的问题和挑战，阐述了本学位论文的研究内容及贡献。\n\n第二章对可控文本生成图像技术近年来的发展进行了全面的总结。首先简要介绍了图像生成中的主流建模方法，然后描述了文本生成图像的发展历程以及在可控文本生成图像方向的代表工作和技术路线。最后介绍了可控文本生成图像方面代表性的数据集和评价指标。\n\n第三章研究了基于引导的布局结构可控文本生成图像技术。首先将背景布局信息引入布局生成中，并设计了基于transformer进行序列到序列的离散化布局补全生成算法，然后本工作引入了在大规模物体数据集预训练后的图像生成模型，并设计细粒度文本-布局交互模块，有效地将预训练物体生成器中单个物体的生成能力迁移到包含多个物体场景图像的生成过程中。通过背景布局提供的更全面的语义信息和单个物体生成的先验知识为图像生成的过程中提供全面引导，最终大幅提升图像生成的质量。\n\n第四章研究了基于学习的属性可控文本生成图像技术提出了首个基于学习的细粒度肤色可控文本生成图像方法，首先设计了基于自适应频率导向的颜色\n\n直方图，更加准确表示了用户的肤色信息，并且针对颜色统计量信息在文本生成图像模型尤其扩散模型中难以学习的问题，设计了颜色一致性匹配奖励的策略，显式地提升输入图像和输出图像之间的肤色一致性。最终大幅增强生成图像与参考图像间的肤色一致性，并且将该方案进一步地拓展到了发色控制任务。\n\n第五章研究了基于人脸特征精确表达的身份可控文本生成图像技术提出基于人脸特征提取器的多尺度、多令牌的人脸身份表示方法，首先利用人脸特征提取器中的多尺度特征去表示人脸身份图像中的底层细节与高层语义信息，提升生成人物身份的相似性，然后设计自增强的编辑学习方法，学习模型已有的对名人身份保持的文本生成图像能力，提升该表示的编辑性。基于精确表达的人脸特征，本文的方案在人脸相似性和编辑性指标上都有明显的优化。\n\n第六章是全文的总结和展望。总结了本学位论文的主要研究内容和成果，并对未来可控文本生成图像发展方向进行了讨论与展望。\n\n# 第2章 可控文本生成图像综述\n\n# 2.1 引言\n\n作为人工智能生成内容(AIGC)领域的重要组成部分，可控文本生成图像方向近年来得到广泛的关注，取得了跨越式的发展，陆续在我们日常生成生活中得到了实际应用。在本节中，我们系统性地介绍可控文本生成图像领域的发展及其代表性工作。首先介绍图像生成生成任务中的统计建模方法，其次梳理文本生成图像的发展历程，然后介绍了与本学位论文相关的三种可控文本生成图像的相关工作：包括位置可控性、属性可控性以及身份可控性。随后，介绍了评价指标和数据集。最后，对本章内容进行了总结。\n\n# 2.2 图像生成模型建模方法\n\n可控文本生成图像本质上属于一种生成模型，本节介绍被学术界广泛使用的几种生成模型及其基本原理，包括变分自编码器、生成对抗网络、自回归模型、扩散模型、非自回归模型以及修正流模型。\n\n# 2.2.1 变分自编码器\n\n变分自编码器 (Variational autoencoder, VAE)[32] 是一种生成模型，由 Kingma 和 Welling 于 2013 年提出。该方法属于概率图模型，结合了贝叶斯推理和神经网络，通过变分推断实现了对复杂数据分布的建模。\n\n![](images/9e5270a7113e54254c1f602087050fa30097cc24571fb293d790f07912f4b8e0.jpg)  \n图2.1 变分自编码器的基本结构\n\nVAE的基本结构如图2.1所示，由一个编码器和一个解码器组成。编码器将输入图像  $x$  映射到潜在空间  $Z$  的概率分布，解码器则从潜在空间  $Z$  中采样并生成新的数据  $\\hat{x}$  。具体而言，编码器学习到的是潜在变量  $z$  的均值  $\\mu$  和方差  $\\sigma^2$  ，即 $p(z|x) = \\mathcal{N}(\\mu (x),\\sigma^2 (x))_\\circ$\n\n由于直接对  $p(z|x)$  进行优化比较困难，VAE引入了变分推断的方法，通过\n\n优化变分下界来近似真实的后验分布。具体来说，VAE通过最大化变分下界来训练模型，目标函数如下：\n\n$$\n\\mathcal {L} (\\theta , \\phi ; x) = \\mathbb {E} _ {q _ {\\phi} (z | x)} [ \\log p _ {\\theta} (x | z) ] - \\mathrm {K L} (q _ {\\phi} (z | x) \\| p (z)) \\tag {2.1}\n$$\n\n其中，  $q_{\\phi}(z|x)$  是近似后验分布，通常选择为高斯分布；  $p_{\\theta}(x|z)$  是生成分布；KL项为Kullback-Leibler散度，用于衡量近似分布与先验分布之间的差异。\n\n为了确保变分下界是梯度可导的，VAE引入了重参数化技巧。通过将随机变量  $z$  表示为  $z = \\mu +\\sigma \\odot \\epsilon$  ，其中  $\\epsilon \\sim \\mathcal{N}(0,1)$  ，该模型可以进行端到端的训练。VAE的训练过程可以分为以下几个步骤进行：\n\n1. 使用编码器将输入数据  $x$  编码为潜在空间的均值  $\\mu(x)$  和方差  $\\sigma^2(x)$ 。  \n2. 通过重参数化技巧从潜在空间中采样  $z$ 。  \n3. 使用解码器生成重建数据  $\\hat{x}$ 。  \n4. 计算重建误差和KL散度，构建损失函数。  \n5. 通过反向传播算法优化模型参数。\n\n经过训练多次后，我们即可以通过在先验分布中进行采样来生成目标分布的图像。\n\n基于变分自编码器，研究者们进一步提出了向量量化变分自编码器技术(VQ-VAE)[48-49]，该技术通过离散化潜在空间中的表示，增强了生成模型的表达能力和训练稳定性，并在图像生成中得到了广泛应用。\n\n![](images/c762e9efca355ecdb0f121ea13d46a963df33e4d4a1f48cf670ab003f9418ca4.jpg)  \n图2.2 向量量化变分自编码器技术的基本结构\n\n如图2.2所示，向量量化变分自编码器技术的基本结构包括编码器、解码器和码本。编码器将输入数据  $x$  映射到潜在空间的表示，随后通过向量量化技术将该连续表示转换为离散表示。解码器从离散表示中生成重建数据  $\\hat{x}$ 。\n\n编码器将输入数据  $x$  映射到潜在空间中的连续表示  $z_{e}(x)$  。这一过程类似于传统的变分自编码器，但不再使用高斯分布来建模潜在空间。\n\n在VQ-VAE中，潜在空间中的连续表示  $z_{e}(x)$  通过最近邻搜索被量化为离散表示  $z_{q}(x)$  。具体来说，编码器输出的每个向量被替换为码本中最相近的向量：\n\n$$\nz _ {q} (x) = e _ {k} \\quad \\text {w h e r e} \\quad k = \\arg \\min  _ {j} \\| z _ {e} (x) - e _ {j} \\| \\tag {2.2}\n$$\n\n其中， $e_j$  是码本中的向量。解码器接收离散表示  $z_q(x)$ ，并生成重建数据  $\\hat{x}$ 。\n\nVQ-VAE的损失函数由重建误差、量化误差和码本更新项三部分组成：\n\n$$\n\\mathcal {L} = \\| x - \\hat {x} \\| ^ {2} + \\| \\operatorname {s g} \\left[ z _ {e} (x) \\right] - e \\| ^ {2} + \\beta \\| z _ {e} (x) - \\operatorname {s g} [ e ] \\| ^ {2} \\tag {2.3}\n$$\n\n其中，sg表示停止梯度（stop-gradient）操作，第一项为重建误差，第二项为量化误差，第三项为码书更新项， $\\beta$  是权衡系数。\n\nVQ-VAE的训练过程包括以下几个步骤：\n\n1. 使用编码器将输入数据  $x$  映射到潜在空间的连续表示  $z_{e}(x)$ 。  \n2. 通过向量量化技术，将连续表示  $z_{e}(x)$  转换为离散表示  $z_{q}(x)$ 。  \n3. 使用解码器生成重建数据  $\\hat{x}$ 。  \n4. 计算损失函数，包括重建误差、量化误差和码本更新项。  \n5. 通过反向传播算法优化编码器、解码器和码本的参数。\n\n后续相关工作[49-52]从模型结构、编码效率和生成质量等角度陆续进行了相应的改进。\n\n# 2.2.2 生成对抗网络\n\n生成对抗网络（Generative Adversarial Networks, GAN）[53]由Ian Goodfellow等人在2014年提出，是一种能够生成高质量数据的深度生成模型。GAN通过对抗训练的方式，使生成模型能够生成与真实数据难以区分的样本。近年来，GAN在图像生成领域得到了广泛应用。\n\n![](images/a742501273239f1079cca3ccc7669dba608ffd0527f67191fc926a98a337023a.jpg)  \n图2.3 生成对抗网络的基本结构\n\n如图2.3所示，GAN主要由生成器和判别器两个组件组成。生成器的目标是从随机噪声中生成逼真的数据样本，而判别器的目标是区分真实数据与生成数据。两者通过博弈的方式进行训练，最终达到一个平衡状态。\n\n生成器  $G$  接受一个随机噪声向量  $z \\sim p_z(z)$  作为输入，并输出生成的样本  $\\hat{x} = G(z)$ 。生成器的目标是最大化判别器对生成样本的错误判断概率，即生成器\n\n的损失函数定义为：\n\n$$\n\\mathcal {L} _ {G} = \\log (D (G (z))) \\tag {2.4}\n$$\n\n判别器  $D$  接受一个数据样本  $x$  作为输入，并输出该样本为真实数据的概率。判别器的目标是最大化对真实数据的正确判断概率，同时最小化对生成数据的错误判断概率，即判别器的损失函数定义为：\n\n$$\n\\mathcal {L} _ {D} = \\log (D (x)) + \\log (1 - D (G (z))) \\tag {2.5}\n$$\n\nGAN的训练过程是一个动态博弈的过程，生成器和判别器进行交替优化。生成器试图欺骗判别器，使其认为生成数据是真实的；而判别器则尽力识别生成数据和真实数据。具体的训练步骤如下：\n\n1. 固定生成器，更新判别器，使其最大化判别真实数据和生成数据的能力。  \n2. 固定判别器，更新生成器，使其生成的数据尽可能逼真，欺骗判别器。  \n3. 重复上述步骤，直到生成器和判别器达到平衡。\n\n由于动态博弈会导致不容易稳定训练的问题，后续的工作如  $\\mathrm{DCGAN}^{[54-55]}$  通过更优秀的网络结构设计， $\\mathrm{WGAN}^{[56-61]}$  通过对损失函数的改进，进一步提升GAN生成图像的质量。在模型结构设计上，主要包括经典的  $\\mathrm{BigGAN}^{[19]}$  结构和StyleGAN系列结构[62-64]。\n\n# 2.2.3 自回归模型\n\n自回归模型（Auto-Regressive Model, AR）是一类生成模型，通过逐步预测数据中的每个元素实现对整体数据分布的建模。自回归模型最初应用于时间序列数据的建模，近年来在图像生成领域取得了显著进展。自回归模型通过将图像像素或像素块视作一个序列，并对其进行逐步预测，从而生成高质量的图像。\n\n![](images/31795b497002b5ca19d36506252ed60942788eed04af4bbb7df99995d510e2d5.jpg)  \n图2.4 自回归模型的基本结构\n\n自回归模型的核心思想是将每个像素的生成条件化为之前已生成的像素，如图2.4所示。具体来说，假设图像由像素序列  $x_{1}, x_{2}, \\dots, x_{n}$  组成，自回归模型通过\n\n以下方式建模图像的联合概率分布：\n\n$$\np (x) = p \\left(x _ {1}, x _ {2}, \\dots , x _ {n}\\right) = p \\left(x _ {1}\\right) \\prod_ {i = 2} ^ {n} p \\left(x _ {i} \\mid x _ {1}, x _ {2}, \\dots , x _ {i - 1}\\right) \\tag {2.6}\n$$\n\n其中，  $p(x_{i}|x_{1},x_{2},\\dots ,x_{i - 1})$  表示在给定前面所有像素的情况下，生成当前像素的条件概率。\n\n自回归模型的具体实现方式多种多样，其中较为经典的包括PixelRNN、PixelCNN模型以及transformer模型。\n\nPixelRNN[65] 使用循环神经网络建模图像像素序列。它通过水平和垂直两个方向上的 RNN 来生成图像，每个像素的生成依赖于其左侧和上方的所有像素。PixelRNN 的主要优点在于能够捕捉长距离依赖关系，但由于 RNN 的序列性，生成速度较慢。\n\nPixelCNN[65] 使用卷积神经网络来实现自回归建模。通过引入掩码卷积，PixelCNN 确保每个像素仅依赖于其前面的像素。PixelCNN 在捕捉局部依赖关系和并行计算方面具有优势，生成速度较快。\n\nTransformer 模型在自然语言处理中的成功应用启发了其在图像生成领域的应用。基于 Transformer 的自回归模型, 如 Image Transformer 和 iGPT (Image GPT), 通过自注意力机制 (Self-Attention Mechanism) 建模图像像素之间的依赖关系。结合上一节中介绍的 VQVAE 方法对图像进行更紧凑的离散化编码后, 生成效率可以得到进一步提升[12]。\n\n自回归模型的训练过程包括以下几个步骤：\n\n1. 定义自回归模型结构。  \n2. 通过最大似然估计优化模型参数，使生成的像素序列概率最大化。\n\n这些自回归生成模型为逐像素生成图像提供了有效的途径，能够捕捉图像中的空间依赖关系，并为复杂图像的生成提供良好的基础。然而，由于逐步生成的限制，自回归模型在速度上相对较慢，尤其在高分辨率的图像生成任务中。此外，随着序列长度增加，生成过程中的误差累积也可能影响图像的整体质量。因此，如何在保持生成质量的同时提升生成速度，成为自回归生成模型的进一步优化方向。\n\n# 2.2.4 扩散模型\n\n扩散模型（Diffusion Model）是一类通过逐步向数据添加噪声并学习去噪过程来生成图像的生成模型。这类模型基于随机过程理论，利用马尔可夫链（Markov Chain）实现数据的逐步生成。近年来，扩散模型在图像生成、图像修复以及其他生成任务中表现出色。\n\n扩散模型的核心思想是通过一系列逐步增加噪声的步骤将数据转换为纯噪\n\n![](images/7ed0637f0324b852594073bf903205a5cac6a1eb869d3f549d74ef1af741c15f.jpg)  \n图2.5 扩散模型的基本结构\n\n声，再通过学习逆过程逐步去除噪声以恢复原始数据。扩散过程和去噪过程可以视为马尔可夫链的两个方向，如图2.5所示。\n\n前向扩散过程（Forward Diffusion Process）是指逐步向数据添加噪声的过程。给定一个真实数据分布  $x_0 \\sim q(x_0)$ ，前向过程定义为一个时间步长为  $T$  的马尔可夫链，每一步通过向数据添加高斯噪声进行扩散，如下所示：\n\n$$\nq \\left(x _ {t} \\mid x _ {t - 1}\\right) = \\mathcal {N} \\left(x _ {t}; \\sqrt {1 - \\beta_ {t}} x _ {t - 1}, \\beta_ {t} I\\right) \\tag {2.7}\n$$\n\n其中， $\\beta_{t}$  是每一步的噪声系数。\n\n逆向扩散过程（Reverse Diffusion Process）指通过去噪重建原始数据的过程。目标是学习一个模型  $p_{\\theta}(x_{t - 1}|x_t)$ ，使其能够从每一步去除噪声，从而恢复原始数据。该过程通过以下分布进行建模：\n\n$$\np _ {\\theta} (x _ {t - 1} | x _ {t}) = \\mathcal {N} (x _ {t - 1}; \\mu_ {\\theta} (x _ {t}, t), \\sigma_ {\\theta} ^ {2} (t) I) \\tag {2.8}\n$$\n\n其中， $\\mu_{\\theta}$  和  $\\sigma_{\\theta}$  是神经网络参数化的均值和方差。\n\n扩散模型的训练目标是最小化重建数据与原始数据之间的差异。通常采用以下形式的变分下界（Variational Lower Bound, VLB）作为损失函数，如下所示：\n\n$$\nL = \\mathbb {E} _ {q \\left(x _ {1}: T \\mid x _ {0}\\right)} \\left[ \\sum_ {t = 1} ^ {T} \\operatorname {K L} \\left(q \\left(x _ {t - 1} \\mid x _ {t}, x _ {0}\\right) \\| p _ {\\theta} \\left(x _ {t - 1} \\mid x _ {t}\\right)\\right) - \\log p _ {\\theta} \\left(x _ {0} \\mid x _ {1}\\right) \\right] \\tag {2.9}\n$$\n\n该损失函数通过最小化每一步KL散度及最终的重建误差来优化模型。\n\n# 2.2.5 其他生成方法\n\n非自回归生成方法。非自回归生成方法采用一种掩码机制来实现高效的并行图像生成。代表性的工作有谷歌研究团队提出MaskGIT[68]，该方法通过引入掩码预测方法，克服了传统自回归模型生成速度慢的缺点，同时保持了生成图像的高质量。\n\n如图2.6所示，与自回归模型逐像素生成不同，该方法一次性预测多个像素，从而显著提高了生成速度。具体来说，该方法首先对图像进行随机掩码，将一部\n\n分像素置为未知。然后预测这些掩码像素的值。预测完成后，更新图像并再次掩码，重复这一过程，直到所有像素都被预测出来。每次掩码的选择可以基于模型的不确定性或其他策略，以优化生成效果。\n\n![](images/d1d8183c5b2bff28f4d1f42917abfdcd54b5f4d30b648db9717c13affec776bc.jpg)\n\n![](images/fbdeba0b9a655e7ccd8afd83ba04a7f9254501574c34388477feab42810a9d73.jpg)  \n图2.6 非自回归模型的基本结构\n\n![](images/95814ee9a7d3d1acf3b9e7fe8989137a623484192e28a268de174c1950cc0e07.jpg)\n\n![](images/c90586689f88cb6ed960d94d411b8cdda8ddcca7a131c586ccb8b59bc9e5c463.jpg)\n\n修正流。修正流[69]的核心思想是通过一系列修正变换，将复杂数据分布映射到简单分布（如高斯分布），然后通过逆向变换生成数据。这些修正变换不仅包含标准的流变换，还包括额外的修正项，以捕捉更复杂的数据分布。\n\n# 2.3 文本生成图像模型演进\n\n文本生成图像技术旨在根据自然语言描述生成逼真且语义一致的图像。自任务提出以来，经过8年多的发展，如图2.7所示，早期的文本生成图像方法仅能生成模糊或不够真实的图像，且难以应对复杂的多物体场景。近年来，基于扩散模型[66]和预训练理解模型（如CLIP、BERT等）的生成方法极大推动了领域进展，使得生成的图像不仅具有逼真的视觉效果，还能精确地反映文本描述的细节。本节将从生成方法、模型结构以及数据增强等方向分别介绍文本生成图像领域的关键进展。主要阐述生成方法在建模图像分布上的演进，文本特征如何与图像特征进行交互以及如何构造一致性的图文数据。\n\n![](images/8b710c05f71afceef85e8580bea4dcb51ba4529037e2bb976bd28654e303b8c4.jpg)  \n图2.7 文本生成图像的效果进展\n\n![](images/44e18a9f243c3bb49df1c16b02ac0bb89d4a86d95da6b470df75cbd66dd74067.jpg)\n\n# 2.3.1 生成方法层面\n\n# 1. 基于生成对抗网络的文本生成图像\n\n文本生成图像技术旨在根据自然语言描述生成逼真且语义一致的图像。早期工作主要采用GAN[1]作为任务的基础生成模型。GAN-INT-CLS[13]是第一个提出文本生成图像的工作。如图2.8所示，其将文本特征表示与随机噪声进行级连，作为条件生成对应的图像。鉴别器侧同时考虑图像-文本对，如果是输入真实的图像及其匹配的文本则为真实的，而如果是真实的图像及其不匹配的文本，或者生成的图像和其对应的文本则是虚假的图文对。AttnGAN[4]引入了图文匹配\n\n![](images/d97571ee73d21c83c55fe5b9b426e138fbf90da041997ea0139988a8ab84b87a.jpg)  \n图2.8第一个基于生成对抗网络的文本生成图像方法GAN-INT-CLS介绍\n\n辅助损失，以提升图文的一致性。MirrorGAN[70]引入了图文描述模型以优化一致性，目标是希望由文本生成的图像经过图文描述模型后能够生成对应的文本。XMCGAN[71]通过对生成图像与真实图像进行对比学习来训练判别器，从而大幅提升图像生成的质量。随着后续DALL·E2的出现，自回归和扩散模型迅速成为大规模生成模型的新标准。这种快速转变引发了一个根本性问题：我们能否扩展GANs以从LAION等大型数据集中受益？GigaGAN[20]回答了这一问题，设计了一种全新的GAN架构，并证明了GANs在大规模场景下文本到图像生成中的可行性。GigaGAN提供了三个主要优势。首先，在推理时间上，其速度提高了几个数量级，仅需0.13秒即可合成一张512像素的图像。其次，它能够合成高分辨率图像，例如在3.66秒内生成1600万像素的图像。最后，GigaGAN支持多种潜在空间编辑应用，如潜在插值、风格混合和向量算术操作。\n\n# 2. 基于自回归模型的文本生成图像\n\nOpenAI公司在2021年2月提出的DALL-E[23]系统首次证明了自回归模型在图像生成领域的强大能力，通过增大数据规模和模型参数，基于向量量化编码和自回归模型的图像生成算法在MS-COCO[72]数据集零样本训练的情况下超过了其他模型在MS-COCO上训练的效果。这一结果揭示了模型规模扩展的重要性。同年的Cogview[73]进一步验证了这一点，其结果如图2.9所示。Wu等人[74]提出了一种名为NUWA的统一多模态预训练模型，该模型可以生成新的或编码现有的视觉数据（即图像和视频），用于各种视觉合成任务。为了在不同场景下\n\n同时涵盖语言、图像和视频，该工作设计了一个3D Transformer编码器-解码器框架，该框架不仅可以将视频作为3D数据处理，还可以分别适应文本和图像作为1D和2D数据。此外，还提出了一种3D邻近注意力（3DNA）机制，以考虑视觉数据的特性并降低计算复杂度。Make-a-Scene[75]拓展了控制的接口，设计了与人相关的向量量化编码方式，使得文生图的过程可以更加灵活，符合人类偏好。Parti[24]进一步证明了在自回归模型进行规划化实验的有效性。在MS-COCO数据上达到了当时最佳的性能。\n\n![](images/7cb83388db59305d13acf8599cb5ae397ccfbee162af1ebdee7f3e8bdffb1d7b.jpg)  \n图2.9基于自回归模型的文本生成图像方法Cogview介绍,图片来源于[73]\n\n# 3. 基于扩散模型的文本生成图像\n\n目前学术界和工业界应用最多的模型 Stable Diffusion 对应的算法 Latent Diffusion[27] 是在 2021 年年末提出的，该方法通过变分自编码器将图像压缩到下采样 8 倍的隐空间中，然后在该空间训练扩散模型，最后通过解码生成对应的图像。为了能提升图文一致性和图像生成的质量，该工作采用了 Classifier-Free Guidance[76] 策略进行推理。该方法进一步在更大规模的数据上进行训练，最终得到了 Stable Diffusion 模型。OpenAI 公司在 2022 年 3 月提出的 Glide[11] 系统，其零样本生成图像的能力超过了其之前提出的 DALL-E，并且由于 diffusion 迭代去噪的过程，训练得到模型原生就可以支持 in-painting/out-painting 等操作。为了加快训练速度和减小难度，该方法也采用了两阶段训练方式，第一阶段先生成  $64 * 64$  分辨率的图像，第二阶段再将该图像放大 4 倍，得到  $256 * 256$  大小分辨率的图像。谷歌公司提出的 Imagen[25] 进一步通过三阶段的训练方式，将图像分辨率增大到了  $1024 * 1024$ ，如图 2.10 所示。\n\nVQ-Diffusion[77]利用扩散模型生成离散的token，然后解码得到图像，加速了图像生成的过程，并且质量也较高。DALL-E2[26]将文本表示通过扩散过程转换为图像向量表示，然后再通过扩散模型将图像表示转换为具体的图像，该方法进一步提升图像生成的质量。百度公司提出的Ernie-vilg 2.0[78]系统针对多轮迭代的特性，允许生成模型在对不同时间步的参数不共享，增加了模型对复杂的建模能力。由于Transformer在视觉模型中强大的可拓展性，Pixart-alpha:[79]将\n\n![](images/6255dd568175b59adcde7d71dd98cbfc84d53a52c06ab39d1df7b4adb5bc6612.jpg)  \n图2.10 基于扩散模型的文本生成图像方法Imagen介绍\n\n![](images/10e749c5382dd2806e3453a9feef79c1de77bc84ab7afdb06deff2b9929f115d.jpg)\n\ntransformer结构引入到基于扩散模型的文本生成图像中，并利用在ImageNet数据上预训练的模型加快了训练的效率。Pixart-sigma[80]进一步通过高效令牌压缩策略将生成图像的分辨率增加到了4K。SD-XL[81]进一步扩大了Stable Diffusion模型容量和大小。并引入额外的分辨率和裁剪信息去有效利用更多的数据。同时在高质量、高分辨率数据上训练了一个额外的Refiner模型，以改善生成图像的细节。\n\n# 4. 基于其他生成方法的文本生成图像\n\n近年来，研究者探索了多种创新的生成方法，以提升文本生成图像（文生图）模型的质量和效率。传统的扩散模型虽生成质量较高，但通常需要较多的推理步数才能生成高质量图像。为解决这一问题，一些新方法开始引入更高效的生成框架，如修正流和非自回归模型，以显著减少推理时间并提升图像质量。\n\n基于修正流的生成方法。SD3[9]和Instaflow[82]采用了修正流（rectified flow）的生成方法来训练文生图模型。修正流的优势在于，它可以通过对传统流模型进行调整，使得模型能够更高效地生成样本。在修正流的框架下，模型在更少的推理步数内就能生成高质量的图像样本，从而有效地减少了生成时间，提升了生成效率。具体而言，修正流引入了一种流动约束，使得生成过程更加平稳，减少了样本生成过程中的不确定性。该方法不仅加速了生成过程，还在图像质量上取得了显著提升。\n\n基于非自回归模型的掩码生成方法。MUSE[83]则探索了利用非自回归模型进行掩码生成的方法。传统的自回归生成模型通过一步步生成序列，使生成过程\n\n较为耗时。相较之下，非自回归模型允许模型同时生成多个图像区域，显著减少了推理步数。MUSE采用了掩码生成策略，即在生成过程中使用掩码来逐步生成目标区域，以达到文本与图像的高效匹配。具体来说，MUSE将文本中的信息以掩码形式注入到生成器中，通过逐步填充掩码区域，实现了更高效的生成过程。实验结果表明，MUSE在保持生成质量的同时显著减少了生成时间，展现出有竞争力的生成效果。\n\n# 2.3.2 文本表征层面\n\n在文本生成图像的研究中，文本表征的选择对生成效果和模型的训练难度有着显著影响。早期工作通常采用非预训练的文本表示，直接针对文本生成图像（文生图）任务进行训练。然而，这种方法容易导致模型的文本理解能力不足，且增加了训练过程的难度。为解决这些问题，研究者逐步探索了基于图文匹配预训练的文本表示、基于大规模文本预训练的文本表示以及混合表征方法，从而优化文本生成图像的效果。\n\n基于图文匹配预训练的文本表示。AttnGAN[4]首次提出在文生图任务中使用图文匹配的预训练方法，以提升文本理解和生成一致性。具体而言，AttnGAN对训练数据集进行图文匹配预训练，并使用得到的DAMSM（Deep Attentional Multimodal Similarity Model）文本编码器，将文本特征准确地传递至生成器，从而显著降低了模型对文本学习的难度，提升了生成图像内容的语义一致性。随着CLIP[6]模型的出现，这种方法得到了进一步推广。CLIP在大规模数据集上进行图文匹配预训练，具备更强的文本-图像理解能力，成为了许多文生图任务中的标准文本编码器，大幅提升了生成图像的质量和一致性。\n\n基于大规模文本预训练的文本表示。除了图文匹配预训练，基于大规模文本预训练的文本表示方法同样取得了显著成效。XMCGAN[71]利用BERT[5]的强大文本理解能力，将其作为输入文本的编码器，使得模型能够更好地捕捉文本的语义信息。Imagen[25]进一步验证了仅在文本领域进行训练的大语言模型在文生图任务中的潜力。它采用了T5系列模型[84]，并发现随着文本编码器规模的增加，图像生成质量显著提升。这表明，基于大规模文本预训练的编码器不仅提升了文本语义的表达力，还对生成图像的细节呈现有积极作用。\n\n基于混合表征的文本表示。为进一步增强文本生成图像的表现，一些方法采用了混合表征策略，将图文匹配预训练模型和大规模文本预训练模型的优势结合。SD-XL[81]利用多种CLIP的文本表示，以确保生成过程对文本信息的多层次捕捉。SD3[9]则结合了CLIP和T5模型的文本表示，从而提升文本信息的多模态表达能力。通过这种混合表征，模型能够在生成过程中更精确地理解和表达复杂的文本描述，提高生成图像的细节和视觉一致性。\n\n# 2.3.3 模型结构层面\n\n级联结构。针对早期方法生成图像分辨率较低的问题，StackGAN[14-15,85]通过引入级联结构，逐层串联多个生成器和判别器进行分辨率递增的训练。该方法从低分辨率的  $64 \\times 64$  开始，逐步生成更高分辨率的图像，最终达到  $256 \\times 256$  的清晰度。通过这种逐层优化的训练策略，StackGAN有效解决了高分辨率图像生成的难题，生成图像的质量和细节表现得到了显著提升。\n\n布局结构作为中层表示。为了应对生成对抗网络（GAN）在生成包含多个物体的复杂场景图像时的表现欠佳问题， $\\text{InferGAN}^{[28]}$  和  $\\text{ObjGAN}^{[29]}$  提出了分阶段生成策略。这种方法将文本到图像的生成过程分解为两个子任务：首先生成图像的布局结构，然后再利用布局信息生成最终图像。具体而言，模型首先根据输入文本描述生成布局图，这一布局图作为中间层的图像结构表征，为后续图像的细节生成提供了参考。布局结构不仅包含了场景中各个物体的位置与大小信息，还捕捉了物体之间的空间关系，从而形成一个初步的场景语义表示。接下来，模型以此布局为基础，进一步生成符合布局的真实图像。在分阶段生成策略的引导下， $\\text{InferGAN}$  和  $\\text{ObjGAN}$  成功地将复杂场景的生成问题分解为较为简单的步骤，这种解耦式处理方法有效地降低了多物体场景生成的难度，并且显著提升了生成图像的语义一致性和图像质量。\n\n注意力机制。AttnGAN[4]引入了基于注意力机制的文本生成网络，通过在生成器中引入文本注意力模块，使得文本信息能够在生成过程中动态注入。这种设计不仅提升了生成图像的质量，还通过注意力热图提供了生成过程的可解释性。随后，DMGAN[7-8]进一步改进了这一框架，采用动态记忆机制更新文本表征，使得生成过程中各阶段的文本表示能够自适应地更新，以更精确地反映各阶段的生成需求。最近提出的SD3模型的MMDIT结构在实现文本-图像交互时采用了类似DMGAN的双向更新方法，进一步增强了文本信息对生成图像的精准控制。\n\n# 2.3.4 数据增强\n\n文本描述增强。RiFeGAN[86]通过检索输入文本相关的描述，增强当前文本的语义信息，进而提升图像生成的质量。这种方法通过引入外部相似描述扩展了输入文本的语义范围，使模型能够更好地理解和捕捉生成图像所需的复杂概念与细节。类似地，DALL-E3[87]针对训练数据中的文本噪声和简短描述问题，重新生成了训练图像的文本描述，以实现更高的图文匹配度。这种增强策略显著提高了生成模型的精确性，使生成图像更符合输入文本的意图，并减少了由于描述不充分而产生的模糊或不精确生成结果。这一策略现已成为大规模文本生成图像模型训练的默认选择，为生成高质量图像奠定了坚实的基础。\n\n检索增强。RE-IMAGEN[88]引入了图文检索机制，将语义上最相关的图像作为参考，结合输入文本生成目标图像。这种方法利用了图文匹配的优势，通过从大型图像数据库中检索出与文本最匹配的图像，作为生成图像的辅助参考，从而为生成过程提供了更加准确的语义和视觉指引。\n\n# 2.4 可控文本生成图像\n\n# 2.4.1 位置可控性\n\n由于文本在表示结构信息（例如位置和密集标签）方面具有挑战性，使用空间信号（例如布局[72]、人体姿态[89-90]、人体解析[91]和分割掩码[92-93]）来控制文本到图像的扩散方法，成为可控文本生成图像中的一个重要研究领域。首先，我们介绍生成对抗网络领域中在空间位置控制中比较有影响的系列工作，然后介绍在扩散模型中实现空间位置控制的两种方法：基于条件编码的方法和基于模型先验的方法。\n\n# 1. 生成对抗网络中的位置控制方法\n\n![](images/f47cca57db1ced199162b168709bab098ac1f6df584e8aaa33820839af75c5c3.jpg)  \n图2.11 pix2pix方法介绍, 图片来源于[94]\n\nIsola等人[94]提出了pix2pix框架,如图2.11所示,是GAN领域第一个实现位置控制的方法。作为在监督环境下进行图像到图像转换的通用解决方案,pix2pix的训练目标结合了条件GANs与生成图像和真实图像之间的逐像素  $\\ell_1$  损失和逐块判别器(PatchGAN)损失。PatchGAN尝试区分每个局部图像块,而不是整个图像。这个设计显著减轻了判别器的负担,因为区分局部图像块比区分整个图像需要的模型容量要少得多。后续Wang等人[95]通过一种新颖的特征匹配损失以及新的多尺度生成器和判别器架构生成了  $2048\\times 1024$  分辨率的图像。SPADE[96]发现以往的方法:直接将语义布局作为输入提供给深度网络,然后通过卷积、归一化和非线性层的堆叠进行处理这种方法并非最优,因为归一化层往往会“冲淡”语义信息。为了解决这个问题,该方法提出通过一种空间自适应的、可学习的转换来使用输入布局调节归一化层中的激活。在多个具有挑战性的数据集上的实验表明其优势。OASIS[97]提出了一种简化的GAN模型,只需要对抗性监督就\n\n能获得高质量的结果。具体来说，该方法将判别器重新设计为一个语义分割网络，直接使用给定的语义标签图作为训练的真实标签。通过为判别器以及生成器提供更强的监督，利用空间和语义感知的判别器反馈，该方法能够合成出与输入标签图对齐度更高、保真度更高的图像，从而无需使用之前的感知损失等辅助损失。\n\n# 2. 扩散模型中基于条件编码的方法\n\n![](images/35d44b9bac993b161b44d347ed36f766951d2190d9c7f94de3e74b5d9c9c914b.jpg)  \n图2.12 ControlNet方法介绍, 图片来源于[98]\n\nControlNet[98]是扩散模型中实现位置控制的奠基性工作，并于2023年获得了著名的Marr奖。如图2.12所示，该方法通过在主干网络UNet结构中引入一个额外的编码器副本，然后通过提出的“零卷积”层与原始UNet层相连，以避免灾难性遗忘。在后续的众多研究中，这一方法被广泛采用为基线方案[99-100]。类似地，T2I-Adapter[101]提出更加轻量化的分支，用于外部位置的控制。后续SCEdit[102]提出了一种名为SC-Tuner的高效生成微调模块。MCM[103]通过对文生图输出进行调制完成空间控制。\n\n由于 ControlNet 系列的工作需要为每种控制信号类型训练不同的模型, 一些\n\n研究者开始探究能够处理各种空间信号的更通用方法[99,104-105]。Qin等人[99]提出了一个任务感知的HyperNet，用于调节扩散模型以适应不同类型的条件。在这种方法中，条件通过专家混合（MOE）适配器进行编码。同时，任务指令通过任务感知HyperNet转换为任务嵌入向量，并集成到零卷积中，以精确调节各种不同的条件特征对主干网络的影响。\n\nSpaText[106]等工作进一步拓展了controlNet的控制接口，支持对局部区域进行描述。该方法首先通过引入  $\\mathrm{CLIP}^{[6]}$  图像嵌入来作为构建空间文本表示的代理表征。训练时，通过学习CLIP图像嵌入及物体形状为条件的图像生成，推理时，通过CLIP空间的文本表示到图像表示的先验网络，完成文本指令到CLIP图像嵌入的转换，从而实现细粒度的图像布局控制。\n\n以上方法主要针对密集的布局条件化得分预测领域，而在一系列使用方框定义布局的任务中，也产生了许多创新的方法[106-114]。其中，代表性工作GLIGEN[107]以基础语言为生成基础，通过门控机制将这些基础信息嵌入新的可训练层，从而实现更受控的生成。InteractDiffusion[112]则进一步引入图像间的交互关系作为条件。\n\n此外，除了通用物体外，一些研究专注于面部领域，在面部解析条件下合成面部图像[115-117]。其中部分工作[116-117]探索了多种条件协同控制下的图像生成方法。\n\n为了增强空间控制，一些方法探索了建模图像和布局信息的联合分布。Hyperhuman[118]提出了潜在结构扩散模型，将图像信息和布局信息进行级联，同时预测图像和布局信息。而JointNet[119]则设计了一个预训练文本到图像扩散模型的扩展新分支，用于单独预测布局信息。\n\n# 3. 扩散模型中基于模型先验的方法\n\n由于注意力机制能够明确建模文本和图像令牌之间的关系，因此通过调节注意力图来控制生成过程成为一种无需额外训练的有效技术，被广泛用于得分预测中，以便从模型中提取和利用先验结构信息[120-123]。这种方法通过调整注意力分布，在不增加额外训练负担的前提下，实现了对生成图像中对象位置、大小和布局等结构信息的精确控制，满足了多样化的生成需求。\n\neDiff-I[120]中提出的“paint-with-words”技术通过将每个词的交叉注意力图与相应的分割图对齐，使得用户可以通过文本指定图像中对象的空间位置和结构布局。这一方法在生成图像时，利用词语与对象的关联信息，通过注意力分布的调整实现了目标位置的精确控制，从而提升了生成图像的结构一致性。这种操作为用户提供了便捷的空间布局控制方式，使得生成过程更加直观且符合用户预期。\n\n此外，DenseDiffusion[121]进一步扩展了注意力图的调节能力，通过引入多\n\n种正则化策略，使得注意力热图在布局控制方面表现出更高的精确性和灵活性。DenseDiffusion的正则化方法确保了生成图像中每个元素的空间位置和布局更为准确，同时能够动态调整对象之间的距离、大小及相互关系。相比于传统注意力调节方式，DenseDiffusion不仅提高了布局控制的细腻度，还赋予了用户对图像细节的更多操控自由，使得系统在复杂场景下能够稳定地生成高质量的图像。\n\n通过这些技术的发展，调节注意力图已经成为控制生成图像布局的关键手段之一。这类无需训练的控制策略大幅提高了图像生成的可控性和用户的操作便捷性\n\n# 2.4.2 属性可控性\n\nGandikota等人[124]研究属性可控性的精细化处理。这能够在文本生成图像模型中进一步提升文本在属性层面的可控性。该方法识别出对应于单一概念的低秩参数方向，同时最小化对其他属性的干扰。使用一小组提示词或示例图像创建一个滑块；从而为文本或视觉概念创建滑块方向。概念滑块是即插即用的，可以高效组合并连续调节，从而实现对图像生成的精确控制。在与先前编辑技术的定量实验比较中，该方法表现出更强的目标编辑效果且干扰更低。该工作展示了天气、年龄、风格和表情的滑块，以及滑块组合。Attribute-Control[125]展示了在常见的CLIP文本嵌入中存在允许对主体特定属性进行细粒度控制的令牌级编辑方向，并且文本生成图像模型能够解释这些方向。T2I扩散模型能够通过仅在单个图像上使用重建损失目标，将高级语义概念反向传播到其文本嵌入输入，作为现有嵌入的适应。具体来说，该方法设计了两种用于识别描述特定概念的对比文本提示中的特定属性或概念方向的方法：一种是简单的无优化方法，另一种是更稳定的基于优化的方法。\n\nRen等人[117]探索了在人脸图像中进行属性控制方法，利用CelebA[126]数据中提供的40个0-1属性标签（如是否有刘海、是否微笑、是否戴眼镜等）作为条件，在文生图模型的基础上学习这些属性的控制。Composer[127]将图像颜色这一属性信息进行单独建模，通过LAB空间颜色直方图的形式表示，生成全局颜色可控的图像。Ge等人[36]提出使用支持字体样式、大小、颜色和脚注等格式的富文本编辑器，从富文本中提取每个单词的属性，以实现局部样式控制、显式令牌重加权、精确的颜色渲染和详细的区域合成。该方法通过基于区域的扩散过程实现这些功能，首先基于纯文本的扩散过程的注意力图获得每个单词的区域。对于每个区域，该工作通过创建特定区域的详细提示并应用区域特定的引导来强化其文本属性，并通过基于区域的注入保持其相对于纯文本生成的保真度。\n\n# 2.4.3 身份可控性\n\n身份可控性旨在生成同一身份的人类/物体在不同文本描述下的图像，需要同时满足人物的一致性和文本指令的编辑性两个要求。本节将从通用物体的身份可控性和人脸身份的可控性两个方向展开讨论。\n\n# 1. 通用物体身份的可控性\n\n该方向目前主要的技术路线有：1）基于微调的方法，通过调整模型参数或文本嵌入来学习物体的身份信息；2）基于编码器的方法，通过编码器提取身份条件并将其输入扩散模型；以及3）基于无需训练的方法，通过外部参考引导生成过程。\n\n![](images/1218276360e86a52871e9be7cf725edf75e65631db9270a3b9e6e95ca5b720f3.jpg)  \n图2.13 基于微调的身份控制方法DreamBooth介绍,图片来源于[42]\n\n基于微调的身份控制方法。该方法的基本思路是选择性地微调一部分原始文生图模型的参数，以在文本到图像模型中重建这些概念[40-42,128-129]。作为文本到图像扩散模型的基本输入，文本对身份控制起着至关重要的作用。奠基性的工作Textual Inversion (TI)[40]通过将用户提供的身份图像嵌入到文本嵌入空间中的新词中，实现了身份的可控性。该方法扩展了分词器的词典，并使用去噪过程对提供的图像进行额外的词优化。DreamBooth[42]采用了类似的思路，但利用低频词（例如，sks）表示身份图像，并通过先验保持损失更新UNet的参数，避免过拟合，以提升输出的多样性。其具体结构如图2.13所示。TI和DreamBooth的简单且有效的训练特性也成为后续工作的基线方案。此外，Custom Diffusion[43]分析了微调过程中权重的偏差，发现交叉注意力层参数，特别是键和值投影（即 $W^{k}$ 和 $W^{v}$ ）起到了更加重要的作用。因此作者在微调时只对这些投影层进行了更新。后续一些方法尝试扩展文本嵌入空间，例如，考虑到每个UNet层以及扩\n\n散模型不同生成时刻的区别[128?]，允许不同层或不同时刻可以学习不同的文本嵌入。[130]进一步将不同时刻的潜在表示与参考物体通过DDIM Inversion得到噪声轨迹特征相结合，以提升物体的一致性。\n\n参数高效微调(PEFT)[131-134]在个性化方法中起着至关重要的作用。低秩适应  $(\\mathrm{LORA})^{[132]}$  已广泛应用于多个个性化技术方案中[42,135-138]。此外，Xiang等人提出了ANOVA[139]，探索了新增适配器[131]来学习新的概念，并通过实验表明在交叉注意力块之后放置适配器显著提高了性能。为了促进PEFT在扩散模型微调中的全面应用和评估，LyCORIS[140]开发了一个开源库①。该库涵盖了广泛的PEFT方法，包括但不限于LORA[132]，LoHa和DyLORA[133]。LyCORIS还引入了一个详细的框架，用于系统分析和评估这些PEFT技术，极大地推进了扩散模型中身份可控领域的发展。\n\n此外，个性化领域的一个关键挑战是从提供的样本中解耦出与身份无关的概念。许多研究[135,141-144]发现，在定制过程中，杂散信息与目标身份交织在一起的常见问题，例如在基于主体的生成中无意中学习了图像的背景。为了有效地从样本中分离和提取关键身份信息，一些研究[141,145-146]尝试了使用显式掩码的方法。类似地，Disenbooth[135]和DETEX[142]设计专门的损失函数来减轻背景元素在个性化过程中的影响。DETEX更进一步，尝试将主体的姿势信息与整体概念解耦。PACGen[143]采用了激进的数据增强技术，通过改变物体在图像中的大小和位置，帮助将空间信息与核心概念本身分离。\n\n同时在小规模数据集上训练扩散模型往往面临另一个重大挑战：保真度和可编辑性之间的精细平衡[147]。为了解决这个问题，一些研究引入了保持机制，专注于防止对输入样本过拟合的策略[42-43,130,148-150]。例如，Perfusion[148]通过将概念的交叉注意力键锁定在其先前类别，并采用门控秩-1方法进行身份概念的学习。SVDiff[150]采用了一种不同的方法，调整模型权重矩阵中的奇异值。这一技术旨在最小化过拟合风险，并减轻诸如语言漂移等问题。此外，OFT[149]强调了在保持模型语义生成能力中，权重矩阵中超球面能量的重要性。因此，它引入了一种正交微调方法，进一步促进了模型在有限训练数据面前的泛化能力。\n\n除了上述方法外，一些研究人员还探索了其他训练技术，旨在优化生成性能，加速微调过程，并最大限度地减少GPU内存使用[151-152]。具体来说，DVAR[151]识别了标准训练度量在评估概念学习收敛性方面的局限性，并采用了一种基于简单方差的早期停止准则，增强了微调过程的效率。Gradient-Free Textual Inversion[152]采用了一种创新的方法，将优化过程分为两个部分：搜索空间的维度降低和子空间中的非凸、无梯度优化。这一方法在性能几乎不受影响的情况下，实现了显著的优化加速。\n\n基于编码器的身份控制方法在基于编码器的身份学习中，通过学习特定身份来实现图像生成控制具有显著优势。无需对每个个体进行耗时的微调过程，编码器方法在提高效率方面尤为突出。这些编码器方法中的一些专注于特定领域，旨在精确嵌入目标领域的特征。例如，InstantBooth[46]使用面向面部和猫等特定领域的专用编码器，配合训练过的适配器，从文本中提取嵌入信息并生成详细的补丁特征，从而实现概念学习。一些研究则在更广泛的开放世界图像上训练编码器，以便在开放环境中提取更广泛的条件和特征表示，例如  $\\mathrm{ELITE}^{[44]}$  、Unified Tuning[45]、Subject2Vec[153]等。这些方法通常利用诸如  $\\mathrm{CLIP}^{[6]}$  和  $\\mathrm{BLIP-2}^{[154]}$  等大型预训练模型作为图像编码器，并学习少量适配参数。例如，ELITE集成了基于CLIP的全局映射网络与局部映射网络，全局网络将层次化的图像特征转化为多个文本嵌入，而局部网络则将补丁特征注入视觉交叉注意力层中，从而在生成过程中保留更精细的图像细节。此外，Arar等人[155]提出了基于超网络的方法，能够直接预测微调所需的参数，从而进一步提升生成效率。而  $\\mathrm{SUTI}^{[153]}$  则受到学徒学习[156]的启发，采用了一种独特的方法，在数百万互联网图像集群上的大量专家模型上进行训练。然后，通过训练学徒模型模仿这些专家模型的行为，实现了对开放领域的广泛图像理解和概念迁移。最近的  $\\mathrm{CAFE}^{[157]}$  模型则基于大型预训练语言模型和扩散模型，构建了一个个性化的图像定制助手。该模型能够通过用户提供的少量定制需求生成个性化图像，体现了在控制信号和视觉细节间的高度协同。\n\n这些编码器方法的优势在于通过少量的适配参数微调或创新的网络架构，在节省计算资源的同时实现了对复杂视觉特征的保留和对控制信号的精确映射，为文本到图像生成任务中的多样性和个性化生成奠定了坚实的基础。\n\n无需训练的个性化得分预测。无需训练的个性化方法的核心在于利用基于文本生成图像（text-to-image）的扩散模型中的两个关键先验能力：1）扩散模型能够通过前向加噪过程获取每一步的带噪特征，这一过程赋予了模型在不同噪声水平下生成特征的灵活性。2）在生成过程中，每个自注意力模块可以结合来自参考图像的带噪特征，从而产生自注意力令牌特征，这使得生成的图像能够更加忠实地反映参考图像的视觉特征和细节。\n\n其中，代表性工作包括了StoryDiffusion和MasaCtrl[158-159]。StoryDiffusion利用带噪特征捕捉图像生成中的上下文依赖关系，从而生成包含多帧连续内容的故事情节图像。MasaCtrl则通过精细化的自注意力令牌特征控制，实现了对生成图像细节和风格的精准调控，满足了在特定个性化需求下的生成要求。\n\n这些方法通过直接利用扩散模型中的固有特性，避免了传统个性化模型所需的额外微调步骤，因而显著降低了个性化生成的计算成本。这种无需训练的个性化方法在生成效率与图像保真度之间达成了良好平衡，为不同场景下的快速\n\n个性化图像生成提供了更为高效的解决方案。\n\n# 2. 人物身份的可控性\n\n以人为中心的图像生成是图像生成中的重点任务。用户往往希望能够创作与参考人脸一致的图像。本部分重点介绍相关方案。\n\n![](images/c91576fdc21b1c378fa350f4e6aaa2e1a9ae9be2438384e4f0136c2f68de10df.jpg)  \n图2.14 基于编码器的身份控制方法Face0介绍,图片来源于[160]\n\n与通用物体身份一致性中的方案类似，如图2.14所示，许多基于人物的方法将面部图像编码为文本嵌入空间，以提供身份条件[160-162]。为了在身份保持和可编辑性之间取得平衡，Fastcomposer[161]提出了一种结合从个体参考图像中提取的视觉特征与文本提示的新方法。具体来说，该方法通过多层感知器将与人类相关的文本嵌入（例如，“男人”和“女人”）与视觉特征融合，有效地结合了人物身份的文本和视觉条件。 $\\mathcal{W} + \\text{Adapter}$ [163]则引入了一种使用基于StyleGAN[63]逆编码器的创新方法。\n\n与基于主体的方法相比, 基于人物的生成方法可以显著受益于面部分割, 这些分割可以通过解析模型或注释获得[161-162,164-166]。例如, 一些工作, 如 Stellar[165], 在数据处理中使用面部掩码消除背景元素, 从而更清晰地聚焦于输入数据中的人物身份。相反, 其他方法则利用面部掩码来构建[161-162,164,166]或调整损失函数[167]。\n\n# 2.5 评价指标\n\n# 1. 图像生成质量评估\n\nInception Score。Inception Score (IS)[168] 通过使用预训练的 Inception-v3 网络[169]对生成的图像进行分类，从而得到条件标签分布  $p(y|x)$  。如果网络能够生\n\n成有意义的物体，则条件标签分布应具有较低的熵。如果网络还能够生成多样化的图像，则其边缘分布  $\\int p(y|x = G(z))dz$  应具有较高的熵。IS同时评估了每张图像在分类中的真实性，以及生成图像整体的多样性。可以通过计算  $p(y|x)$  和  $p(y)$  之间的Kullback-Leibler(KL)散度来测量。然而IS指标存在明显的不足，例如，它无法检测过拟合，也无法衡量类内的多样性。因此，一个记住了ImageNet训练集或者每类只生成一个完美图像的网络，可能会获得非常高的IS。此外，由于Inception-v3模型是在单个物体数据上进行训练的，因此，它不太适合对包含多个对象的图像进行评估。\n\nFID。FID[170] 衡量了真实图像分布和生成图像分布之间的距离，具体来说，它通过预训练网络提取的特征来进行比较。相比于 IS，FID 是一个更有说服力的指标，并且能更好地捕捉到各种扰动。与 IS 类似，FID 使用预训练 Inception-v3[169] 模型的最后一个池化层的激活值来获取视觉特征。为了计算 FID，这些激活值被假定遵循多维高斯分布。真实数据和生成数据之间的 FID，其特征均值和协方差分别为  $(\\mu_r, \\Sigma_r)$  和  $(\\mu_g, \\Sigma_g)$ ，计算公式见公式 (2.10)。\n\n$$\n\\mathrm {F I D} = \\left\\| \\boldsymbol {\\mu} _ {r} - \\boldsymbol {\\mu} _ {g} \\right\\| _ {2} ^ {2} + \\mathrm {T r} \\left(\\boldsymbol {\\Sigma} _ {r} + \\boldsymbol {\\Sigma} _ {g} - 2 \\left(\\boldsymbol {\\Sigma} _ {r} \\boldsymbol {\\Sigma} _ {g}\\right) ^ {1 / 2}\\right) \\qquad (2. 1 0)\n$$\n\nFID对提取的特征假设其遵循高斯分布，但这一假设可能并不总是成立。此外，FID具有较高的偏差，因此需要相同数量的样本才能进行公平比较。 $\\mathrm{KID}^{[171]}$  是FID的一种无偏替代方法，但在每类样本数量较少时，其方差仍然较大。\n\n# 2. 图文一致性\n\n生成逼真的图像只是评价一个优秀文本生成图像（T2I）模型的一个方面，另一个重要的评估特性是生成的图像是否与输入的文本描述语义一致。上述讨论的指标无法衡量生成图像与输入描述的匹配程度。我们将在此介绍常用的R-precision[4]、视觉语义相似度（VS)[85]，以及语义物体的准确性（SOA)[172]。\n\nR-precision。R-precision[4]通过提取图像和文本特征之间的检索结果排名来衡量文本描述与生成图像之间的视觉语义相似度。具体而言，除了生成图像的真实标签外，还从数据集中随机抽取额外的标签。然后，计算图像特征与每个标签的文本嵌入之间的余弦相似度，并按相似度递减的顺序对标签进行排序。如果生成图像的真实标签在前  $r$  个标签中排名，则计为成功。在默认设置中，R-precision通过设置  $r = 1$  并随机抽取99个额外标签来计算。换句话说，R-precision评估生成的图像比99个随机抽取的图像更接近输入的文本描述。\n\n值得注意的是，与CUB-200 Birds[173]数据集上的分数相比，最先进的模型在COCO数据集上实现的R-precision通常更高。这是因为与CUB-200 Birds相比，COCO数据集中的图像和标签更加多样化，使得区分对应标签和随机标签变得更加容易。\n\n视觉语义（VS）相似度。VS相似度由Zhang等人[85]提出，通过计算生成的图像和文本之间的距离来衡量合成图像与文本的对齐程度。该方法使用预训练的视觉语义嵌入模型。具体来说，通过分别学习两个映射函数，将图像和文本映射到一个共同的表示空间中。然后通过公式(2.11)计算相似度，其中  $f_{t}(\\cdot)$  是文本编码器， $f_{x}(\\cdot)$  是图像编码器。\n\n$$\n\\mathrm {V S} = \\frac {f _ {t} (t) \\cdot f _ {x} (x)}{\\left| \\left| f _ {t} (t) \\right| \\right| _ {2} \\cdot \\left| \\left| f _ {x} (x) \\right| \\right| _ {2}} \\tag {2.11}\n$$\n\n早期的算法主要采用R-precision来度量图文匹配程度，采用VS相似度工作较少，后来随着CLIP模型的提出，大家往往采用CLIP-score[174]，即CLIP作为编码器的VS相似度，作为主要的定量指标。\n\n图像描述度量。为了衡量生成图像与文本的相关性，Hinz等人[172]使用图像描述生成器[175]为生成的图像生成描述，并报告标准的语言度量，例如BLEU[176]，METEOR[177]和CIDEr[178]。生成的描述应与用于生成图像的输入描述相似。假设是这些代理度量更倾向于生成反映输入描述含义的图像的模型。然而，可能存在完全不同的描述能够正确描述同一图像。由于这些度量依赖于n-gram重叠，因此可能与人的判断不相关[172]。\n\n语义物体准确性。Hinz等人[172]提出了SOA指标，使用预训练的对象检测器来评估图像中描述中明确提到的物体是否在生成的图像中出现。例如，我们可以从描述“狗坐在沙发上”中推断出图像应包含一个可识别的狗和一个沙发，因此图像检测器应能够检测到这两个对象。更具体地，他们提出了两个指标：SOA-C表示每个类别中检测到给定对象的图像数量，而SOA-I表示在多少图像中检测到所需的对象。作者为每个标签构建了一个有效词列表和一个包含排除字符串（例如，将“热狗”排除在“狗”之外）的列表。尽管SOA基于描述中提到的词汇，但它假设了相当客观且严格的描述，其中描述大致是可见对象的词列表。因此SOA可能不太适合评估对象之间的含义、交互和关系，以及可能存在的主观性。\n\n一致性综合指标。T2i-CompBench[179]系统性地评价了图文一致性中的3个大类：属性绑定、物体关系、复杂的组成问题。该基准可以进一步细化为六个子类：颜色绑定、形状绑定、稳定绑定、空间关系、非空间关系及复杂组成，用以便深入探讨不同类型的图文一致性挑战。\n\n# 3. 人类偏好指标。\n\nImageReward[10]是当前最大的通过收集人类偏好数据来评估图像生成系统的项目。该项目的核心在于建立一套基于人类偏好的评分标准，以便为生成的图像分配更符合用户审美和需求的评分。通过分析用户偏好，ImageReward为后续图像生成模型的优化提供了可靠的数据基础，推动了生成图像质量的提升。\n\n在 ImageReward 之后， $\\mathrm{HPS}^{[180]}$  项目进一步细化了偏好数据的收集方式，聚焦于 Discord 用户的图像偏好数据。HPS 通过微调预训练的 CLIP 模型来学习图像与文本之间的偏好关系，从而使得模型能够根据不同用户的需求偏好更精准地生成图像。HPS-v2 在此基础上扩展了数据来源的多样性，不仅包含了更多样化的提示词（prompt），还引入了多种图像生成模型的数据，提升了偏好模型的泛化能力，使其在面对不同生成任务时能够更有效地满足用户需求。\n\nPickScore[181]开发了一种开放的图像比较平台，用于收集大量的图像对比数据。这种平台化设计使得研究者可以在同一平台上进行大量的图像偏好数据采集，形成了一个共享资源，为社区提供了宝贵的比较数据。此外，PickScore平台使用户能够对不同生成模型和生成策略下的图像进行直接对比，促进了生成模型在多维度偏好理解上的改进。\n\n快手公司提出的  $\\mathbf{MPS}^{[182]}$  项目则进一步细化了图像偏好评估的维度，将图像对比扩展到多个具体维度，如细节质量、美学效果和图文对齐程度等。MPS通过对比生成图像在这些不同维度上的表现，为生成模型提供了更加精确的优化方向，使其在生成结果的细节丰富性和与文本描述的关联性上都得到了增强。\n\n此外，Liang等人[183]的研究进一步引入了图像级别和单词级别的精确标注，以捕捉更细致的偏好信息。他们的工作通过标注图像中的瑕疵区域和文本中的单词匹配情况，使得模型能够识别并避免在生成过程中出现视觉或语义上的错误。这种细化的标注方式在偏好数据中提供了丰富的细粒度信息，有助于生成模型在处理复杂文本提示和多物体场景时实现更高的表现精度。\n\n这些研究项目通过收集和分析用户偏好，逐渐建立起了评估图像生成质量的多维度基准，不仅促进了生成模型对用户需求的适应性，还推动了图像生成系统在个性化和细节控制方面的不断进步。\n\n# 4. 其他指标\n\n除以上指标外，Lee等人的工作[184]中还系统性地度量了多种评估指标，进一步丰富了对图像生成系统的整体性能理解。这些指标涵盖了生成图像的美学质量、系统对通用知识的理解能力、刻板印象倾向、生成效率以及多语言处理能力等关键方面，为评价生成模型在复杂多样的应用场景中的表现提供了更全面的视角。\n\n首先，美学质量是图像生成的重要评估指标之一。Lee等人提出的美学度量评估了生成图像在视觉上是否具有吸引力，包括色彩搭配、构图比例、清晰度等因素，以确保生成结果符合用户对艺术性和视觉美感的需求。这一评估为高质量的图像生成提供了重要参考，尤其适用于需要高审美水准的创意和设计领域。\n\n其次，通用知识的理解能力是生成系统在表达常识性和逻辑性方面的重要衡量标准。该指标评估了模型在生成过程中是否能够准确表达日常场景、概念关\n\n联和知识性细节，以确保生成图像不仅在视觉上逼真，还具备合理的逻辑和现实关联。这一能力尤为重要，因为它直接影响了图像生成系统在真实世界中应用的可靠性和可信度。\n\n此外，刻板印象倾向的评估帮助研究者理解生成模型在性别、种族、职业等方面是否存在偏见。这一指标通过检测生成图像中的潜在偏见，为系统开发提供了重要的反馈，从而确保图像生成模型符合多样性、公平性和包容性的要求，降低了潜在的社会负面影响。\n\n生成效率则是衡量系统在资源消耗和生成速度方面的性能标准。生成效率的提升使得生成系统能够在有限的计算资源下高效运行，满足实时应用场景的需求，并进一步拓展了系统在工业、娱乐和多媒体创作等领域的适用性。\n\n最后，多语言理解能力是评价生成模型在多语言环境中表现的关键指标。Lee等人的研究通过对系统生成不同语言描述图像的准确性进行度量，验证了系统在处理不同语言和文化场景的适应性。多语言能力的提升使得生成系统更具全球化应用潜力，以满足多元化用户需求。\n\n# 2.6 数据集\n\n数据集是每个机器学习问题的核心，直接影响模型的表现和泛化能力。在文本生成图像研究中，广泛使用的基准数据集包括Oxford-102 Flowers[185]、CUB-200 Birds[173]和MS-COCO[72]。这些数据集在规模、类别多样性和图像复杂性方面各有不同，适用于不同层次的生成任务需求。\n\nOxford-102 Flowers 和 CUB-200 Birds 是相对较小的数据集，分别包含约 10,000 张图像，主要集中于单一类别的物体（花朵和鸟类），并为每张图像提供十个描述标签。这些数据集的图像通常背景简单，物体占据主导地位，适合用来研究单一物体的生成和属性控制。因此，这类数据集为研究视觉细节、纹理生成和简单布局提供了良好的测试环境，但其有限的图像类别和简单场景使得模型在更复杂的生成任务中难以展现完整能力。\n\n与之相比，MS-COCO数据集具有更高的复杂性和多样性，包含约123,000张图像，每张图像均附有五个描述标签。COCO中的图像通常包含多个不同类别的物体，并且物体之间常常相互作用，场景布局更为复杂。相较于Oxford-102Flowers和CUB-200Birds，COCO提供了多物体场景和多样化背景的训练数据，非常适合用于研究模型在复杂场景下的生成效果。这一特性使得MS-COCO成为测试生成模型场景理解和布局控制能力的重要基准。\n\nZhou等人[186]设计的小规模面部图像数据集专门面向文本到面部生成和文本引导的面部图像处理任务，具有丰富的面部属性标签。该数据集专注于细粒度\n\n的面部特征，为研究基于文本的面部属性控制和面部表情生成提供了一个高质量的资源。\n\nMulti-Modal CelebA-HQ[187]是另一个面向文本引导的图像生成任务的重要数据集。它包含了30,000张高质量图像，并且每张图像对应了10个文本描述。此外，该数据集包含了多样化的标注信息，如分割掩码和草图，使得研究者可以在同一数据集上实现多模态融合的实验，例如结合分割和草图进行更精细的面部生成和编辑。\n\nDeepFashion-MultiModal[188]则是一个专为时尚相关的文本生成图像任务设计的数据集，包含了丰富的多模态标签信息，如人体解析标签、关键点、细粒度属性和文本描述。这些标注信息有助于模型学习服饰、人体姿态和细节纹理的生成，使其在文本生成服装或人体图像的任务中表现更加出色。\n\nDiffusiondb[189]是大规模文本到图像生成数据集中的先驱，包含了1400万张图像。这些图像是通过Stable Diffusion使用真实用户提供的提示和超参数生成的，为研究者提供了丰富的生成任务数据。这种用户生成的数据集能更真实地反映出实际应用中的多样化需求，为模型适应用户输入的多样性提供了极大的支持。\n\nLaion-5B[190]是当前全球最大的公开图文对数据集，包含了58.5亿图文对。该数据集的庞大规模为训练大规模文本生成图像模型提供了充足的数据支持，尤其适用于预训练阶段，有助于提升模型在各种任务中的泛化能力。\n\n类似的，coyo-700m[191]也是一个大规模的数据集，包含了7亿个图文对。作为开放数据集，coyo-700m为各种多模态任务提供了一个庞大的基础，不仅支持文本生成图像任务，还适用于跨模态理解、图像检索等多种应用。\n\n这些数据集的丰富性和多样性为文本到图像生成的不同研究方向提供了支撑，使得模型在面部生成、时尚生成以及大规模开放图文对生成等领域的应用成为可能。\n\n# 2.7 本章小结\n\n本章介绍了可控文本生成图像的发展历程，首先介绍了目前生成建模的常见方法，然后探讨了文本生成图像领域的改进路线，接着介绍了文本生成图像领域中的位置可控性、属性可控性以及身份可控性的相关工作。最后，本章还介绍了评价指标和数据集。\n\n尽管可控文本生成图像技术已经取得了显著进展，但在生成的图像层面仍然存在一些问题，例如真实感不足、图像细节缺乏以及人物肢体结构容易崩坏等。在可控性层面，身份可控性除了一致性的要求外，还涉及对参考物体的编辑\n\n的要求。这方面的技术尚不成熟，仍然面临着身份一致性和文本一致性的双重挑战。为了进一步提升图像生成的质量，未来的研究可以探索更先进的建模方法以及更大规模的图文预训练。为了在可控性方面取得突破，可以通过探索原生多模态的生成架构来减少图文冲突，并借助文本到视频模型中的3D一致性和运动一致性，探索其在可控文本生成图像中的应用。\n\n# 第3章 基于引导的布局结构可控文生图\n\n图像的布局和物体的位置分布直接影响观众的视觉焦点与信息传达效果。通过设置特定的布局结构，可以让观众的注意力自然地集中在关键人物/事件上。然而，仅仅依靠文生图模型，只能实现如“人物在图像的左边/右边”等粗略的控制，无法实现精确的位置布局控制。本章进行精细化布局结构可控的研究，使得在文本生成图像时，用户可以通过控制物体包围框(bounding box)实现物体位置的精确可控。\n\n现有位置可控的文本生成图像相关工作在简单的图像上的生成效果已经取得了很大的进展。然而，当生成复杂场景图像时，其质量往往较差，具体体现为：1）生成的图像背景（如围栏、湖泊）在复杂的真实场景中往往不真实；2）生成图像中的物体（如大象、斑马）通常呈现出严重的形状扭曲或关键部位缺失。为了解决这些局限性，本研究提出了一种新颖的两阶段方法，其中，第一阶段重新设计了文本到布局的过程，将背景布局与现有的物体布局结合起来；第二阶段将现有的类别到图像模型中的物体知识转移到布局到图像的生成过程中，以提高物体的保真度。具体来说，本研究引入了基于Transformer的架构作为布局生成器，用于学习从文本到物体和背景布局的映射，并提出了一种细粒度文本-布局交互模块（TL-Norm），以自适应地将物体知识转移到图像生成中。得益于背景布局和转移的物体知识提供全面引导，本研究所提出的方法在图像质量指标上显著超越了以前的最新方法，并且在图像-文本对齐以及位置可控性上也取得了卓越的表现。\n\n本章第3.1节主要介绍了现有位置可控性的相关工作因为缺乏引导信息而无法生成合理的场景图像的问题，并阐述了本研究通过背景布局建模和预训练物体生成网络来提供引导信息的动机。第3.2节对相关工作进行的梳理。第3.3节介绍了具体的算法设计，其中第3.3.1部分介绍了基于Transformer的序列到序列的背景布局生成方法，以对布局结构进行完整且精确的建模。第3.3.2节介绍了基于物体知识迁移与细粒度文本-布局交互的图像生成，以提升对复杂场景图像的生成质量。第3.4节通过广泛的定量和定性实验验证了所提方法的有效性。第3.5节对本章进行了总结。\n\n# 3.1 引言\n\n文本生成图像(T2I)是计算机视觉和跨模态分析中的一项重要任务。该任务旨在根据自然语言描述生成真实且语义一致的图像。通过允许用户使用自然语言描\n\n述视觉概念，T2I任务为条件图像生成提供了一种自然且灵活的接口[186,192-193]。文本生成图像的中位置可控性成为近年的研究重点。[28-30]\n\n近年来，基于生成对抗网络（GAN）的方法在位置可控的文本生成图像任务中显示出令人鼓舞的成果。由于用户指定的布局往往都不够全面以及有时希望能够提供系统能够提供预设的布局，一般是通过两阶段方案来解决位置可控的文本生成图像任务，第一个阶段是文本到布局的生成，第二个阶段布局到图像的生成。本研究发现，现有方法的成功主要局限于简单的数据集，如鸟类[173]和花卉[185]，而生成复杂的、真实世界的图像（如MS-COCO[72]）仍然是一个巨大的挑战[28]。如图3.1所示，从左到右依次为：输入文本、相关工作Obj-GAN[29]的结果、相关工作SSA-GAN[194]的结果以及本方案的结果。输入文本中对应的物体和背景区域分别用蓝色和橙色矩形框标出。可以观察到，当前模型往往会生成不现实的背景（如fence、lake和metal structure）以及扭曲的物体（如elephant、giraffe和zebra）。\n\n![](images/11554a196039e66e384c91b988e0b44ce3badb5e6febd384e54103db1f05ed95.jpg)  \n图3.1 当前位置可控的文本生成图像的相关方法生成图像存在的问题\n\n首先，当前方法往往建模的是以物体为中心的布局，两阶段方法尽管能够生成比相较端到端文本生成图像的方法能够生成更真实的以物体为中心的图像。然而，这些方法仍然忽视了背景相关的布局，而背景名词在文本描述中占  $38.2\\%$  的名词，并在图像中占  $70\\%$  的区域[195]。如图3.1所示，当前仅基于物体布局的方\n\n法在文本描述中丢失了大量背景信息。例如，给定文本描述“an elephant standing alone on a cement floor behind a wired fence”，生成的图像中几乎看不到背景名词fence(栅栏)。因此，可以考虑将背景纳入以前景物体为主的布局建模中，以提高生成图像的综合质量。\n\n其次，目前的方法在生成物体时也存在困难（如图3.1中的zebra(斑马)），容易出现变形、难以识别的问题，其原因主要在于可控文本到图像生成任务中广泛使用的COCO-14数据集的数量比较有限。该数据集的训练集是由80,000张图像组成，远少于包含超过1,000,000张图像的ImageNet[196]数据集。因此，有限的成对图像-文本数据阻碍了可控文本生成图像模型对多样物体的知识学习。但直接收集更多带有区域标注的成对图像-文本数据的成本是巨大的，因此，探索其他图像生成模型作为潜在的物体知识来源，成为缓解生成图像中物体扭曲的一种自然选择。当前基于大规模预训练的类别到图像生成模型可以在给定物体类别（如ImageNet中的1000个类别）的情况下生成真实图像。而从另外一个视角来看，类别到图像的生成过程也可以被视为布局到图像生成过程的简化版本，即生成的目标图像中只有一个物体存在。因此，本研究探索了将预训练的单物体图像生成模型的知识转移到文本生成图像中，以促进复杂图像中每个物体的生成。\n\n总的来说，为了解决上述背景不真实和物体扭曲的问题，本研究提出了一种新的两阶段位置可控的文本生成图像方法，称为背景与物体生成对抗网络（BO-GAN），以全面提高背景和物体区域的生成质量及位置可控性。该方法设计了两个新子模型：1）在背景布局生成器方面：重新设计了文本到布局阶段，并采用基于Transformer的架构，预测将背景与物体结合的全面布局；2）在图像生成环节，设计了物体知识转移模块，利用来自单物体图像生成模型的物体知识，以促进布局到图像阶段的生成。实验表明，通过结合这两个组件设计而成的BO-GAN能够生成复杂多样的真实世界图像。\n\n为验证提出的BO-GAN在位置可控的文本生成图像中的优越性，本研究在最广泛使用的基准数据集（COCO-14）上进行了相应实验。定量和定性实验结果表明了提出的BO-GAN在生成真实世界图像方面的有效性。BO-GAN在图像生成指标FID上相比比最新的先进方法改进了  $41.3\\%$  ，在图像-文本对齐度指标上提升了  $40.3\\%$  ，在位置控制性指标上提升了  $68.9\\%$  。\n\n本研究的贡献如下：\n\n- 本研究提出了一种位置可控的文本生成图像方法：背景与物体生成对抗网络 (BO-GAN)。该方法有效提升了现有方法在背景和物体区域的生成质量。  \n- 为了缓解生成复杂场景图像的难度，本研究设计了一种基于Transformer的序列到序列的模型来生成包含背景的布局，从而指导文本到复杂布局的生成过程。\n\n- 本方法表明，通过转移其他图像生成任务中的知识，可以有效提高在有限成对图像-文本数据下物体的生成质量。  \n- 在广泛使用的文本生成图像数据集上，提出的方法在图像质量指标上大幅超越了之前的方法，并实现了当时最优的图像-文本对齐性能。\n\n# 3.2 相关工作\n\n# 3.2.1 文本生成图像\n\n文本生成图像任务受到了视觉和语言研究社区的广泛关注。基于最小-最大博弈学习的优雅设计，GANs[16,70]被广泛应用于这一任务。GAN-INT-CLS[13]是第一个采用条件GAN进行文本生成图像的工作。StackGAN和StackGAN++[14-15]通过堆叠架构逐步优化先前生成的低分辨率图像或特征，提高了生成图像的分辨率。AttnGAN[4]引入了基于网格的词级注意力机制来生成细粒度图像，并提出了一种图像-文本匹配损失（DAMSM损失），以约束生成图像与输入文本之间的语义一致性。DM-GAN[7]利用动态记忆模块自适应调整词级特征。SD-GAN[16]引入了一种语义条件批归一化技术，以更好地融合文本特征。DAE-GAN[17]结合了方面信息以生成图像细节。SSA-GAN[194]通过弱监督的方式学习软掩码图，以便在不同位置自适应地注入不同的文本信息。这些方法在细粒度图像生成方面取得了显著成果，如鸟类[173]和花卉[185]。然而，生成复杂的、真实世界的场景图像（如MS-COCO[72]）仍然具有挑战性。\n\n# 3.2.2 位置可控的文本生成图像\n\n一般来说该任务将图像生成任务分解为两个步骤：文本到布局和布局到图像的生成[28-30]。Hong等人[28]首先通过LSTM模型预测物体布局，然后通过生成新的边界框来优化物体布局。Obj-GAN[29]通过探索物体级注意力和物体级判别器来提高图像质量。R-GAN[30]进一步引入了结构化文本表示，用于预测更准确的边界框，并设计了形状记忆库用于形状生成。本研究工作属于这一范式。然而，为了生成真实的背景，本方案不仅生成物体布局，还生成包含背景布局的综合语义布局（如草地、天空、窗帘）。此外，本研究提出了一种物体知识转移方法，以提高物体生成的质量。通常，文本生成图像任务被分解为两个步骤：文本到布局的生成和布局到图像的生成[28-30]。Hong等人[28]使用LSTM模型预测物体布局，通过生成新的边界框来优化布局，以提高生成的空间一致性和布局合理性。Obj-GAN[29]则在生成过程中引入了物体级注意力和物体级判别器，以增强对每个物体的细节控制和整体图像质量。R-GAN[30]进一步发展了这一范式，引入结构化文本表示来生成更准确的边界框，并设计了形状记忆库以提高物体形\n\n状生成的准确性和多样性。本研究工作也采用了这一范式，但在布局生成过程中进一步优化了背景布局的生成策略，以生成更加真实的场景。本方案不仅生成物体布局，还生成包含背景布局的综合语义布局，例如草地、天空、窗帘等大区域背景信息，以增强图像的环境感和层次感。此外，本研究提出了一种物体知识转移方法，从大规模预训练的图像生成模型中迁移物体生成知识，通过预训练的知识提升生成物体的质量和细节表现。这样不仅改善了物体的生成质量，还在复杂背景和多物体场景中实现了更高的布局一致性和视觉效果，使生成的图像更加符合文本描述的要求。\n\n# 3.2.3 文本到布局生成\n\n之前的所有文本到布局生成方法[28-30]均采用基于LSTM的模型，通过分类分布建模类别概率，并使用多变量高斯混合模型来建模位置和大小信息。具体来说，该模型首先预测物体的边界框标签，然后生成边界框的左上角位置和尺寸。实验表明，这种基于LSTM的布局生成方法在仅包含前景物体的情况下能够生成较高质量的布局，并支持具有竞争力的图像生成效果。然而，当布局中包含背景布局时，本研究发现基于LSTM的模型在生成合理的综合布局方面存在局限性。背景布局的加入对生成模型提出了新的挑战，因为背景元素在布局中具有大面积连续性，且通常与多个前景物体存在复杂的空间关系。而基于LSTM的方法建模能力较弱，在处理背景元素与前景物体的全局一致性和空间协调性时表现不足，往往导致生成的布局不合理，无法准确反映文本描述中的场景关系。\n\n# 3.2.4 布局到图像生成\n\n为了从布局生成真实的图像，当前主流的方法首先根据布局生成语义掩码，然后从语义掩码生成图像。Layout2Im[31]通过conv-LSTM生成掩码，然后利用类似VAE的损失来函数建模每类的外观。LostGAN-V1[33]首先分别预测每个边界框的掩码，然后形成整体的语义掩码。语义掩码通过ISLA-Norm层注入到掩码到图像生成器中。LostGAN-V2[197]进一步通过集成从不同阶段的特征图中学习到的掩码来逐步优化掩码。\n\n# 3.3 算法设计\n\n如图3.2所示，所提出的方法包括两个步骤：布局生成及图像生成。首先，为了为背景生成提供准确而具体的空间语义指导，设计了基于Transformer的seq2seq模型生成背景感知的布局。然后，为了缓解物体扭曲问题，引入了细粒度文本-布局交互模块TL-Norm，以有效地将预训练的类别到图像模型中的物体知识转移\n\n![](images/cc083d9c82edfbbbb137825112f4fd5a3765236bed55dc629b231f50b29b2777.jpg)  \n图3.2 BO-GAN的整体流程\n\n到布局到图像的生成过程中。\n\n# 3.3.1 基于Transformer的序列到序列的背景布局生成\n\n由于布局可以被定义为一个离散的序列，文本到布局的生成可以视为一个序列到序列的生成过程。然后，采用基于Transformer的模型来学习从文本到物体和背景布局的转换过程，如图3.3所示。\n\n文本表示。由于从图像中学习到的文本嵌入可能包含对布局生成无用的信息，如颜色信息等，与之前的工作不同[29-30]，本工作没有使用来自图像-文本匹配任务的预训练文本编码器进行布局生成，而是使用了初始化参数的Transformer编码器来进行文本表示。这样学习学习更好的布局相关的文本表示，排除布局无关的信息的干扰。文本的表示根据布局生成的损失进行端到端的优化。\n\n文本到布局生成器。为了对以文本输入为条件的离散布局序列进行建模，本工作采用了基于Transformer的编码器-解码器架构，Transformer架构在推理和长距离\n\n![](images/0caa77b58b93e54aa45bd25d46ad364696aee37366c4b85e7fe051bb53aff462.jpg)  \n图3.3 基于Transformer的序列到序列的布局补全生成\n\n依赖建模方面具有优秀的能力。编码器由多个相同的自注意力模块组成，每个模块包括一个多头自注意力层和一个前馈全连接层。两个层都跟随有残差连接和层归一化，从而在有效融合层间信息的同时，保证了模型训练的稳定性。在解码器部分，同样由多个结构相同的模块构成，负责从编码信息中逐步生成布局序列。与编码器结构不同的是，解码器模块中除了多头自注意力层和前馈层外，还额外引入了多头交叉注意力层，用于接收编码器输出的信息。这种交叉注意力机制能够有效整合文本输入的语义信息和布局生成序列间的相关性，确保解码器能够生成与文本条件高度一致的布局内容。\n\n布局表示。本工作采用了类似Layout-Transformer[198]中的方法来进行布局序列表示。一个布局  $(L)$  由若干个  $(N)$  边界框  $(B)$  组成，每个边界框由其类别标签 $(c)$  、左上角位置  $(x,y)$  、宽度和高度  $(w,h)$  表示：\n\n$$\nL = \\left[ B _ {1}, B _ {2}, \\dots , B _ {N} \\right], \\quad B _ {i} = \\left[ c _ {i}, x _ {i}, y _ {i}, w _ {i}, h _ {i} \\right]. \\tag {3.1}\n$$\n\n边界框的顺序由  $y$  确定。此外，连续的位置  $(x, y, w, h)$  被离散化为从0到127的整数，附加两个表示布局开始和结束的标记  $s_{bos}, s_{eos}$  。至此，布局被表示为一个离散序列：\n\n$$\nL = [ s _ {b o s}, c _ {1}, x _ {1}, y _ {1}, w _ {1}, h _ {1}, \\dots , c _ {N}, x _ {N}, y _ {N}, w _ {N}, h _ {N}, s _ {e o s} ]. (3. 2)\n$$\n\n推理。在推理阶段，离散序列的标记生成过程采用自回归的方式，即每一步生成\n\n的结构都依赖于输入的条件文本和先前生成的布局信息及用户指定的布局信息。具体而言，自回归生成首先将条件文本输入模型，以获取与布局内容相关的上下文信息。然后，模型基于这一上下文信息逐步生成布局序列的标记，在每一步中，模型都会将之前生成的布局标记作为输入，从而确保生成的布局序列保持连续性。如果模型生成最后一个方框不完整，那么就忽略最后一个不完整的方框，只使用完整的方框产生的布局信息进行第二阶段的图像生成。\n\n# 3.3.2 基于物体知识迁移与细粒度文本-布局交互的图像生成\n\n布局到图像阶段的重点是根据文本描述和背景感知布局生成真实的图像。为此，本工作提出了TL-Norm，以自适应地将来自ImageNet[196]预训练的类别到图像模型的物体BigGAN[19]知识转移到布局到图像的阶段，该预训练模型能够以优秀的质量生成1,000个物体类别的图像。\n\n# 1. 在布局生成图像任务中迁移类别生成图像模型的能力\n\n当布局中仅存在一个边界框且该边界框覆盖整个图像时，布局到图像的生成过程可以简化为一个类别到图像的生成任务。基于这一观察，我们推测：一个经过充分训练的类别到图像模型能够有效地辅助布局到图像的生成，尤其是在物体生成的环节。接下来，我们详细介绍如何通过修改条件批归一化（CBN）层，使类别到图像模型能够适应布局到图像的生成任务。\n\n条件批归一化。CBN[199]是一种广泛用于条件图像生成中实现条件控制的层[16,19,33,55,96]。给定输入特征图：  $x\\in \\mathbb{R}^{N\\times C\\times H\\times W}$  ，首先对每个通道进行零均值和单位标准差归一化：\n\n$$\n\\bar {x} _ {n c h w} = \\frac {x _ {n c h w} - \\mu_ {c} (x)}{\\sigma_ {c} (x)},\n$$\n\n$$\n\\mu_ {c} (x) = \\frac {1}{N H W} \\sum_ {n, h, w} x _ {n c h w}, \\tag {3.3}\n$$\n\n$$\n\\sigma_ {c} (x) = \\sqrt {\\frac {1}{N H W} (x _ {n c h w} - \\mu_ {c} (x)) ^ {2}}.\n$$\n\n然后使用条件信息 cond 通过预测尺度和偏差来调制归一化的特征：\n\n$$\n\\dot {x} _ {n c h w} = (\\lambda (c o n d) + 1) \\bar {x} _ {n c h w} + \\beta (c o n d), \\tag {3.4}\n$$\n\n其中函数  $\\lambda$  和函数  $\\beta$  通过MLP或卷积层实现。通过这种方式，CBN能够利用条件信息对生成过程进行有效控制，\n\n用于布局到图像的CBN。文献[33]中提出的实例特定和布局感知特征归一化(ISLA-Norm）是一种CBN模块，可实现布局到图像生成的细粒度布局控制。它包含四个步骤：\n\n1. 标签嵌入。首先，对布局中每个实例类别进行嵌入处理，用嵌入向量  $\\vec{d}$  表示。此向量结合了实例类别的嵌入表示  $d^{e}$  以及特定于实例的样式噪声  $\\vec{d}^{noise}$ ，从而表示类别信息和随机样式变化。  \n2. 实例特定投影。在CBN中使用标签嵌入来计算每个边界框的实例特定尺度  $\\lambda (b)$  和偏差  $\\beta (b)$  。具体而言，对于每个边界框  $b$  ，标签嵌入向量  $\\vec{d}$  会通过CBN层映射为尺度  $\\lambda (b)$  和偏差  $\\beta (b)$  ，从而在特征归一化时能够针对性地调整每个实例的特征分布。这一步使得模型能够根据实例特性进行动态的特征控制。  \n3. 掩码预测。使用掩码预测器对每个边界框进行前景与背景区域的分离。掩码预测器由一系列卷积层、上采样层以及实例归一化模块组成，通过 sigmoid 激活函数输出软掩码  $M_{b}$ ，为每个边界框提供前景区域的概率分布。软掩码  $M_{b}$  能够帮助模型在特征调整时聚焦于边界框内的前景物体。  \n4.ISLA  $\\lambda (L)$  和  $\\beta (L)$  的计算。将每个实例的  $\\lambda (b)$  和  $\\beta (b)$  扩展到边界框区域，并与相应的软掩码  $M_{b}$  相乘，以限定作用范围在前景区域内。接着，通过在所有实例上进行求和，将这些边界框内的调制结果合并为整个布局的特征控制参数  $\\lambda (L)$  和  $\\beta (L)$  ，并应用于全图的生成过程。这样，ISLA-Norm能够实现对不同实例和布局区域的精细化控制，使生成的图像更加贴合输入布局的结构信息。\n\n用于类别到图像的CBN。一个类别到图像模型[19,55]包含几个ResBlock和一个非局部块，如图3.2所示。ResBlock包含多个卷积层，用来来学习不同阶段之间的非线性映射。类别到图像中的条件信息[19,55]由类别嵌入和一个小维度的噪声向量组成。归一化特征的预测尺度和偏差表示为  $\\lambda (cls)\\in \\mathbb{R}^{N\\times C}$  和  $\\beta (cls)\\in \\mathbb{R}^{N\\times C}$  。用于布局到图像的类别到图像模型。经过广播操作后，尺度和偏差表示为  $\\lambda (cls)\\in$ $\\mathbb{R}^{N\\times C\\times H\\times W}$  和  $\\beta (cls)\\in \\mathbb{R}^{N\\times C\\times H\\times W}$  ，其中每个位置具有相同的特征。因此，可以在类别到图像模型中将  $\\lambda (cls)$  和  $\\beta (cls)$  替换为通过ISLA-Norm计算的  $\\lambda (L)$  和 $\\beta (L)$  ，以促进布局到图像的生成。\n\n# 2. 细粒度文本-布局交互模块 TL-Norm\n\n本研究进一步考虑布局到图像生成中的文本上下文，以实现最终的布局-图像一致性。整合文本信息的直接方法是将从句子嵌入和布局计算出的尺度  $\\lambda$  和  $\\beta$  相加。其表示为：\n\n$$\n\\lambda_ {L + T} ^ {\\text {s u m}} = \\lambda (L) + \\lambda (\\text {s e n t} _ {-} \\text {e m b}), \\tag {3.5}\n$$\n\n$$\n\\beta_ {L + T} ^ {s u m} = \\beta (L) + \\beta (s e n t \\_ e m b).\n$$\n\n在这种设计中，如果不考虑噪声，则从相同实例类别计算出的初始掩码  $M_{b}$  、实例特定投影  $\\lambda_{b}$  和  $\\beta_{b}$  在不同文本中是相同的。但在实际应用中，相同的实例类别在不同文本语境下往往呈现出不同的含义和外观。例如，假设输入句子为“一个\n\n正在操场上奔跑的男孩”和“一个微笑的男孩看着相机”。前一句描述中，“男孩”更侧重于表现其动态动作，生成的图像可能展示其奔跑的姿态和运动的背景；而后一句则着重刻画男孩的面部表情，图像可能突出笑脸的细节。因此，为实现准确的生成，实例类别信息在生成过程中应与文本上下文建立更紧密的关联，以便根据语境来调整实例的姿态、形状和表情等。\n\nTL-Norm。本研究提出了TL-Norm来实现这一目标。受到[29]的启发，通过类别-文本注意力计算实例的上下文文本信息。假设输入文本有  $T$  个词标记，使用[4]中的预训练文本编码器编码后，可以获得两种词嵌入：来自第一个输入嵌入层的上下文无关词嵌入： $W^{e} = \\{w_{1}^{e}, w_{2}^{e}, \\dots, w_{T}^{e}\\} \\in \\mathbb{R}^{T \\times H}$  和来自文本编码器最后一层的上下文词表示： $W^{c} = \\{w_{1}^{c}, w_{2}^{c}, \\dots, w_{T}^{c}\\} \\in \\mathbb{R}^{T \\times D}$  。布局中的背景和物体类别嵌入也可以通过输入嵌入层表示： $C = \\{c_{1}, c_{2}, \\dots, c_{N}\\} \\in \\mathbb{R}^{N \\times H}$  。文本关注的实例类别嵌入可以通过如下公式表示，首先计算类别嵌入和上下文无关词嵌入之间的注意力分数：\n\n$$\ns _ {i, j} = \\frac {\\exp ((c _ {i}) ^ {T} w _ {j} ^ {e})}{\\sum_ {k = 1} ^ {T} \\exp ((c _ {i}) ^ {T} w _ {k} ^ {e})}, \\tag {3.6}\n$$\n\n然后用注意力分数加权上下文词表示：\n\n$$\nh _ {i} = \\sum_ {k = 1} ^ {T} s _ {i, j} w _ {k} ^ {c}. \\tag {3.7}\n$$\n\n最后, 将原始 ISLA-Norm 模块中的  $d_{i}^{e}$  替换为  $d_{i}^{e} + h_{i}$  。其他操作与原始 ISLA-Norm 相同。\n\n因此，TL-Norm具体的四个步骤如下：\n\n1. 文本关注的标签嵌入。布局中的每个实例类别由  $d$  表示， $d$  结合了文本关注的类别嵌入  $(d^{e} + h_{i})$  和样式噪声  $d^{noise}$ ，其中  $d^{e}$  是类别嵌入。  \n2. 掩码预测。掩码预测器用于区分边界框中的前景和背景。它由几个卷积层和上采样层实现，最后一层是一个 sigmoid 函数，为每个边界框输出一个软掩码  $M_{b}$ 。  \n3. 实例特定投影。使用标签嵌入计算每个边界框在CBN中的实例特定尺度  $\\lambda(b)$  和偏差  $\\beta(b)$ 。  \n4. 文本关注的布局感知特征计算。本研究将每个实例的  $\\lambda(b)$  和  $\\beta(b)$  扩展并与软掩码相乘至相应的边界框。然后，通过汇总将它们合并在一起，得到文本关注的布局感知特征。\n\n生成器和判别器具体结构。图3.4展示了生成器的详细架构。生成器由两个主要部分组成：TL-Norm 和类别到图像生成器骨干网络。TL-Norm 模块用于调整生成过程中空间特征的调制参数，以确保生成图像的局部细节与全局一致性，并尽可能保证原始网络的生成能力。图像生成器骨干网络来源于预训练的模型，通过\n\n![](images/a1b5e937628e455d3e0abfed7795590f6fec1104d12a56dae7c05fded7a04f91.jpg)  \n图3.4 BO-GAN的生成器结构\n\n一系列卷积和反卷积层，逐步构建图像的空间结构和内容细节，通过Non-local网络结构实现全局信息的感知。图3.5绘制生成器和判别器中ResBlock的具体结构。\n\n图3.6展示了判别器的详细架构。本研究使用区域级判别器和图像级判别器，分别区分物体和图像级特征。两个判别器联合建模了图像局部-全局的综合信息。图像级判别器的骨干网络来自预训练的类别到图像判别器，借鉴了预训练模型的鉴别表征能力，前四个块由区域级判别器和图像级判别器共享，其中同样引入Non-local网络结构实现更精准的鉴别。在区域级特征鉴别网络中通过ROIPooling的方法获取图像的局部区域特征，然后通过与类别相关的潜入信息进行点积，归一化后得到最后鉴别的结果。图像级判别器则侧重于图像整体质量的判别，并通过与文本特征进行点积操作，将生成图像与文本的相关性进行真伪建模。这种方法确保了生成图像中各个物体的真实性和一致性，能够有效检测到局部特征与指定类别之间的细微差异。相比之下，图像级判别器主要侧重于整体图像质量的判别。这种结合局部与全局、文本与图像的多层次鉴别方式，能够提升生成图像在结构、内容和语义上的高质量一致性。\n\n# 3.3.3 模型训练\n\nBO-GAN 模型是基于两阶段的模型。其训练基本流程如算法3.1所示，每个阶段的损失函数如下所述。\n\n布局生成器的损失函数。本研究采用教师强制策略来训练布局生成器。在每一步\n\n![](images/fb0f63182e0728f8580314273619044db6bf3fe59f919361726f1399d26ed475.jpg)  \n图3.5 BO-GAN中生成器和判别器中ResBlock的具体组成\n\n![](images/555bf3061c38c30c0b2e94c96d8c6b9788174562a3ae927fa012073be64a5415.jpg)\n\n![](images/c3cc75762249320f936665589256e7320ea100d391cacf7e7c12cbaa5d3075a7.jpg)  \n图3.6 BO-GAN的判别器结构\n\n生成布局标记时，将真实标记而非先前生成的标记输入模型，以加快收敛并提高生成的准确性。为了缓解在有限数据集上的过拟合问题，引入了标签平滑损失，以避免模型对单一类别过度自信，使得生成器具有更好的泛化能力。该损失计算预测分布  $p_i$  与 one-shot 分布平滑后  $\\hat{p}_i$  之间的 KL 散度：\n\n$$\nL o s s _ {l a y o u t} = - \\frac {1}{K} \\sum_ {i = 1} ^ {K - 1} K L \\left(p _ {i} \\| \\hat {p} _ {i}\\right), \\tag {3.8}\n$$\n\n其中  $K$  表示类别的数量。\n\n图像生成器的损失函数。图像生成器的目标是生成1)与文本输入语义一致的真实图像，以及2)每个边界框内的高质量实例(位置可控性)。第一部分的生成器损失来自判别器和一个预训练的图像-文本匹配模型  $\\mathrm{DAMSM}^{[4]}$  。DAMSM提供了对生成图像和文本描述的语义嵌入，从而能够直接计算图像与文本在嵌入空间中的相似度损失，以引导生成器学习到文本输入的语义信息。此外，判别器则直接从真假图像判别的角度评估生成图像的质量。第二部分的损失来自物体级别的判别器：\n\n$$\n\\begin{array}{l} L _ {G} = \\lambda_ {D A M S M} L _ {D A M S M} + \\lambda_ {i m g} E _ {i m g \\sim P _ {i m g} ^ {g}} [ - D (i m g | s) ] \\tag {3.9} \\\\ + \\lambda_ {o b j} E _ {o b j \\sim P _ {o b j} ^ {g}} [ - D (o b j | y) ]. \\\\ \\end{array}\n$$\n\n判别器的目标则是通过区分真实和伪造的图像及物体，提高生成图像的真实性。：\n\n$$\n\\begin{array}{l} L _ {D} = \\lambda_ {o b j} (E _ {o b j \\sim P _ {o b j} ^ {g}} m a x [ 0, 1 + D (o b j | y) ] \\\\ + E _ {o b j \\sim P _ {o b j} ^ {r}} \\dot {m a x} [ 0, 1 - D (o b j | y) ]) \\tag {3.10} \\\\ + \\lambda_ {i m g} \\left(E _ {i m g \\sim P _ {i m g} ^ {g}} \\max  [ 0, 1 + D (i m g | s) ] \\right. \\\\ + E _ {i m g \\sim P _ {i m g} ^ {r}} \\max  [ 0, 1 - D (i m g | s) ]), \\\\ \\end{array}\n$$\n\n其中， $\\lambda_{DAMSM}, \\lambda_{obj}, \\lambda_{img}$  是用于加权各自损失的超参数。 $P_{obj}^{g}, P_{obj}^{r}$  分别表示生成的和真实的物体分布。 $\\boldsymbol{P}_{img}^{g}$  和  $P_{img}^{r}$  分别表示生成的和真实的图像分布。\n\n# 算法3.1 BO-GAN训练的基本流程\n\n1 Input: 文本-布局-图像三元组数据集  $(T, L, I)$ , 随机初始化的文本到布局生成模型  $G_{L}$ , 随机初始化的文本-布局到图像生成模型  $G_{I}$ , 随机初始化的文本-布局到图像的判别器  $D_{I}$  \n2 Output: 文本到布局生成模型  $G_{L}$ , 文本-布局到图像生成模型  $G_{I}$  \n3/\\*先训练文本到布局生成模型  $G_{L}^{*} / \\mathbb{I}$  \n4 while  $G_{L}$  没有收敛 do  \n5 从训练集  $(T,L,I)$  中采样  $t_j,l_j,i_j$  作为训练数据，其中  $t_j$  为输入， $l_j$  为目标输出  \n6 按照以公式3.8为损失函数，来更新模型  $G_{L}$  \n7 end  \n8/\\*再训练文本-布局到图像生成模型  $G_{L}^{*} / \\mathbb{I}$  \n9 while  $G_{I}$  没有收敛 do  \n10 从训练集  $(T,L,I)$  中采样  $t_j,l_j,i_j$  作为训练数据，其中  $t_j,l_j$  是输入， $i_j$  是目标输出  \n11 按照公式3.9为损失函数，来更新生成器  $G_{I}$  \n12 按照公式3.10为损失函数，来更新判别器  $D_{I}$  \n13 end\n\nMS-COCO-14基准[72]上的图像分辨率为  $256 \\times 256$  。但当前开源的类别到图像生成器和判别器仅在  $128 \\times 128$  分辨率下进行训练。由于现有硬件资源限制，无法在ImageNet上训练生成  $256 \\times 256$  图像的模型，因此本研究首先在  $128 \\times 128$  设置下使用预训练的类别到图像模型训练所提出的布局到图像模型，然后使用\n\n渐进增长方案[200]扩展到  $256 \\times 256$  。具体来说，生成器和判别器的损失函数在分辨率逐步扩展过程中，结合了低分辨率和高分辨率的损失权重。损失函数如下所示：\n\n$$\nL _ {G} = L _ {G 2 5 6} + \\max  (0, 1 - \\operatorname {c u r} _ {-} \\text {s t e p} / f a d e _ {-} \\text {s t e p}) L _ {G 1 2 8}, \\tag {3.11}\n$$\n\n$$\nL _ {D} = L _ {D 2 5 6} + \\max  (0, 1 - \\operatorname {c u r} _ {-} \\text {s t e p} / f a d e _ {-} \\text {s t e p}) L _ {D 1 2 8}, \\tag {3.12}\n$$\n\n其中，cur_step 表示当前的训练步数， $fade\\_step$  是  $128 \\times 128$  设置的总训练步数。当  $cur_{s} tep < fade_{s} tep$  时，损失函数中同时包含低分辨率和高分辨率的损失，随后逐步减小低分辨率的损失权重，直至仅依赖高分辨率的生成损失。通过这种方法显著提升了训练效率，在  $4 \\times \\mathrm{RTX} 3090$  上仅需 8 小时即可在 MS-COCO-14  $256 \\times 256$  达到最佳的性能。\n\n# 3.4 实验结果与分析\n\n表 3.1 BO-GAN 与其他相关方法在 MS COCO-14 数据集上的定量结果对比  \n\n<table><tr><td>方法</td><td>FID ↓</td><td>R-prec ↑</td><td>SOA-I ↑</td><td>SOA-C↑</td></tr><tr><td>Real Images</td><td>2.28</td><td>82.92</td><td>82.91</td><td>75.43</td></tr><tr><td>AttnGAN[4]</td><td>30.53</td><td>52.14</td><td>43.13</td><td>25.68</td></tr><tr><td>DM-GAN[7]</td><td>24.09</td><td>65.27</td><td>53.15</td><td>33.63</td></tr><tr><td>Obj-GAN[29]</td><td>26.07</td><td>58.34</td><td>49.70</td><td>30.84</td></tr><tr><td>R-GAN†[30]</td><td>24.60</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DAE-GAN[17]</td><td>24.87</td><td>64.50</td><td>53.29</td><td>33.92</td></tr><tr><td>DF-GAN[201]</td><td>21.97</td><td>38.13</td><td>40.68</td><td>20.66</td></tr><tr><td>SSA-GAN[194]</td><td>15.73</td><td>57.72</td><td>50.57</td><td>29.18</td></tr><tr><td>BO-GAN</td><td>9.21</td><td>67.03</td><td>68.21</td><td>48.91</td></tr></table>\n\n# 3.4.1 实验设置\n\n# 1. 数据集\n\n本研究的实验在广泛使用的MS-COCO-14数据集[72]上进行，该数据集包含82,783张训练图像和40,504张测试图像。每张图像对应5个描述文本。该数据集包含171个类别，展示了物体在尺度、形状和外观上的多样性，为模型在生成高质量和语义丰富的图像方面提供了良好的数据支持。\n\n# 2. 评估指标\n\n为了全面评估模型生成图像的质量和文本对齐性，本研究采用了三种评估指标：FID、R-precision 和 SOA，以确保模型的各方面性能得以充分衡量。每个模型在测试集中生成 30,000 个描述文本对应的图像，用于对定量指标的计算。\n\nFID. 为了评估生成图像的质量，本研究使用常用的指标 Fréchet Inception Distance[170]。本研究首先从预训练的 Inception-v3 网络的最后一层获取每张图像的特征，然后计算真实图像和生成图像之间的特征分布距离。较低的 FID 表示图像质量更好。\n\n$R$ -precision. 按照  $\\mathrm{AttnGAN}^{[4]}$  中的方法，本研究使用 R-precision 来评估文本与图像的对齐度。该指标通过在文本候选池中是否可以从生成的图像中检索到输入文本来衡量。本研究采用大规模预训练的图像-文本匹配模型  $\\mathrm{CLIP}^{[6]}$  作为评估器。候选文本的数量设置为 100。如果检索到的 top-1 结果是输入文本，则认为生成的图像与输入文本对齐。\n\nSOA. 语义物体准确度 (SOA)[172] 指标用于评估生成的图像是否包含文本中提到的物体。它包含两个子指标：SOA-C 用于量化每个检测到的物体类别的百分比，SOA-I 用于量化检测到所需物体的图像的百分比。该指标综合衡量模型对位置的控制能力和图像的生成质量。\n\n# 3. 实现细节\n\n对于布局生成器，本研究在编码器和解码器中设置了4层Transformer，批量大小设置为64。学习率和调度器按照原始实现[202]进行设置。训练时最大物体数量设置为12。在推理阶段，选择top-p采样[203]，其结果优于贪婪采样。\n\n对于图像生成器, 本方案采用  $\\mathrm{BigGAN}^{[19]}$  作为预训练的类别到图像模型。排除面积小于图像  $2\\%$  的边界框。最大物体数量设置为8。物体  $\\lambda_{obj}$  、图像  $\\lambda_{img}$  和图像-文本对齐  $\\lambda_{DAMSM}$  的权重分别设置为1.0,0.1,0.2。批量大小设置为40，对于  $128\\times 128$  的图像，需要  $2\\times$  RTX3090，对于  $256\\times 256$  的图像，需要  $4\\times$  RTX3090。本工作使用Adam优化器，设置  $\\beta_{1} = 0.0,\\beta_{2} = 0.999$  。将光谱归一化[61]用于生成器和判别器层。生成器和判别器的学习率设置为0.0001。在渐进增长训练时，将fade_step设置为5,000。\n\n# 3.4.2 主要结果\n\n表 3.2 BO-GAN 与其他方法的在人工评价指标上的对比结果  \n\n<table><tr><td></td><td>Obj-GAN</td><td>SSA-GAN</td><td>BO-GAN</td></tr><tr><td>top-1</td><td>4.4%</td><td>18.4%</td><td>77.2%</td></tr></table>\n\n定量结果。BO-GAN与当前最先进模型在MS-COCO-14数据集上比较结果如\n\n![](images/71e70f997c980da539fcf90b355fdbed72ebda03be028b8eb5f65deeebbdf526.jpg)  \n图3.7 BO-GAN和其他方法的定性结果比较\n\n表3.1所示，在FID方面，显著优于第二好的模型SSA-GAN[194]，从15.73下降到9.21。在R-precision方面，BO-GAN优于第二好的模型DM-GAN，从  $65.27\\%$  提升到  $67.03\\%$  ，表明生成图像与输入文本在语义上具有更高的一致性。在SOA-I/C指标上，BO-GAN也超越了下一个最佳模型DAE-GAN，从53.29/33.92提升到68.21/48.91。BO-GAN能够在图像中更好地生成文本中提到的物体。这些结果表明，BO-GAN能够生成文本和布局更一致的图像，并显著提高了图像质量。\n\n为了进一步评估性能，本研究进一步进行了人工评估来对比BO-GAN与其他先进模型的实际生成效果。随机抽取了500个描述及其对应的由Obj-GAN、SSA-GAN和BO-GAN生成的图像，然后请用户根据图像质量以及文本与图像之间的一致性，在候选图像中选择最佳（top-1）图像。表3.2显示，BO-GAN获得了最高的  $77.2\\%$  的选择，相比之下，Obj-GAN和SSA-GAN的选择率则远低于BO-GAN，表明BO-GAN在用户主观评价中表现更好。\n\n定性结果。图3.7及图3.8展示了最新的单阶段模型  $\\mathrm{SSA - GAN}^{[194]}$  和经典的两阶段模型Obj-GAN[29]生成的图像。总体而言，BO-GAN方法生成了具有更多结构化物体和更逼真背景的图像。例如，在第四列中，BO-GAN生成了几乎真实的消防栓，而其他方法生成的消防栓几乎无法辨认。在第二列中，BO-GAN能够生成清晰的窗口背景。相比之下，SSA-GAN和Obj-GAN生成的背景表现出模糊、不真实的缺陷，缺乏明显的纹理和结构。\n\n图3.9展示了BO-GAN生成的中间背景布局和相应的图像。可以观察到，BO-\n\n![](images/88cf55f5e64b1b8c5b18cc5f07887aeb3e9d0a7dcff735e0674f4c60b2f2d072.jpg)  \n图3.8 BO-GAN和其他方法的定性结果比较的更多示例\n\nGAN 能够根据输入文本描述对场景中的内容及其相对位置进行全面规划，生成出较为粗略的背景布局。这一布局结构为生成图像提供了初步的空间分布和物体构成，使得模型在生成过程中具备更高的控制性，并在第二阶段实现了布局的控制以及生成了更好质量的图片。\n\n# 3.4.3 消融实验\n\n本节进行了消融研究，以验证以下方面的有效性：1）背景布局，2）基于Transformer的布局生成器，3）从BigGAN进行的物体知识转移，4）TL-Norm。\n\n表 3.3 背景生成和 TL-Norm 的消融实验  \n\n<table><tr><td>Obj</td><td>Bg</td><td>TL</td><td>FID ↓</td><td>R-prec ↑</td><td>SOA-I ↑</td><td>SOA-C ↑</td></tr><tr><td>✓</td><td></td><td></td><td>7.41</td><td>65.05</td><td>55.26</td><td>34.96</td></tr><tr><td>✓</td><td>✓</td><td></td><td>7.18</td><td>65.70</td><td>55.88</td><td>35.45</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>6.42</td><td>66.84</td><td>61.04</td><td>42.05</td></tr></table>\n\n# 1. 背景布局\n\n如表3.3所示，在背景布局的帮助下，生成的图像在所有指标上均优于仅有物体布局的图像。在仅有物体布局的情况下，生成图像的FID、R-precision、SOA-I和SOA-C指标分别为7.41、65.05、55.26和34.96。在添加背景布局后，FID值降至7.18，R-precision、SOA-I和SOA-C指标均有所提升，分别达到65.70、55.88和35.45。这一结果显示，背景布局能够为图像生成提供额外的上下文信息，通\n\n![](images/2795822975d9f8430b4d6e79548cc20f2f7b49a0922bb8c78354c14ae8f250b8.jpg)\n\n![](images/87885e7b0eb9b10622b1ab2545fa64edbf2ca894f22dd496abd9068c9dd127db.jpg)\n\n![](images/7efd1c0a946c1fdc585448eb91e4bfbe15c140dd7594c7f0804e2fc8334e8f08.jpg)\n\n![](images/0c64fa18ed5633db80a2c2210ddd37a77126c9115d19fa063baa69fa55751289.jpg)\n\n![](images/b795ebe982cc31eb9db2f181e8e989921a725b3cdf9c16ea3faa13ef0381edc0.jpg)\n\n![](images/cb1c52593d11868c899d2fb692fa21059f6758097a97a51e3e2594330073bcce.jpg)\n\n![](images/a51848f1582d88b17188f2aab6aff7a892ebc198320755428ecab0d865bb434f.jpg)  \n图3.9 BO-GAN生成的中间背景和物体布局及相应的最终图像\n\n![](images/11f1db7c0a4b2c0181ca6a24739fc4a62d40472cbb13cdc6ab67eb7ccbc651bc.jpg)\n\n过对场景结构的预先规划使生成器在布局和场景细节方面更加完整，进而生成的图像更加真实并和文本更加一致。如图3.10所示，基于背景布局，背景“街道”更加清晰、细节更加丰富，进一步验证了背景布局可以帮助图像背景生成。\n\n# 2. TL-Norm\n\n通过将TL-Norm与其基线(公式3.5中的方案)进行比较来验证其有效性。如图3.10所示，TL-Norm显著提升物体“公交车”的质量。表3.3中的第二行和第三行显示，与基线相比，所有指标都有所提升，特别是在SOA指标上，TL-Norm的引入显著提升了模型在生成图像时对物体位置和类别的控制能力。SOA-I指标从\n\n![](images/5b961576913610bc7e0498bf067b384c25fcc0ce4b8b13bda2c9359ddbc6d091.jpg)  \n输入文本\n\n![](images/0e279c2f3b53cb9935aadbcbca2bca8470ec5032b4cc5de793fbfd125b32d128.jpg)\n\n![](images/c4cf1794959aeda442c844c7aeb449aa24eeaf3fe7f82f55e680a54bbb595ad6.jpg)  \n物体组成的布局\n\n![](images/63a437414f1d4b1fb5e52657fb13cb4d6465e408dfd21191e4923413e32c2194.jpg)\n\n![](images/0dd29cfb85a683b2c0a2f6c553620c0ffc0cfaa5b0821f20765db637f8767038.jpg)  \n背景+物体组成的布局  \n背景+物体组成的布局并使用TL-Norm\n\n![](images/52460ade627cb1f81390d6074ecea971fd2518ac44f6f89f2a44b06ba2a9c708.jpg)\n\n![](images/1224e10c2e7d42cc4a729f911883d8dbfcc676b034a2b1c053e509b2d38de3a8.jpg)  \n图3.10 背景布局和TL-Norm的消融实验结果\n\n55.88 提升到 61.04，SOA-C 指标从 35.45 提高至 42.05，这些提升表明 TL-Norm 更有效地实现了从类别到图像的物体知识迁移，增强了生成器在细粒度控制和物体一致性上的表现。\n\n表 3.4 物体知识迁移的消融实验  \n\n<table><tr><td>G</td><td>D</td><td>FID ↓</td><td>R-prec ↑</td><td>SOA-I ↑</td><td>SOA-C ↑</td></tr><tr><td></td><td></td><td>13.1</td><td>57.67</td><td>51.72</td><td>30.76</td></tr><tr><td>✓</td><td></td><td>8.28</td><td>63.89</td><td>55.54</td><td>36.18</td></tr><tr><td>✓</td><td>✓</td><td>6.42</td><td>66.84</td><td>61.04</td><td>42.05</td></tr></table>\n\n# 3. 布局生成器\n\n为了评估基于Transformer的布局生成器的性能，本研究使用第二阶段的布局到图像模型从生成的布局中生成图像。表3.5显示，基于Transformer的布局生成器优于之前基于LSTM的方法。FID从29.66改进到9.21。此外，本方法在与真实布局的比较中也表现出了竞争力（FID从9.21到7.97，R-precision从67.03到70.43）。在物体质量指标上，生成的布局甚至超越了真实布局。这是因为生成器倾向于生成比真实布局更大的边界框，因此在最终图像中对应的物体更容易生成。图3.3(c)显示，LSTM在生成综合布局方面存在困难。布局中的下部区域没有语义类别，导致最终图像中出现无意义的背景。而且布局中呈现了三头牛，但根据标题应该只有一头牛。总体而言，这些结果突显了基于Transformer的布局生成器在具有多样空间配置的综合布局中的优越性。\n\n![](images/4c49d99243411561d119acbbbc31a0d969246d8e0da846fafb6faf67c98c0621.jpg)  \n图3.11 利用Transformer建模布局生成的优势\n\n# 4. 物体知识转移\n\n本工作使用BigGAN的生成器和判别器作为物体知识源。如表3.4所示，与从头训练的模型相比，SOA的结果显著提升。如图3.12(b)所示，BO-GAN可以生成更好的物体，如船、人，表明类别到图像模型作为物体知识源可以帮助T2I任务中的物体生成。这说明了利用大规模数据集上训练的类别到图像模型可以在布局生成图像中作为有效的先验知识，提升物体生成的质量。\n\n表 3.5 使用 Tranformer 建模布局生成的优势  \n\n<table><tr><td>模型</td><td>FID ↓</td><td>R-prec ↑</td><td>SOA-I ↑</td><td>SOA-C ↑</td></tr><tr><td>GT layout</td><td>7.97</td><td>70.43</td><td>66.40</td><td>48.26</td></tr><tr><td>LSTM</td><td>29.66</td><td>42.15</td><td>47.43</td><td>30.84</td></tr><tr><td>Transformer</td><td>9.21</td><td>67.03</td><td>68.21</td><td>48.91</td></tr></table>\n\n并且本研究发现基于这种该先验模型，模型训练可以明显收敛得更快。如图3.13基于预训练模型先验进行引导时，模型训练10个epoch的效果，就相当于从头训练100个epoch的效果，相当于加速了10倍，且模型可以收敛到更低的FID。这种加速效果主要得益于预训练模型在大规模数据集上已经学习到丰富的物体特征，使得BO-GAN能够在T2I任务中有效利用这些知识，从而在初始化阶段便具备较强的生成能力。相比从头训练，预训练模型提供了一个更高的起点，使得模型可以快速学习到布局和语义特征，显著减少训练时间。\n\n![](images/50f34545fee5d21a35a9f44c479938cbea5c029f3f4278b88835f859bf25c7ad.jpg)  \n图3.12 利用预训练类别生成物体模型的优势\n\n![](images/768c5a30a6a3bf9aaa78f82c80450028010601dc061bf01252b1abc3592e477d.jpg)  \n图3.13 使用预训练类别生成物体模型后对收敛速度的影响\n\n# 3.5 本章小结\n\n本文提出了一种用于布局可控文本生成图像的背景与物体生成对抗网络(BO-GAN)，旨在解决现有方法在生成复杂场景图像时的局限性。所提出的两阶段方法包括以下核心步骤：首先在文本到布局阶段重新设计布局生成，将背景布局与物体布局结合起来，以生成完整的图像结构；接着，在布局到图像阶段，利用单物体图像生成模型的预训练知识进行物体知识转移，以提升物体的生成质量。从技术上，本工作首先设计了基于Transformer序列到序列的布局生成器去建模复杂的布局分布，然后提出细粒度文本-布局交互模块去实现物质知识的高校迁移。大量定量和定性实验验证了BO-GAN在生成复杂、真实的场景图像\n\n方面的显著效果，特别是在图像质量和位置可控性上表现优异。\n\n本方案的局限性体现在以下几个方面：1）当场景中的物体数量增多时，布局补全的质量下降。布局生成器在处理复杂场景和多物体关系时容易出现位置偏差或细节丢失，导致生成的布局难以与目标描述完全一致。2）生成图像的质量仍有较大提升空间。受生成对抗网络的限制以及预训练数据量的不足，当前模型在迁移后的生成效果在细节和整体质量上都存在改进的空间。3）布局生成过程中容易产生误差累计问题。由于训练中采用教师强制策略，导致训练和测试阶段存在不一致：训练时的历史布局信息基于真实数据，而测试时依赖于模型的生成数据。当生成质量较低时，错误会逐步累积，影响后续的布局和图像质量。\n\n未来工作可以从以下角度进行改进和探索，以进一步提升布局和生成的整体质量：\n\n1）增强布局生成器的鲁棒性以应对更复杂的场景需求。未来可以通过多种方法提升布局生成器在多物体场景中的表现。例如，通过收集更多样化的数据并增加布局生成器的模型容量，使其能够更准确地建模物体之间的复杂关系。此外，还可以探索将大语言模型应用于布局生成任务中，以利用语言模型在生成复杂多物体关系描述中的强大建模能力，使生成的布局更加贴合目标描述。通过这些方法，布局生成器的鲁棒性和在复杂场景中的准确性将得到显著提升。  \n2）引入大规模的文本生成图像基础模型作为生成先验。为提升图像生成的质量，可以在布局生成和图像生成阶段直接引入大规模预训练的文本生成图像模型作为生成先验。这类模型通常具备更丰富的语义理解和更精细的生成能力，通过将其迁移至本任务中，可以帮助生成器在视觉质量、细节表现和颜色控制上达到更高的水准。这样不仅能够提升生成的图像质量，还可以让模型更有效地利用大规模数据中学到的通用图像特征，显著优化生成结果。  \n3）采用强化学习策略以减少布局生成的误差累积。在布局生成任务中，可以尝试用强化学习来替代教师强制策略，使得布局生成的训练和测试更加一致。强化学习能够让模型在训练过程中自主学习布局的生成策略，并面向整体图像生成的最终目标进行优化。通过这种方法，布局生成器能够逐步调整生成过程，以减小在测试阶段的误差累积，使生成结果更贴合全局布局目标。\n\n# 第4章 基于学习的属性可控文生图\n\n随着国际化的深入，宣传的内容往往需要考虑全球场景下的内容合规性。在此背景下，保持用户的关键属性信息不变对图像生成系统尤为重要。如图4.1所示，用户在使用谷歌 Gemini 文生图模型时，发现其存在无法保持美国国父华盛顿的人种肤色(白人)。这种无法保持用户关键的肤色信息的现象引入了大量用户的强烈不满，会导致用户满意度下降，并引发潜在的法律风险。本章进行精细化属性可控的研究，以确保在文本生成图像中不改变用户的关键属性信息。通过这种方法，系统不仅能更好地满足用户个性化需求，还为在全球范围内的合规性提供了坚实的技术保障。\n\n![](images/f9530bb48fc7bfbb503d49efa002a3eed69bbd5673f5cde7fb0741fd9baac5de.jpg)  \nCertainly! Here is a portrait of a Founding Father of America:\n\n![](images/01217585b0ecd8ca3894b4b7c596746d9245b4c4b5bfc5165dcc700c78381080.jpg)  \n图4.1 谷歌Gemini文生图模型存在无法保持人种肤色的现象\n\n具体来说，本章从肤色这一属性控制出发，提出了 Skin-Adapter 模型，该模型可以在文本生成图像任务中实现了细粒度肤色信息的精确保留。为实现这一目标，本研究首先设计了基于频率的自适应颜色直方图，以准确表示用户的肤色信息。该直方图表示方法通过动态调整颜色分布，适应多样化的肤色范围，使得模型能够准确地保留并再现参考图像中的肤色特征。此外，本研究引入了颜色分布匹配奖励策略，以显著增强输入图像与输出图像之间的肤色一致性。实验结果表明，Skin-Adapter 可以在生成的图像中保持输入图像的细粒度肤色信息。此外，通过与潜在替代方法进行定量和定性比较，验证了本方案设计的优越性。\n\n本章具体组织如下：第4.1节介绍了本课题的研究动机和所面临的关键挑\n\n战；第4.2节对相关研究工作进行了梳理；第4.3节详细介绍了算法设计。首先，第4.3.1节描述了基于自适应频率导向的颜色直方图表示方法，以精确刻画参考肤色信息。接下来，第4.3.2节介绍了基于颜色分布匹配的奖励学习策略，以提升生成图像与输入图像在肤色上的一致性。第4.5节通过广泛的定量和定性实验验证了所提方法的有效性和通用性，表明Skin-Adapter在肤色保持方面的显著优势。第4.6节对本章进行了总结，概括了主要实验结论和未来研究方向。\n\n![](images/d5152a7869136b9a3e6fa346ea8c1163f072b4d038a4cab71ee3c81f917b6e29.jpg)  \n图4.2 Skin-Adapter生成结果及与肤色提示词效果对比\n\n# 4.1 引言\n\n近年来，基于扩散的文本生成图像模型[9,25-27,79]展示了从文本描述生成图像的显著能力。这些模型使得生成生动且富有表现力的以人为中心的图像变得轻而易举。然而，这些模型的可控性往往不足，在实际应用中，单纯依赖文本生成图像模型时，往往无法保留参考图像中的关键信息。\n\n![](images/4f2c85de891a1f6cfdbbbaa62d8a91584834855c38f97121a7a9eeb57842512b.jpg)  \n图4.3 文本描述的模糊性举例\n\n肤色往往是用户希望保留的最重要信息之一。如图4.2所示，仅通过文本描述，用户的肤色通常无法在输入图像和输出图像之间实现一致。其原因在于文本描述是模糊的，如图4.3所示，仅根据文本描述难以确定细粒度的肤色表现。这\n\n种差异可能会对用户体验产生负面影响，并在人工智能应用中引发伦理风险。因此，研究能够使文本生成图像模型保留用户原有肤色的方法至关重要。然而，实现输入图像和输出图像之间细粒度的肤色一致性是非常挑战的。\n\n首先，当前主流的颜色表示方法不足以准确表示肤色信息。要在输入图像和输出图像之间实现颜色一致性，准确表示参考肤色是至关重要的。目前主要有两种方法：颜色平均法和固定颜色直方图法。颜色平均法[204]将肤色视为均匀分布，因此使用平均颜色来表示肤色。然而，肤色区域受多种因素影响，包括光照、阴影和反射，远非均匀分布。固定颜色直方图法[127,205]将每个像素的颜色量化为最接近的  $N$  种预定义颜色之一，然后统计每种预定义颜色的出现次数。然而， $N$  的选择面临两难境地：当  $N$  过大时，将会导致训练不稳定性和计算成本的增加；另一方面，如果  $N$  过小，模型则无法准确表示输入颜色，导致显著的颜色表示误差。这一困境导致大家需要在颜色准确性和训练稳定性之间进行权衡。在实践中，通常将预定义颜色的数量限制在几百个以内[127]。然而本工作通过实验发现，由于用户对肤色变化比对自然图像中的一般颜色分布更加敏感，使得这两种方法的表示误差在肤色方面尤为突出，\n\n其次，对于基于扩散模型的文本生成图模型来说，学习保留肤色信息是具有挑战性的。由于肤色信息是图像中局部区域（即肤色区域）的颜色统计信息，其控制信号相对较弱。此外，在当前的扩散模型训练范式下，颜色一致性是通过在噪声潜在空间中计算的像素级损失来隐式学习的[27,66]。这种损失并没有明确约束输入图像和输出图像之间的颜色一致性。因此，肤色一致性无法得到保证。\n\n本研究提出了第一个用于文本生成图像的肤色保留模型，Skin-Adapter。如图4.2所示，Skin-Adapter能够使文本生成图像模型根据参考图像生成具有一致肤色的人物图像。为了解决上述挑战，Skin-Adapter采用了新颖的基于自适应频率的颜色直方图用于肤色表示，并使用有效的颜色分布匹配奖励进行训练。提出基于自适应频率的量化颜色直方图的动机在于：皮肤区域通常由少数主导颜色组成，其他像素大多接近主导颜色。因此，可以使用这些主导颜色作为量化的预定义颜色，并且只考虑接近主导颜色的像素。与固定颜色直方图相比，这种设计不仅可以降低量化误差，还可以排除与肤色无关的像素（如雀斑或痣）的负面影响。此外，本工作进一步设计了一种一致性奖励策略，显式地学习输入图像和输出图像之间的颜色一致性。在训练过程中，以随机噪声和参考肤色为条件通过多次迭代生成图像来模拟推理过程，计算生成图像和输入图像之间的颜色分布距离，然后通过优化可学习的参数来缩小该距离。这种对颜色分布一致性的显式奖励显著增强了生成图像与输入图像之间的肤色一致性。\n\n本章的贡献可以总结为三个方面：\n\n- 本工作提出了第一个用于文本生成图像的肤色属性保持模型, Skin-Adapter。\n\n该模型生成的图像可以保留输入图像中的细粒度肤色信息。\n\n- 本工作引入了基于自适应频率的量化颜色直方图来准确表示用户的肤色信息，并通过一致性奖励提升输入图像和输出图像之间的颜色分布一致性。  \n- 本工作通过广泛的消融实验验证了方法的有效性，并展示了在不同文生图模型和颜色保留任务中的泛化性。\n\n# 4.2 相关工作\n\n# 4.2.1 可控文本生成图像\n\n近年来,许多方法被提出以增强文本生成图像模型的控制能力,使其能够通过特定信号生成符合用户需求的图像内容。早期的研究聚焦于空间结构控制[98,101],通过指定布局确保生成图像中的元素符合预期的空间分布。此外,身份信息保真控制[164,206-208]已经成为另一重要方向,这些方法保证了生成图像中的人物特征在面部细节、发型等方面保持与用户提供的参考图像一致性。针对个性化生成需求,IP信息[42,209-210]控制方法也被开发出来,以确保生成内容能够准确反映用户的特定定制需求,增强了生成图像的独特性。为了满足风格迁移需求,一些研究还引入了参考风格控制[211-212],使生成图像的风格与参考图像保持一致,满足不同场景下的风格要求。与这些方向相关的最新研究引入了图像级别的颜色直方图控制[127],这为图像的整体色调一致性提供了一种新的控制方式。然而,实现输入图像和输出图像之间的颜色一致性仍然是一个尚未深入探索的问题,特别是当生成图像中的肤色表现需要准确反映输入图像的肤色特征时。这种一致性在人工智能应用的公平性方面具有深远意义,因为生成图像中肤色表现的偏差可能会导致用户的情感不适,甚至引发伦理问题与信任危机。为了解决这一问题,本研究提出了一种肤色保真控制方法,通过在文本生成图像模型中引入针对肤色的精细控制机制,确保输出图像中的肤色特征能够准确匹配输入图像的肤色信息。\n\n# 4.2.2 肤色表示\n\n为了确保输入图像和输出图像之间的肤色一致性，准确捕捉并传递输入图像中的肤色信息显得尤为重要。传统的肤色处理方法多采用颜色平均方式[204]，将肤色视为均匀分布，从而忽略了肤色在不同部位或光照条件下的细微差异。这种简化在实际生成中容易导致肤色表达的单一化，缺乏自然的色彩过渡。而固定颜色直方图方法[127,205]则将颜色量化为有限的预定义类别，这种处理方式尽管能够一定程度上实现肤色控制，但由于量化限制往往无法准确还原输入图像的丰富色彩，导致显著的颜色偏差，特别是在人种和肤色细节多样化的场景中。为了\n\n解决这些不足，本研究提出了一种基于自适应频率的颜色直方图表示方法，该方法通过自适应调整不同颜色频率的分布，使得模型能够捕捉更丰富的肤色细节。\n\n# 4.2.3 基于扩散模型的文本生成图像的奖励方法\n\n肤色是一种相对较弱的控制信号，因为它表示图像中的局部统计信息，而不是全局性的内容或结构信息。在生成任务中，肤色控制要求模型能够保持输入图像中特定肤色的属性，并在生成图像中准确地呈现。然而，当前的扩散模型训练主要依赖于在噪声潜在空间中计算的逐像素损失[66]，缺乏确保最终输出与输入图像一致的明确约束，导致寻训练得到的模型在肤色一致性上比较受限，影响模型生成图像的公平性和用户体验。为了弥补这一不足，受到最近扩散奖励进展的启发[10,213]，本工作设计了一种颜色一致性奖励，显式地强化输入和输出图像之间的颜色一致性，从而确保生成图像在肤色表现上更加准确和一致。\n\n# 4.3 算法设计\n\n给定参考人像图像，本工作的目标是使预训练的文本生成图像（T2I）模型在不同的文本提示下生成肤色一致的人物图像。\n\n![](images/f67fcba497a5f8aa35ac1bfaf5bfcd93d3829b969b3a06cce3014a233be4c455.jpg)  \n(a) 模型结构  \n图4.4 Skin-Adapter模型结构和训练方法\n\n![](images/55e9a6c67e5fdacc841e15b0f9568891a19085b78859847323c4bb8e73f6504e.jpg)  \n(b) 训练损失\n\n整体框架如图4.4所示。给定输入图像，首先提取人脸区域并应用人脸解析模型获取与皮肤相关的像素。这些像素通过提出的基于自适应频率（AF）颜色直方图的统计方法进行表示，这是一种更准确且更稳健的肤色表示方式。然后，该特征通过Perceiver Resampler[214]映射器进行编码。视觉信息通过额外的视觉交叉注意层注入到文本生成图像模型中。为了显式学习肤色一致性，通过生成随机噪声图像并经过T步模拟推理过程，计算颜色分布匹配距离作为奖励。\n\n在接下来的部分中，首先简要描述在本工作中使用的基于扩散的文本生成图像模型，以及如何将通过Perceiver Resampler映射器将视觉信息注入到T2I模型中。然后介绍表示肤色的可能方法以及所提出的基于自适应频率的颜色直方图。最后介绍颜色分布匹配奖励，以及如何训练和推理模型。\n\n# 4.3.1 预备知识\n\n# 1. 基于扩散的文本生成图像模型\n\n本工作采用了开源的Stable Diffusion 1.5 (SD1.5)作为文本生成图像模型，该模型经过数十亿张图像的训练，并在图像生成质量和文本理解能力方面表现十分出色。\n\nSD1.5 是一种具有 UNet 主干网络的潜在扩散模型 (LDM)。LDM 首先通过变分自编码器 (VAE) 将输入图像  $x$  表示为低分辨率的潜在空间  $z$  。然后，通过进行以文本条件的扩散模型训练，从文本输入  $c$  生成目标图像的潜在代码。该扩散模型的损失函数可以表示为：\n\n$$\n\\mathcal {L} _ {\\text {d i f f u s i o n}} = \\mathbb {E} _ {\\epsilon , z, c, t} [ \\| \\epsilon - \\epsilon_ {\\theta} (z _ {t}, c, t) \\| _ {2} ^ {2} ], \\tag {4.1}\n$$\n\n其中， $\\epsilon_{\\theta}$  是由模型预测的噪声，其参数为可学习的  $\\theta$ ， $\\epsilon$  是从标准正态分布中采样的噪声， $t$  是时间步， $z_{t}$  是时间步  $t$  时的噪声潜变量。在推理过程中，潜在代码由扩散模型生成，随后通过 VAE 解码器将其映射到图像空间。\n\n在SD1.5中，文本条件通过交叉注意力注入到网络中：\n\n$$\nC r o s s A t t e n t i o n (Q, K, V) = S o f t m a x \\left(\\frac {Q K ^ {T}}{\\sqrt {d}}\\right) V, \\tag {4.2}\n$$\n\n其中， $Q$  表示来自 UNet 的特征， $K, V$  是在每一层 CrossAttention 中投影的文本特征。\n\n# 2. 通过额外的交叉注意力层引入视觉控制\n\n为了将肤色信息注入到文本生成图像模型中，本工作遵循IP-Adapter[209]的设计，在文本交叉注意力的基础上引入了另一个用于视觉信息的交叉注意力层。具体来说：\n\n$$\n\\begin{array}{l} C r o s s A t t e n t i o n (Q, K, V, K _ {i m g}, V _ {i m g}) = S o f t m a x \\left(\\frac {Q K ^ {T}}{\\sqrt {d}}\\right) V \\\\ + S o f t m a x \\left(\\frac {Q K _ {i m g} ^ {T}}{\\sqrt {d}}\\right) V _ {i m g}, \\tag {4.3} \\\\ \\end{array}\n$$\n\n其中，  $K_{img} = W_k^{img} \\cdot f_{img}, V_{img} = W_v^{img} \\cdot f_{img}$  。在训练过程中，保持原模型参数不变，只学习与视觉相关的权重  $W_k^{img}, W_v^{img}$  和投影模块。\n\n# 3. Perceiver Resampler 映射器\n\n我们采用Perceiver Resampler作为视觉属性信息的映射器，旨在更高效地处理高维和复杂数据。该模型通过Resampler模块对输入特征进行降维处理，减少数据冗余以节省计算成本\n\n首先，输入数据经过预处理提取初步的特征表示。然后其中的模块对输入嵌入进行采样。具体地，采用一组可学习的降维查询向量与输入特征交互，通过多头注意力机制捕捉输入数据的核心信息。这种降维查询的机制通过选取输入嵌入的关键信息，将高维数据压缩至低维表示，从而使模型的计算效率大幅提高。通过保留核心特征为后续处理提供了低维、信息丰富的表示，同时保持数据表示的完整性。\n\n![](images/2c7d58dab57e7f2863fb279f875749edc519fc03ec70dfb6bd7869646c8823b9.jpg)  \n图4.5 Preceiver Resampler模型结构\n\n在降维后的数据表示上，Perceiver Resampler进一步利用经典的Perceiver主干部分进行处理。通过交叉注意力和自注意力机制的结合，模型能够捕获全局与局部的复杂关系。交叉注意力负责将降维后的表示与降维查询向量结合，确保降维后的特征嵌入能够与输入信号保持语义一致性，而自注意力则在降维后的嵌入内传播信息，增强全局语义的提取。最终，Perceiver Resampler的低维嵌入通过线性层映射到输出维度，用于下游任务。\n\n# 4.3.2 基于自适应频率导向的颜色直方图表示\n\n本节首先介绍两种主流的肤色表征方法，即颜色平均法和固定颜色直方图表征，并分析其在肤色一致性和细节保真度方面的局限性。随后，我们提出一种基于自适应频率导向的颜色直方图方法，用于更精确地表征肤色特征，以实现输\n\n入和生成图像之间的更高色彩一致性。\n\n对于输入图像，首先提取面部皮肤区域内所有像素的  $<\\mathbf{R}, \\mathbf{G}, \\mathbf{B}>$  信息  $H$ ，得到一个  $N \\times 3$  的矩阵，其中  $N$  是面部皮肤区域的像素数量，3对应于三个颜色通道： $<\\mathbf{R}, \\mathbf{G}, \\mathbf{B}>$ 。\n\n# 1. 颜色平均表示法。\n\n这种方法简单直观，广泛应用于各种颜色控制相关的生成任务中[36]。它直接计算皮肤区域中每个通道的平均值。\n\n$$\nC _ {\\text {m e a n}} (i) = \\frac {1}{N} \\sum_ {n} H _ {n, i}, \\tag {4.4}\n$$\n\n这种方法的局限性在于它假设肤色分布是均匀的，但实际上，肤色的分布是高度多样化的。光照、阴影、肤色自然渐变等因素都会导致肤色分布的复杂化，因此颜色平均法忽略了这些细节，无法准确表现出真实肤色的信息层次。这种过度简化的处理方式会在生成任务中引发显著的色彩失真问题，导致生成图像的肤色表现不够自然或真实。如图4.6中Mean Color一列所示，生成图像中的肤色较为单一，与输入图像相比缺乏真实肤色的渐变和丰富性。\n\n# 2. 固定颜色直方图表示法。\n\n该方法首先定义一组固定的预定义颜色，然后遍历所有像素，将每个像素近似为最接近的预定义颜色。最后，统计并归一化每种预定义颜色的出现次数。具体而言：\n\n1. 初始化。通过将每个  $\\langle \\mathbf{R},\\mathbf{G},\\mathbf{B}\\rangle$  通道等间距量化为  $K$  个值来初始化颜色直方图，生成一个三维数组  $C_{FQ}$ ，其维度为  $K\\times K\\times K$ 。该数组表示每种预定义颜色的出现次数。将该矩阵初始化为零， $C_{FQ}[R_q,G_q,B_q] = 0$ 。  \n2. 统计出现次数。对于  $H_{q}$  中的每个像素  $\\langle R_{i}, G_{i}, B_{i} \\rangle$ ，识别出最接近的预定义颜色  $\\langle R_{q}, G_{q}, B_{q} \\rangle$ ，并增加对应的频率： $C_{FQ}[R_{q}, G_{q}, B_{q}] + = 1$ 。  \n3. 归一化。通过将直方图  $C_{FQ}$  中的每个值除以像素总数  $N$  来归一化，得到归一化后的直方图： $C_{FQ}[i,j,k] = \\frac{C_{FQ}[i,j,k]}{N}$ 。\n\n与颜色平均法相比，这种方法能够更好地捕捉复杂的颜色分布。然而，其准确性受限于量化锚点颜色的数量。如果量化颜色的数量非常大，例如在没有量化，也就是大约有1600万  $(255\\times 255\\times 255)$  个预定义颜色的极端情况下，高维输入会给训练带来显著挑战。因此，当前实践中使用的量化颜色相对有限，这对于自然图像可能足够，但对于如肤色等敏感信息则会引入显著误差，如图4.6中Fixed Color Histogram一列所示。\n\n# 3. 基于自适应频率的颜色直方图表示。\n\n为了解决前述颜色表示方法中的不准确性，本研究提出了一种基于自适应频率的颜色直方图表示方法。其动机在于，皮肤颜色区域主要由少数几种主要颜\n\n色组成，大多数颜色分布在这些主要颜色周围，而其他像素则可以被视为离群点，对整体肤色表示贡献不大。通过使用这些主要颜色作为预定义的目标量化颜色，绝大多数像素可以与更贴合的颜色进行匹配，从而减少量化误差。相比于传统的颜色平均方法或预定义颜色直方图方法，该方法能够更准确地捕捉图像中的肤色特征，提高颜色一致性。\n\n算法5.1中详细描述了基于自适应频率的颜色直方图生成流程。具体而言，本方法首先选择最常见的未记录颜色作为量化锚点，并统计所有接近该颜色的像素频率。接着，继续选择下一个最常见的未记录颜色，重复该过程，直到达到预设的像素覆盖率  $R$  或锚点数量  $N_{q}$  。为了排除离群颜色的影响，方法还使用像素误差范围  $\\delta$  来限定颜色的匹配范围。像素误差范围  $\\delta$  在本工作中设置为3。颜色距离函数Dis用于衡量像素点之间的距离，定义如下：\n\n$$\nD i s (r _ {i}, g _ {i}, b _ {i}, r _ {j}, g _ {j}, b _ {j}) = m i n (| r _ {i} - r _ {j} |, | g _ {i} - g _ {j} |, | b _ {i} - b _ {j} |), \\qquad \\qquad (4. 5)\n$$\n\n其中  $< r_{i}, g_{i}, b_{i} >, < r_{j}, g_{j}, b_{j}>$  分别为两个像素点。\n\n1. Input: 像素记录的覆盖率  $R$ , 量化颜色的数量  $N_{q}$ , 像素误差范围  $\\delta$ , 颜色距离函数  $Dis$\n\n2 Output: 自适应频率的颜色直方图  $C_{af} \\in \\mathbf{R}^{N_q \\times 4}$\n\n3 初始化颜色直方图矩阵  $C_{af} \\in \\mathbb{R}^{N_q \\times 4}$ ,  $C_{af}[:] = 0$ , 当前记录的像素比例  $r_c = 0$ , 记录颜色数量  $i = 0$\n\n4统计每种颜色的出现概率，并按出现频率降序排序，得到包含像素值及其频率的有序列表  $\\pmb{F}$\n\n5/\\*遍历所有未记录的颜色，记录与当前最频繁颜色  $C_{af}[i]$  接近的颜色的出现次数\\*/\n\n算法4.1 基于自适应频率导向的颜色直方图算法流程  \nwhile  $(r_c < R)$  &  $(i < N_q)$  &  $(len(F) > 0)$  do  \n $\\mathrm{D} = \\mathrm{len}(\\mathrm{F})$   \n提取最频繁的未记录颜色，将  $C_{af}[i,:3] = F[0][:3]$ $r_c \\gets r_c + F[0][3]$ .  \n从列表F中弹出  $F[j]$   \nj=0  \nwhile  $j < len(F)$  do  \nif Dis(F[j][:3], C_{af}[i,:3]) <  $\\delta$  then  \n更新当前未记录颜色出现的频率到  $C_{af}$ :  $C_{af}[i,3] = C_{af}[i,3] + F[j][3]$ .  \n从列表F中弹出  $F[j]$   \nend  \nelse  \n $j = j + 1$   \nend  \nend\n\n23 将矩阵  $C_{af}$  的频率维度进行归一化,  $C_{af}[:,3] = \\frac{C_{af}[:,3]}{\\sum C_{af}[:,3]}$ .\n\n24 return  $C_{af}$\n\n通过使用该自适应频率导向的颜色直方图，算法能够动态地选择最具代表性的肤色锚点，从而实现更加精确的肤色表示以及通过排除离群点的方式，消除\n\n了无关肤色区域的干扰。如图4.6中AF Color Histogram一列所示，本工作的表示方法能够准确地表示输入的肤色，同时排除了由不准确的肤色分割引起的噪声和无关信息(白色像素点)。\n\n![](images/dd9b4ef1f542b9943070c01d5bb61cd430e815e0db1d9736071a4767db632feb.jpg)  \n图4.6 不同颜色表示方法的重建结果\n\n# 4.3.3 基于颜色分布匹配的奖励学习\n\n尽管自适应频率导向的颜色直方图可以准确表示输入的肤色信息，但在当前基于扩散的文本生成图像模型中，生成能够反映该信息的图像仍然面临挑战。生成模型的噪声注入和多步迭代过程导致肤色信息在生成过程中被削弱或偏离原始目标，从而影响生成图像的颜色一致性。为了解决这一问题，本研究提出了颜色分布匹配奖励（Color Distribution Matching Reward, CDM Reward），以显式地保证生成图像与输入图像之间的颜色一致性。\n\n具体来说，首先通过多步推理生成最终的图像，表示如下：\n\n$$\nx _ {p r e d} = T 2 I _ {(x _ {T}, c _ {t x t}, C _ {a f} (x _ {i n p u t}))}, \\tag {4.6}\n$$\n\n其中  $T2I$  代表多步推理过程， $x_{T}$  表示初始噪声， $c_{txt}$  为输入文本， $C_{af}(x_{input})$  为参考的自适应频率导向颜色直方图。然后，通过最小化输入图像和输出图像之间\n\n的颜色分布差距，来实现奖励机制，其损失函数形式如下：\n\n$$\n\\mathcal {L} _ {\\text {c o l o r}} = D \\left(C _ {a f} \\left(x _ {\\text {i n p u t}}\\right), C _ {a f} \\left(x _ {\\text {p r e d}}\\right)\\right), \\tag {4.7}\n$$\n\n其中  $D$  表示两个颜色直方图之间的12距离。由于颜色直方图是不可微的，本工作参考了Histogan[205]中的可微分直方图方法，获取关于参考颜色的可微分颜色直方图。首先预定义颜色  $[p_0, p_1, \\dots, p_i \\dots]$ ，该集合由固定颜色直方图和自适应频率导向颜色直方图中的颜色锚点组成。然后可微分的自适应频率导向颜色直方图可以表示为：\n\n$$\nC _ {a f} \\approx [ h (p _ {0}), h (p _ {1}), \\dots , h (p _ {i}), \\dots ], \\tag {4.8}\n$$\n\n$$\nh \\left(p _ {i}\\right) = \\frac {1}{Z} \\sum_ {x} k (r (x), g (x), b (x), r \\left(p _ {i}\\right), g \\left(p _ {i}\\right), b \\left(p _ {i}\\right)), \\tag {4.9}\n$$\n\n其中  $r(x), g(x), b(x)$  分别表示颜色  $x$  的  $\\langle r, g, b \\rangle$  通道。 $Z$  为归一化项。函数  $k$  通过核函数计算  $r(x), g(x), b(x)$  与  $r(p_i), g(p_i), b(p_i)$  之间的距离。在本文中，采用了逆二次核函数：\n\n$$\nk (r (x), g (x), b (x), r \\left(p _ {i}\\right), g \\left(p _ {i}\\right), b \\left(p _ {i}\\right)) = \\prod_ {f \\in \\{r, g, b \\}} \\left(1 + \\left(\\frac {\\left| f (x) - f \\left(p _ {i}\\right) \\right|}{\\sigma}\\right) ^ {2}\\right) ^ {- 1}, \\tag {4.10}\n$$\n\n其中  $\\sigma$  为距离平滑系数，用于控制距离的敏感度，设置为0.1。\n\n# 4.3.4 模型训练和推理\n\n# 1. 模型训练\n\n本工作采用两阶段方法来训练 Skin-Adapter，以生成具有肤色可控的高质量图像。在第一阶段，使用标准的扩散损失  $\\mathcal{L}_{diffusion}$  进行模型训练；在第二阶段，引入颜色分布匹配奖励，以增强对肤色的控制能力。完整的损失函数定义如下：\n\n$$\n\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {d i f f u s i o n}} + \\lambda \\mathcal {L} _ {\\text {c o l o r}}, \\tag {4.11}\n$$\n\n其中  $\\lambda$  是奖励权重，用于平衡扩散损失和颜色分布匹配奖励。实验发现，模型在第二阶段训练时仅需要20,000次迭代就可以收敛。参考clark等人[213]的方法，在应用颜色分布匹配奖励时，推理了20步，但仅使用最后一次扩散时间步的梯度来更新参数，从而提高了训练效率。\n\n# 2. 模型推理\n\n本工作使用无分类器指导（Classifier Free Guidance, CFG)[76]来生成具有一致肤色的高质量图像。CFG通过结合条件和无条件生成信息来调控模型输出，其公式如下：\n\n$$\n\\epsilon_ {\\mathrm {p r d}} = \\epsilon_ {\\mathrm {u c}} + \\beta_ {\\mathrm {c f g}} \\left(\\epsilon_ {\\mathrm {c}} - \\epsilon_ {\\mathrm {u c}}\\right), \\tag {4.12}\n$$\n\n其中  $\\epsilon_{\\mathrm{prd}}$  代表最终模型输出结果，  $\\epsilon_{\\mathrm{uc}}$  表示来自无条件图像和空文本输入的预测， $\\epsilon_{\\mathrm{c}}$  表示条件图像和文本输入的结果。  $\\beta_{\\mathrm{cfg}}$  是CFG系数，用于控制条件信息对生成结果的影响程度。\n\n# 4.4 实验结果与分析\n\n# 4.4.1 实验设置\n\n# 1. 数据集\n\n训练数据集是LAION-5B[190]的子集。该子集经过严格筛选，旨在确保模型训练的图像数据质量和代表性。具体而言，从LAION-5B中挑选出具有美学评分较高的单人图像，并过滤掉了人脸较小的图像，最终得到约50万张图像的训练数据集。这些图像为模型的肤色控制与保真性训练提供了良好的支持。\n\n在模型的测试阶段，为了验证模型在多样化肤色上的适用性和生成质量，我们设计了一个包含多种肤色的测试数据集。测试集包含四种不同肤色的图像，分别为黄色、深色、白色和棕色，每种肤色各40张图像，共计160张。这些测试图像覆盖了不同的肤色特征，确保模型能够在不同肤色的条件下生成具有一致性和保真度的图像。此外，为了全面测试模型在不同场景下的生成表现，本研究设计了10个覆盖各种生活场景和情绪的测试提示，如表4.1所示。这些提示包括“nurse（护士）”、“chef（厨师）”、“smiling（微笑）”等职业或情绪性描述，以及“The Great Wall（长城）”、“Time Square（时代广场）”等特定场景，以考察模型在不同环境和任务需求下的生成能力。每张输入图像依据不同的提示生成4张图像，从而确保生成样本的多样性和广泛性。\n\n表 4.1 用于肤色一致性测试的文本描述  \n\n<table><tr><td>Index</td><td>Prompt</td></tr><tr><td>1</td><td>headshot, a nurse, looking at the camera, high quality, 4k</td></tr><tr><td>2</td><td>headshot, a chef, looking at the camera, high quality, 4k</td></tr><tr><td>3</td><td>headshot, a person is smiling, looking at the camera, high quality, 4k</td></tr><tr><td>4</td><td>headshot, a person wearing a shirt, looking at the camera, high quality, 4k</td></tr><tr><td>5</td><td>headshot, a person with a scarf, looking at the camera, high quality, 4k</td></tr><tr><td>6</td><td>headshot, a person in the Great Wall, looking at the camera, high quality, 4k</td></tr><tr><td>7</td><td>headshot, a person in the Time Square, looking at the camera, high quality, 4k</td></tr><tr><td>8</td><td>headshot, a person in the snow, looking at the camera, high quality, 4k</td></tr><tr><td>9</td><td>headshot, a person in the forest, looking at the camera, high quality, 4k</td></tr><tr><td>10</td><td>headshot, a person is angry, looking at the camera, high quality, 4k</td></tr></table>\n\n# 2. 指标\n\n本工作使用两种肤色分布距离评估肤色一致性，以全面衡量生成图像在肤色一致性上的表现。主要指标CDM Loss是基于自适应频率的颜色直方图距离，能够准确表示不同图像之间的颜色分布。次要指标Mean Loss是颜色平均距离，提供了肤色一致性的一个简单度量标准，作为辅助参考。\n\n# 3. 实现细节\n\n本工作采用广泛使用的Stable Diffusion 1.5作为文本生成图像的基础模型。皮肤区域通过人脸检测及五官分割模型来获取得到。\n\n训练分为两个阶段进行。第一阶段只使用常规的扩散损失进行训练，迭代20万次，第二阶段引入CDM奖励来加强肤色控制能力，训练约2万次。批量大小设置为8。实验在8张A100GPU服务器上进行。奖励权重  $\\lambda$  的大小会显著影响生成图像的质量和肤色控制效果，奖励权重过大时会影响图像的生成质量，通过实验调整参数，最终将  $\\lambda$  设置为0.001。\n\n在推理阶段，本研究使用DDIM采样器[215]，该采样器能够实现比较高效的采样，步数设置为35。引导比例设置为7.5。\n\n![](images/4fae1fa1d62cd03676dd37fe58c12900351b7f7d48c6b0e3189505851e91e894.jpg)  \n图4.7 肤色保持不同方法的定量结果比较\n\n# 4.4.2 主要实验结果\n\n本工作对比了当前不使用肤色提示信息、仅使用肤色相关提示词以及本研究提出的 Skin-Adapter 模型在文本生成图像任务中的定量结果来评估肤色一致性。\n\n如图4.7所示，从定量实验结果中可以看出，引入 Skin-Adapter 显著提高了肤色保留效果。在未使用任何肤色提示或参考图像中肤色信息的情况下，生成图像的肤色一致性较差，具体表现为 CDM Loss 为 0.478，Mean Loss 为 0.460。这种较高的损失值表明，模型在未引导的情况下无法精确匹配输入图像的肤色信息，生成图像的肤色与输入图像的差异较大。通过引入肤色相关的提示词来向模型提供肤色信息，能够在一定程度上提高生成图像的肤色一致性。在这种设置下，CDM Loss 从 0.478 改善至 0.382，Mean Loss 也从 0.460 降至 0.379。相比无提示\n\n![](images/13e651a631e4d66bb182ec6cfb83fd8b4e9cd3d8b3bdc58d49797b8ae1fc633a.jpg)  \n图4.8 Skin-Adapter生成结果样例\n\n的情况，引入肤色提示词改善了肤色保真度，CDM Loss 和 Mean Loss 分别改进了 0.096 和 0.081。进一步地，通过本研究提出的 Skin-Adapter 模型能够更加显著地提升肤色一致性。Skin-Adapter 采用了自适应频率的颜色直方图表示以及颜色分布匹配奖励机制，使得模型能够在多种提示条件下生成与用户肤色一致的图像。实验结果表明，Skin-Adapter 将 CDM Loss 降低至 0.216，Mean Loss 也降至 0.207，实现了最佳的肤色一致性。这些定量结果显示了 Skin-Adapter 在肤色一致性控制上的优越性。\n\n图4.8表明 Skin-Adapter 能够在各种不同提示条件下保持用户的肤色一致性,无论是职业、表情、服饰或场景的变化,都能生成与输入肤色一致的人物图像。在不同提示词描述的职业(如护士、厨师)、表情(如微笑、生气)、服饰(如围巾、帽子)以及场景(如长城、雪地、森林)下,Skin-Adapter 模型均能生成出符合参考肤色的高质量人物图像。这些实验结果表明了 Skin-Adapter 具有较强的鲁棒性。\n\n# 4.4.3 消融实验结果\n\n本节对比了不同颜色编码学到的肤色保持效果以说明本研究提出的自适应频率导向的颜色直方图算法的优越性。进一步地，通过对比实验表明引入设计的\n\n![](images/6ea7ec4fdfd82da66ea2418b7321b6e725c0d0ebd1e44cb682bbf028975134e9.jpg)  \n图4.9 自适应频率导向的颜色直方图算法与不同颜色编码算法的比较\n\n表 4.2 关于不同颜色表示方法的消融研究  \n\n<table><tr><td>方法</td><td>CDM 损失↓</td><td>平均颜色损失↓</td></tr><tr><td>RGB mean</td><td>0.332</td><td>0.345</td></tr><tr><td>fixed color</td><td>0.293</td><td>0.322</td></tr><tr><td>我们的方法</td><td>0.260</td><td>0.305</td></tr></table>\n\n肤色分布匹配损失后可以进一步地提升一致性。最后，展示了 Skin-Adpater 对不同文本生成图像结构的泛化性，并且由于本工作的简单有效性，只需要将该兴趣的区域从皮肤区域修改成头发区域，就可以训练对应的发色一致性模型，显示出了不错的发色保持效果。\n\n# 1. 肤色表示的有效性\n\n本节比较了不同颜色编码方法的效果，验证了自适应频率导向颜色直方图在肤色表示方面的有效性。定量和定性结果显示，与第二好的颜色直方图方法相比，如表4.2所示，本研究方法将颜色分布损失降低了  $11.2\\%$  。如图4.9所示，基于平均颜色或固定量化直方图的方法在生成的肤色与原始肤色之间表现出显著差异，其中平均颜色的方法差异最大，固定颜色直方图的差异较平均颜色的方法更小，但是仍然十分明显。而使用自适应频率导向颜色直方图后会更加准确反映原始图像中的肤色信息。\n\n表 4.3 颜色分布匹配奖励的有效性  \n\n<table><tr><td>方法</td><td>CDM 损失↓</td><td>平均颜色损失↓</td></tr><tr><td>基线</td><td>0.260</td><td>0.305</td></tr><tr><td>增加一致性奖励</td><td>0.207</td><td>0.216</td></tr></table>\n\n![](images/746b8cac99e29bd90d34088fbd9738c5c3d6397d6083f7ff8768372545b32f23.jpg)  \n图4.10 颜色分布匹配奖励的消融实验\n\n# 2. 颜色分布匹配损失的有效性\n\n为了进一步提升生成图像中的肤色一致性，本研究在基础的扩散损失训练之外，特别引入了颜色分布匹配奖励机制，以更有效地保持输入与生成图像之间的肤色特征一致性。如图4.10所示，仅依赖扩散损失训练的Skin-Adapter模型在肤色呈现上仍存在一定偏差，生成图像的肤色不总是能够准确匹配输入图像。这种不一致性可能是由于传统扩散损失在图像生成过程中，主要关注整体像素的还原，而缺乏对细粒度颜色分布的控制能力。在定量分析上，这一奖励机制带来了显著的提升。如表4.3所示，加入颜色分布匹配奖励后，CDM Loss相较于仅使用扩散损失的模型优化了  $20.3\\%$  ，Mean Loss优化了  $29.1\\%$  ，表明在生成图像的肤色一致性和还原度上均取得了显著改善。这些结果表明，颜色分布匹配奖励机制在确保生成图像的肤色一致性方面具有重要作用，使得Skin-Adapter更加适用于对肤色敏感的应用场景，提升了模型在精细化肤色控制方面的能力。\n\n# 3. 使用后处理存在的问题\n\n如图4.11所示，通过后处理方式进行肤色一致性调整虽然能够在一定程度上达到肤色匹配的效果，但往往会导致生成图像在真实自然度上有所欠缺。这种后处理方法依赖于肤色分割，将特定区域的颜色调整为目标肤色。然而，由于分割的准确性难以完全保障，生成的图像容易出现细节处理不佳的问题，例如在眼角区域可能会出现色块错位或边缘过渡生硬的瑕疵。这些不自然的瑕疵在实际使用中无法被接受。相比之下，可以发现通过 Skin-Adapter 模型进行肤色一致性控制，可以更有效地生成高质量且真实自然的图像，避免了后处理颜色调整带来的不自然性。\n\n![](images/13e6d2816c784de8af00fecba410b2256b6e74bed2d40b7d7a4c01e8d26ffda5.jpg)  \n图4.11 通过后处理进行肤色一致性调整存在的问题\n\n![](images/cc55d6779341d06db9efa75e580e376632b57f9600c77803001bea823022b36a.jpg)  \n图4.12 Skin-Adapter的SD-XL上的效果\n\n# 4. Skin-Adapter的泛化性\n\n本节展示了 Skin-Adapter 的泛化能力，验证了其在不同文本生成图像模型架构中的适用性。首先，本研究发现，Skin-Adapter 的设计不仅适用于基于 SD1.5 的文本生成图像模型，还可以扩展应用于更复杂的架构中，如 SD-XL[81]。图4.12展示了适用于 SD-XL 架构的 Skin-Adapter-XL 的结果，显示出在不同模型架构下保持一致的肤色表现能力，证明了 Skin-Adapter 的设计具有较好的通用性。\n\n此外，由于 Skin-Adapter 的颜色表示方法和损失函数设计具有高度的通用性，该方法不仅能够应用于肤色一致性任务，还可以轻松适应其他颜色保留的应用场景。通过简单调整目标区域，例如将原先定义的皮肤区域替换为头发区域，即可将同样的技术扩展至发色一致性任务，从而训练出一个专门的 Hair-color Adapter 模型，图 4.13 展示了 Hair-color Adapter 在发色保持方面的表现结果，发色在生成图像中得到了良好的还原，表明了该方法在颜色一致性控制上的灵活性和可靠性。其简单有效的设计使其具备在图像生成任务中灵活拓展的能力，能够适应多种颜色保留需求。这种灵活性不仅提高了生成模型的可用性，也为颜色一致性控制提供了有效的技术手段，满足了多样化的应用场景需求。\n\n![](images/13750e2c5deb511b2b9fe07b392879ab2dcab4f3fdc77e89d214d4aaa013cf89.jpg)  \n图4.13 Hair-colorAdapter的效果\n\n# 4.5 本章小结\n\n本研究提出了 Skin-Adapter，一种用于文本生成图像任务中保持用户肤色一致性的有效方法。Skin-Adapter 通过结合自适应频率导向的颜色直方图和颜色分布匹配奖励机制，解决了在生成图像中保留输入肤色特征的难题。具体而言，自适应频率导向的颜色直方图通过有限的颜色编码精确量化了肤色的主要像素分布，实现了更为准确的肤色表示；同时，颜色分布匹配奖励进一步增强了输入图像与生成图像之间肤色统计的一致性。相关定性和定量实验表明，Skin-Adapter 能够显著提高生成图像在肤色保真性上的表现，验证了其在肤色一致性控制中的有效性。\n\n尽管 Skin-Adapter 在保持用户肤色一致性方面取得了显著成效，但该方法在特定光照条件下仍然面临挑战。由于当前的生成过程未完全解耦颜色和光照，导致在极端光照条件（例如蓝光或红光环境）中生成的人物肤色会受到环境光的显著影响。这会使生成图像中的肤色偏蓝或偏红，难以准确还原用户在正常光照条件下的肤色特征。此现象表明，当光照条件与训练数据中的标准环境存在较大偏差时，生成模型无法在不偏离原有肤色的前提下灵活适应环境光变化，导致其在极端光照环境下的表现存在局限性。\n\n未来的研究可以从以下几个方向进行改进和拓展，以提升 Skin-Adapter 的适用性和生成质量。首先，为了更好地应对多种光照条件并减少光照对肤色的影响，未来研究可以尝试在生成过程中有效地解耦颜色和光照信息。将颜色表示转化为光照解耦的颜色空间，例如 LAB 颜色空间，可以为生成过程带来更大的灵活性，使模型在不同光照条件下生成肤色稳定的图像。此外，推理阶段可以有选择地包含或排除环境光照信息，以确保生成的人物肤色在极端光照条件下与用户输入肤色保持一致，这将显著提高模型在不同光照条件下的适用性。\n\n其次，为了提升 Skin-Adapter 的应用范围和通用性，未来可以将该方法扩展到更加广泛的图像生成任务中，不仅限于人物肤色控制。通过将自适应频率导向的颜色直方图和颜色分布匹配奖励机制应用于通用物体颜色控制任务，该方法\n\n可以为其他对象（例如服饰、道具等）提供颜色一致性保障，使其生成的图像更加符合用户的颜色偏好。这一拓展不仅能提升模型的应用多样性，还可以为具有复杂颜色要求的生成任务（如产品展示图、视觉营销内容等）带来显著的优化。\n\n最后，Skin-Adapter 在整体图像颜色控制方面也具有广阔的研究潜力。未来的研究可以探索如何将 Skin-Adapter 的机制应用于整体图像的颜色和风格调整，进一步增强生成图像的视觉一致性和多样性控制。例如，用户可以指定整体色调偏好或风格主题（如暖色调、冷色调等），而生成模型在保持输入肤色的同时，自动调整其他区域的颜色，创造出具有高度视觉和谐的图像效果。这种扩展将使 Skin-Adapter 适用于更广泛的图像生成应用场景，如艺术创作、品牌设计等，满足不同领域的个性化需求。\n\n# 第5章 基于人脸特征精确表达的人脸身份可控文生图\n\n通过生成符合特定身份和场景的人物图像在文化传播中具有独特的优势，它能够在全球范围内展示中国的风貌和人文故事，增强传播的感染力。尽管当前大规模预训练的文本生成图像模型能够生成多样且高质量的以人为中心的图像，但在精确控制人脸生成方面仍然存在挑战。本章通过研究人脸身份可控的文本生成图像模型，使得模型能够生成与参考图像一致且符合文本描述的人物图像。\n\n现有的基于编码器的人脸身份可控方法在效率和人脸相似度方面显示出了不错的效果，但生成的图像往往无法遵循文本提示且人脸相似度较低。为了解决这一问题，本工作提出了DreamIdentity方法，旨在在词嵌入空间中学习可编辑且准确的人脸身份表示。具体而言，设计了一种新颖的专用人脸身份编码器，学习人脸敏感的准确表示，该编码器利用多尺度的身份感知特征，随后通过多重嵌入投影器生成文本嵌入空间的多个令牌。此外，提出了自增强编辑性学习方法，以增强投影嵌入的可编辑性。通过构建生成的名人脸部和编辑后的名人图像对进行训练，实现了将现成的文本生成图像模型在名人中的成熟可编辑性转移到未见过的身份上。此外，大量实验表明，与标准的文本生成图像过程相比，本方法可以生成更加符合文本且保持身份特征的图像，并且几乎没有额外的时间开销。\n\n本章具体组织如下：第5.1节主要介绍了现有身份可控性的相关工作存在的人脸相似度较低和易于忽略文本信息的问题，分析其原因，然后介绍本研究中提出针对文本生成图像任务进行人脸特征精确表达的研究动机。第5.2节对相关工作进行的梳理。在第5.3节介绍算法设计，其中第5.3.1节介绍了基于多尺度多令牌的身份表示方法，增强输入与输出人脸的一致性。第5.3.2节介绍了为了提升文本跟随能力，提出的基于自增强的编辑性表示学习方法。进一步第5.4节通过广泛的定量和定性实验验证了提出方法的有效性。第5.6节是对本章的总结。\n\n# 5.1 引言\n\n基于扩散模型的文本生成图像（T2I）大模型[25-27]最近在视觉内容创作领域引发了革命性变化。在这些T2I模型的帮助下，现在可以轻松创建生动且富有表现力的人物图像。这些模型的一个颇具潜力的应用是，给定我们个人生活中的特定人脸（如家人、朋友等），它们可以使用文本提示创建与该身份在不同场景的图像。\n\n如图5.1所示，这项任务不同于标准的T2I任务，需要模型既能保持输入人脸身份（即身份保持），又能遵循文本提示（即可编辑性）。一个可行的解决方\n\n案是学习个性化的T2I模型[40,42-43]，该过程涉及通过优化其词嵌入[40]或调整模型参数[42-43]来学习将唯一词汇（记作S*）与身份关联，通常使用来自相同人脸身份的多张图像。然而，这个过程非常低效，至少需要分钟级别的优化并且占用大量内存。因此，为了摆脱每个身份的优化过程，几种最近基于编码器的方法[44,46]提出了直接将全局图像特征（通常是CLIP[6]特征）映射到词嵌入（S*）中，并通过在T2I主干中引入额外的注入分支来结合局部网格特征。然后使用重构输入图像自身作为训练目标。尽管这些方法表现出了较好的效率，但它们往往身份相似性存在不足且难以遵循文本提示。\n\n![](images/100b5588ffd3130603c422cb9ff80eeceaabcc6776b599fa45b650ba7111a8af.jpg)  \n图5.1DreamIdentity生成结果展示\n\n现有非优化方法的可编辑性不足问题源于在词嵌入空间中身份特征表示是不准确且偏向重构的。一方面，现有方法所使用的通用对象编码器（即CLIP）不适合身份保持，比如当前最好的CLIP模型在top-1人脸识别准确率上仍然远低于人脸识别模型（ $80.95\\%$  对  $87.61\\%^{[216]}$ ）。此外，分类/图像-文本匹配网络的最后一层特征难以保持身份信息，因为它主要包含用于识别的高层语义，缺乏详细的面部描述。因此，为了保持身份，这些方法尝试更改预训练的T2I模型架构以注入网格特征，又牺牲了模型的原始可编辑性。因此，图5.4中的ELITE[44]方案生成的图像在遵循文本提示方面表现不佳。另一方面，给定输入人脸图像  $I$ ，现有方法旨在学习一个唯一词汇  $S*$ ，以便  $S*$  可以重构  $I$ 。所学习的  $S*$  确实包含了关键身份信息，但它也捕获了输入图像中尽可能多的与身份无关的细节，如风格和域。这导致  $S*$  偏向于重构，而不利于编辑。\n\n为了缓解这个问题，本研究引入了一种新颖的基于编码器的方法（称为DreamIdentity），在投影的词嵌入空间中提供准确且编辑友好的表示，以增强高效的人脸身份保持图像生成的可编辑性。具体来说，为了确保投影的词嵌入是编辑友好的而非偏向重构的，本研究设计了自增强编辑性学习，将编辑任务纳入训练阶段。它利用T2I模型本身通过生成名人面部图像以及一系列目标编辑后\n\n的名人图像来构建自增强数据集。随后，设计了一个名为  $M^2$  ID编码器的专用多词多尺度身份编码器，其主干是基于ViT的网络[217]并在大规模人脸数据集上进行预训练，然后从其多尺度粗到细的特征中投影多个词嵌入。得益于准确的身份表示，本工作可以保持原始T2I模型不变，不损害其固有的编辑能力。然后，通过结合自增强数据集和典型的人脸数据集对  $M^2$  ID编码器进行训练，以学习编辑友好的身份和准确的词嵌入  $\\mathbf{S}^{*}$  。\n\n本工作的主要贡献总结如下：\n\n- 本工作指出当前基于编码器的方法在高可编辑性方面的不足的原因是在于其偏向重构且不准确的词嵌入表示。  \n- 为了实现准确表示，提出了一个新颖的  $M^2$  ID 编码器，具有多尺度特征和多嵌入投影。为了实现编辑友好性表示，本文引入了自增强编辑性学习，通过基础 T2I 模型本身生成高质量的编辑数据集。  \n- 大量实验验证了本方法的优越性，能够在保持高身份相似度的同时实现符合文本描述的人物形象生成。\n\n# 5.2 相关工作\n\n# 5.2.1 个性化图像合成\n\n文本生成图像旨在根据自然语言描述生成真实且语义一致的图像。早期的工作主要采用生成对抗网络[53]作为该任务的基础生成模型。各种基于此理论的方法被陆续提出[4,7-8,14,17,71,85-86,201],这些模型具有精心设计的文本表示、复杂的文本-图像交互以及有效的损失函数。然而，基于生成对抗网络的模型通常面临训练不稳定性和模型模式崩溃的问题，使其难以在大规模图文数据集上进行稳定的训练[19-21]。随着大语言模型扩展能力的提升[22]，如DALL-E和Parti[23-24]等自回归方法将图像量化为离散标记，并扩展以学习更通用的文本生成图像。然而，自回归方法尽管生成效果优秀，但在细节一致性和高分辨率图像生成方面存在一定局限。最近，扩散模型（如GLIDE[11]、Imagen[25]、DALL-E 2/3[26]、LDM[27]）展示了前所未有的高质量和多样化图像的生成能力和复杂语义理解能力。然而，尽管这些模型能够生成符合文本描述的高质量图像，单凭文本生成的扩散模型在特定人脸或身份的生成控制上仍然存在一定局限性，难以满足个性化、精细化的生成需求。\n\n最近的个性化生成方法，如Textual Inversion[40]、DreamBooth[42]和Custom Diffusion[43]，在定制概念生成方面表现出色，尤其当生成任务涉及特定人脸身份时，这些方法可以应用于身份控制任务。Textual Inversion通过优化新的词嵌入来表示给定的特定概念，将其与模型原有的语义空间相关联。DreamBooth和\n\nCustom Diffusion 则通过微调生成器的全部或部分参数，将稀有词汇与特定概念关联，以实现更精准的个性化生成。然而，这些方法通常依赖多张图像来定义一个身份概念，并且每个身份概念的个性化优化过程耗时较长（通常需耗时几分钟），限制了这些方法在广泛应用中的实用性和效率。\n\n为解决这一限制性问题，本研究提出了一种无需优化过程的方法，能够通过单张图像直接将人脸身份编码为词嵌入，以实现身份一致性控制。与本研究目标相似，最近的一些工作尝试使用嵌入编码器进行高效的个性化图像生成。例如，ELITE[44]、UMM-Diffusion[45]和InstantBooth[46]均使用CLIP编码器最后一层的特征将常见对象编码为词嵌入，同时ELITE和InstantBooth通过引入局部映射网络增加了对生成细节的控制，以提升生成图像的细节一致性。\n\n本研究与现有方法相比在几个方面有所创新和改进：首先，在编码器设计层面，本研究提出了一个专门的身份编码器，能够更准确地进行人脸特征编码。与传统方法使用CLIP的最后一层特征进行预测不同，所提出的身份编码器具有多尺度特征和多词嵌入映射的能力，能够捕捉输入图像中的丰富身份特征，实现更加精确和一致的人脸表征。其次，本研究设计了一种自增强编辑性学习方法，以进一步提升人脸身份在文本生成图像任务中的可编辑性，而不是仅依赖重构目标来训练编码器。通过自增强编辑性学习，模型能够在保持输入人脸身份一致的同时，更好地响应不同的编辑指令。因此，本研究方案在身份控制和可编辑性方面取得显著改进。\n\n# 5.3 算法设计\n\n本研究旨在使预训练的文本生成图像（T2I）模型能够根据一张人脸图像生成该身份在各种文本提示下的图像，确保生成图像保持独特的身份特征，并能够反映提示词中关于服装、配饰、风格或背景的变化。\n\n整体框架如图5.2所示。在一个预训练的T2I模型的基础上，为了实现快速且身份保持的图像生成，首先通过所提出的  $M^2$  ID编码器（多尺度多令牌身份编码器）将目标身份精确地编码到词嵌入空间中，生成特定于该身份的伪词嵌入(记为  $S^{*}$  )。接着将  $S^{*}$  与输入文本提示相结合，以生成符合文本描述的图像。为了防止  $S^{*}$  产生过于重构的效果，并确保在生成过程中具有较强的编辑性，提出了一种新颖的自增强编辑性学习方法，在编辑性目标上对  $M^2$  ID编码器进行训练，以增强其在不同提示条件下的可编辑性和生成多样性。\n\n在接下来的部分中，首先简要介绍使用的预训练基于扩散的文本生成图像模型，然后详细描述提出的  $M^2$  ID编码器和自增强编辑性学习。\n\n![](images/9815d475d3ed2644fc90074eb0eea9afae56a05dc8e809b0eb5e9dfa849d58b3.jpg)\n\n![](images/599c5be26f93885c3c82e3c45ce4e2dd2d8ba99baec26c004518e4fc5595d160.jpg)  \n(b) 训练数据及其目标  \n图5.2 DreamIdentity的框架图\n\n![](images/98ec41377172a330b8c47fbcc8ae511bd4fcc9ba280dbbe20839d66517c9bc21.jpg)  \n(c)  $\\mathbf{M}^2\\mathbf{ID}$  编码器\n\n# 5.3.1 预备知识\n\n# 1. Stable Diffusion 模型\n\n在这项工作中，采用了开源的Stable Diffusion 2.1-base（SD2.1）作为实验的文本生成图像模型，该模型已在数十亿张图像上进行了训练，展现了出色的图像生成质量和提示理解能力。\n\nSD 是一种潜在扩散模型 (LDM)[27]。LDM 首先通过变分自编码器 (VAE)[32] 将输入图像  $x$  表示在低分辨率的潜在空间  $z$  中，然后训练一个基于文本条件的扩散模型，以根据文本输入  $c$  生成目标图像的潜在代码。该扩散模型的损失函数可以表示为：\n\n$$\n\\mathcal {L} _ {d i f f u s i o n} = \\mathbb {E} _ {\\epsilon , z, c, t} [ \\| \\epsilon - \\epsilon_ {\\theta} (z _ {t}, c, t) \\| _ {2} ^ {2} ], \\tag {5.1}\n$$\n\n其中  $\\epsilon_{\\theta}$  是模型预测的带有可学习参数  $\\theta$  的噪声， $\\epsilon$  是从标准正态分布中采样的噪声， $t$  是时间步， $z_{t}$  是时间步  $t$  时的噪声潜在变量。\n\n在推理过程中，扩散模型通过递归去噪操作生成目标图像的潜在代码，完成潜在空间中的图像生成过程。随后，解码器将生成的潜在代码从低维潜在空间映射回原始的图像空间，以获得最终的高分辨率图像。这种设计在降低生成计算需求的同时，能通过VAE的潜在空间压缩特性保留图像的细节信息，使LDM可以在较高分辨率上生成清晰、语义一致的图像。\n\n# 2. 人脸识别模型\n\n我们以ArcFace这一代表性的工作为例介绍人脸识别模型的训练过程。该流程以带标签的人脸图像数据集为输入，利用中心损失和角度间隔约束来提高模型的辨别能力。训练过程分为以下几个步骤。\n\n首先，输入的面部图像经过一系列数据预处理操作，以保证模型能够关注人脸特征学习。这些预处理操作包括图像缩放、归一化和数据增强（如随机裁剪、水平翻转等），以增加数据的多样性并减少过拟合的风险。预处理后的图像被输入到预定义的深度卷积神经网络（如ResNet、VIT等）中，提取出表示输入图像的特征向量，并进行归一化表示。\n\n接下来，为了使模型在特征空间中学习到具有更强区分性的面部表示，Arc-Face引入了角度间隔约束。在标准Softmax损失的基础上，ArcFace通过增加角度间隔参数  $m$  来构造一个基于角度的软间隔损失函数。具体来说，ArcFace将特征向量与类别权重的余弦相似度替代原始Softmax中的线性变换部分，并将分类决策边界通过添加角度间隔来进行扩展。通过这种方式，ArcFace将类别间的决策边界从线性分隔转变为角度间隔分隔，使得同一类别样本在特征空间中更加紧密聚集，不同类别样本之间的间隔更加显著。损失函数公式如下所示：\n\n$$\nL = - \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\log \\frac {e ^ {s \\left(\\cos \\left(\\theta_ {y _ {i}} + m\\right)\\right)}}{e ^ {s \\left(\\cos \\left(\\theta_ {y _ {i}} + m\\right)\\right)} + \\sum_ {j \\neq y _ {i}} e ^ {s \\cos \\theta_ {j}}}, \\tag {5.2}\n$$\n\n其中， $N$  为批量大小， $s$  为尺度因子， $\\theta_{y_i}$  为样本  $i$  与其真实类别权重的夹角， $m$  为角度间隔参数， $\\cos \\theta_j$  表示样本  $i$  与其他类别权重的夹角。该损失函数使得特征向量能够逐步学习到具有较大类别间间隔和较小类内间隔的特征分布。\n\n经过上述训练过程，ArcFace模型能够学习到区分性极强的人脸特征，这些特征在特征空间中具有良好的聚类效果，使得同一类别的样本特征更加集中、不同类别之间的特征间隔显著。训练完成的ArcFace模型能够用于各种面部识别任务，凭借其强大的区分能力实现高精度的身份验证与识别。\n\n# 3. 通用预训练视觉表征模型\n\n通用预训练的视觉表征模型，特别是CLIP和DINO模型，近年来在计算机视觉领域引起了广泛关注。这些模型通过预训练阶段学习到的通用特征表示，能够在图像分类、目标检测、图像生成等任务中迁移并展现出强大的性能\n\nCLIP模型。由OpenAI提出，通过对比学习方式在大规模图像-文本数据集上进行预训练，能生成强大的视觉语言表征。CLIP模型的关键思想是利用图像和对应的文本描述作为正样本对，其他不匹配的图像-文本对则作为负样本进行对比学习。具体来说，CLIP采用两个独立的编码器（一个用于图像，一个用于文本）对输入图像和文本进行嵌入。通过最大化配对图像和文本之间的余弦相似度，同时最小化不配对样本之间的相似度，CLIP在训练中学会了视觉和语言之间的对应关系。\n\nDINO模型。由Meta（FacebookAI）提出的一种自监督学习模型，旨在通过自蒸馏（self-distillation）方法在无标签数据上学习视觉表征。DINO的核心思想是利用自监督学习技术，使模型在无标签数据中获得自适应的特征表示。DINO通\n\n过两部分构成的教师-学生模型结构实现了这种蒸馏学习。教师网络在训练中逐步学习到更稳定的表征，而学生网络则通过学习教师网络的输出来获得图像特征。DINO 的另一个关键创新在于它使用了 Vision Transformer (ViT) 作为模型主干。与传统卷积神经网络（CNN）不同，ViT 在处理图像时不使用卷积操作，而是将图像划分为若干小块（patch），并逐步在各块之间捕捉全局上下文。DINO在这种架构下能够捕获更加全局的图像特征，实现了在自监督场景下的高效特征学习。\n\n由于通用预训练视觉表征模型缺乏在人脸垂类场景的训练，其往往对人脸身份敏感的程度不够。本文提出采样人脸识别模型进行生成过程中的人脸特征表示器。后文对比实验也验证了这一选择的有效性。\n\n# 5.3.2 基于多尺度多令牌的身份表示\n\n为了在不修改预训练的文本生成图像（T2I）模型架构的情况下，在词嵌入空间中准确表示输入人脸的身份特征，本研究提出了一种创新的多词多尺度嵌入ID编码器（ $M^2$  ID编码器）。该编码器能够提取专门用于身份识别的多尺度特征，并将这些特征投影到多词嵌入中，以实现更加精准的身份表示。\n\n主干网络。准确表示面部身份信息对于生成任务至关重要。然而，现有的文本生成图像方法普遍依赖通用图像编码器，如CLIP，其在面部身份表征方面存在显著不足。这是因为CLIP并非专为面部识别任务设计，难以捕捉到精细的身份特征，而面部识别任务的编码器通常在大规模人脸数据集上训练，能够更准确地提取身份特征。正如[216]所示，当前表现最好的CLIP模型（如ViT-L/14）在top-1人脸识别准确率上仍然明显落后于专门的人脸识别模型（80.95%对87.61%）。因此，本研究采用在大规模人脸识别数据集上预训练的ViT主干网络[217]作为ID编码器的基础，确保能够从输入人脸图像中提取出具有高辨识度的身份特征。\n\n多尺度特征。在大多数情况下，仅使用主干网络最后一层的输出特征向量  $\\vec{v}_{final}$  来表示身份，可能会导致次优的身份保持效果。具体而言， $\\vec{v}_{final}$  通常偏向于捕获高层语义信息，更适合于分类任务（如人脸识别），而在生成任务中可能会缺乏低层次细节。例如，对于相同身份但不同表情的图像，面部识别模型倾向于生成相似的特征表示，以减少类别内差异，但生成任务则需要这些细节信息来再现丰富的面部表情特征。因此，单一的高层次表示可能会在图像生成任务中形成信息瓶颈。为了解决这一问题，本研究提出利用来自多层的特征表示以更全面地捕捉身份信息。具体来说， $M^2$  ID编码器结合了来自第3、6、9和12层的特征嵌入，形成多尺度特征向量  $(\\vec{v}_3,\\vec{v}_6,\\vec{v}_9,\\vec{v}_{12})$  ，最后与  $v_{final}$  一起组成多尺度身份表示向量组  $V$  ，定义如下：\n\n$$\nV = [ \\vec {v} _ {3}, \\vec {v} _ {6}, \\vec {v} _ {9}, \\vec {v} _ {1 2}, \\vec {v} _ {f i n a l} ]. \\tag {5.3}\n$$\n\n多令牌嵌入。为了将多尺度身份特征投影到T2I模型的词嵌入空间中，并确保其原有的泛化能力和可编辑性，本研究提出了一种多令牌嵌入机制，以多词表示身份特征。由于单个词嵌入令牌难以全面描述复杂的面部身份信息，本研究通过多令牌机制让多个词嵌入表示同一身份。具体地，每个身份向量  $V$  通过独立的多层感知机（MLP）进行投影，生成多个词嵌入：\n\n$$\ns _ {i} = M L P _ {i} (V), \\text {对 于} i = 1, \\dots , k, \\tag {5.4}\n$$\n\n其中  $k$  表示嵌入的数量。根据实验结果，本研究将  $k$  设为2，以保持生成效果和计算效率之间的平衡。同时，参考[218]的方法，为了防止投影词嵌入向量的过度偏离，进一步对输出嵌入添加了  $l_{2}$  正则化项，使得每个令牌嵌入具有一定的约束，从而提升模型的稳定性和泛化能力。正则化损失表示如下：\n\n$$\n\\mathcal {L} _ {r e g} = \\sum_ {i = 1} ^ {k} \\| s _ {i} \\|. \\tag {5.5}\n$$\n\n得益于上述专用的ID特征，本工作能够在嵌入空间中实现高度的身份保持控制，而不会因特征注入而牺牲预训练T2I模型的可编辑性。\n\n# 5.3.3 基于自增强的编辑性表示学习\n\n基于重构的目标在训练过程中可能会导致学习到的  $S^{*}$  更倾向于直接重构图像，即  $S^{*}$  不仅仅表示该人物的身份信息，而且还表示了其风格，如对于一张真实输入的人脸图像而言， $S^{*}$  还包含了写实风格以及人脸域的信息，而这会限制输入人脸图像的可编辑性。当用户输入风格话相关的 prompt 时，很难生成风格化的结果，而仍然维持真实的风格。因此在重构损失范式下生成的图片很难符合文本的描述。\n\n一种直观的改进方法是将编辑任务纳入训练过程，以提升可编辑性。然而，为这种编辑任务收集成对的数据是非常困难的。因此，本研究提出了一种简单而有效的自增强编辑性学习方法，即通过预训练模型自身生成编辑数据集来实现对编辑性的学习。本工作观察到当前最先进的文本生成图像模型能够在各种背景下生成名人的图像（例如，Boris Johnson, Emma Watson），并保持较高的身份保持和文本一致性。基于这个见解，自增强编辑性学习利用预训练模型本身，通过生成各种名人面孔及其目标编辑后的名人图像来构建一个自增强数据集，该数据集将用于以可编辑性为目标训练  $M^2$  ID编码器。正式地，该数据集的构建包括以下四个步骤：\n\n步骤1：名人列表生成。首先，收集一个候选名人列表。为此，使用大语言模型（即ChatGPT）自动生成四个不同领域（运动员、歌手、演员和政治家）中各自最\n\n著名的400个人名。经过去重处理后，最终获得了包含1015个名人的名单，这些名人涵盖了不同领域，代表了多种身份和背景。\n\n步骤2：名人面孔生成。使用生成的面部图像而非真实图像，因为模型对这些名人有自己的理解。具体来说，在StableDiffusion训练数据集中出现频率较低的名人，其生成的图像与真实人物不太相似，而这些生成的面孔保持了较高的身份相似度。本工作使用提示模板“[名人名字]face, looking at the camera”来生成源图像，然后进行面部裁剪和对齐操作以获得仅包含面部的图像。此外，如果图像的短边长度小于128像素，则忽略该人脸图像，以确保面部特征的清晰度。\n\n步骤3：编辑提示和编辑图像生成。本工作手动设计了各种提示，这些提示包含在不同职业、风格和配饰下的名人图像（例如，“[名人名字] as a chef”，“Oil painting style，[名人名字] face”）。然后这些提示通过T2I模型生成编辑图像，并且在提示中将[名人名字]替换为伪词  $S^{*}$  作为编辑提示。编辑性的文本提示如表5.1所示。\n\n表 5.1 用于训练 DreamIdentity 模型的文本描述集  \n\n<table><tr><td>Index</td><td>Prompt</td></tr><tr><td>1</td><td>Oil painting style, S* face</td></tr><tr><td>2</td><td>Watercolor style, S* face</td></tr><tr><td>3</td><td>Pencil art style, S* face</td></tr><tr><td>4</td><td>Fauvism painting, S* face</td></tr><tr><td>5</td><td>S* as a wizard, looking at the camera</td></tr><tr><td>6</td><td>S* wearing a hat, looking at the camera</td></tr><tr><td>7</td><td>S* as a chef, looking at the camera</td></tr><tr><td>8</td><td>S* as a nurse, looking at the camera</td></tr></table>\n\n步骤4：数据清理。通过上述步骤，可以获得由一组三元组[身份面孔，编辑提示，编辑图像]组成的初始自增强数据集。由于当前扩散模型的不稳定性，编辑后的图像并不总是遵循编辑指令。因此，需要过滤掉自增强数据集中的噪声数据。本工作使用反映身份相似性和文本-图像一致性的ID Loss和CLIP分数作为指标，对每个提示的编辑图像进行排序，然后保留排名前  $25\\%$  的三元组作为最终训练集，从而去除噪声数据，保证数据质量。\n\n最后，如图5.3所示，本研究构建了一个高质量的自增强数据集，该数据集包含大约  $36\\mathrm{k}$  个由预训练T2I模型生成的训练示例。该数据集随后用于以编辑为导向的训练，提升  $M^2$  ID编码器在多样化提示条件下的编辑性和泛化性，从而实现身份保持和高可编辑性目标。\n\n![](images/9c1466531dc6c77bbcb677a3e8ffeb7fcecd09dac8e508da286e3e981868c893.jpg)  \nFauvism painting  \nAs a nurse\n\n![](images/ef35ec51425d25cc401c3432d963f9bd1c52905ac22ce846b932e2244efdf979.jpg)  \nAs a wizard\n\n![](images/8c040b558d11e0c720142fed20ad098aa9cd1f691699f84b80d78968b112d594.jpg)\n\n![](images/e3e85c30d0e27baea017758770472cbdbd850b037b6b976a6ad4fb4e307bd22d.jpg)  \nWearing a hat\n\n![](images/0405ff81d2185498a442e50ed01a4648281fa997034ea0c56a4143ba458b81de.jpg)  \nOil painting Style\n\n![](images/07a07bc33243b39dce2a9a4f90d83ebd2374152ab2fae1af8f6ec12d9cfcf0a7.jpg)  \nWatercolor Style  \nAs a police\n\n![](images/54f4e5fecd399035d6a0f113b8f35b96bfb6176b82abc1487fc0517ae7aef43c.jpg)  \nPencil art style  \n图5.3 自增强数据集示例\n\n![](images/d14e0f8e0fec962a19353edbbbb41d353f762b2fcab1b2ce8ab75bcb19027199.jpg)\n\n# 算法5.1 DreamIdentity算法训练流程\n\n1 Input: 预训练模型  $M_{t2i}$ , 文本一致性评估模型  $M_{text}$ , 人脸一致性评估模型  $M_{face}$ , 重构训练数据集  $D_{recon}$  \n2 Output: 人脸身份可控的文本生成图像模型  $M_{final}$  \n3 /* 第一阶段：构建自增强的编辑性数据集 */  \n4 根据名人列表生成名人数据集  $I_{face}$  \n5 根据名人列表和编辑性文本  $T_{edit}$  生成编辑性目标图像数据  $I_{edit}$ ，构成输入人脸，编辑文本和目标图像三元组，  $< I_{face}, T_{edit}, I_{edit} >$  \n6 按照文本一致性评估模型  $M_{text}$  和人脸一致性评估模型  $M_{face}$  计算得到的文本/人脸一致性指标，选择 top-k 数据作为最终的编辑性训练数据  $D_{edit}$ 。  \n7/*第二阶段：训练多尺度多令牌的编码器*/  \n8 while  $M_{\\text{final}}$  没有收敛 do  \n9 从训练集  $D_{edit}$  或者  $D_{recon}$  中采样  $\\langle i_{face}, t_{edit}, i_{edit} \\rangle$  作为训练数据，其中  $\\langle i_{face}, t_{edit} \\rangle$  是输入， $i_{edit}$  是输出。  \n10 按照公式5.6为损失函数，更新模型  $M_{final}$  \n11 end  \n12 return  $M_{final}$\n\n# 5.3.4 模型训练\n\n本研究将  $\\mathsf{FFHQ}^{[63]}$  数据集与自增强数据集结合，形成了一个包含106k个样本的综合训练数据集，以充分利用FFHQ数据集中高质量、多样化的面部图像以及自增强数据集中编辑多样性和身份保真度的优势。其中，FFHQ数据集为模型提供了70k张标准人脸图像，涵盖了不同年龄、性别和种族的面部特征，保证了模型的基础人脸识别识别和生成能力。而自增强数据集则提供了36k张由预训练的\n\nT2I模型生成的面部图像，包含丰富的编辑属性，以增强模型在身份保持的同时对多样化文本提示的响应能力。通过将这两种数据来源结合，模型能够同时学习到丰富的身份和编辑性特征，从而在不同场景下生成更符合输入身份的图像。\n\n在训练过程中， $M^2$  ID编码器的总损失函数由两部分组成：扩散模型的噪声预测损失和嵌入正则化损失。噪声预测损失  $\\mathcal{L}_{diffusion}$  用于优化扩散模型对高质量一致性图像的生成能力，使模型能够在逐步去噪的过程中恢复高质量的图像内容。而嵌入正则化损失  $\\mathcal{L}_{reg}$  则用于约束多词嵌入向量的范数，以确保模型在保持身份一致性的同时能够在词嵌入空间中平衡稳定性和可编辑性。总损失函数定义如下：\n\n$$\n\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {d i f f u s i o n}} + \\lambda \\mathcal {L} _ {\\text {r e g}}, \\tag {5.6}\n$$\n\n其中  $\\lambda$  是嵌入正则化权重，用于控制嵌入正则化损失对总损失的影响程度。合理设置  $\\lambda$  的值可以在增强身份一致性和生成多样性之间取得平衡，确保模型既能保持输入身份特征，又能响应编辑提示的多样性需求。\n\n# 5.4 实验结果与分析\n\n# 5.4.1 实验设置\n\n数据集。本研究在广泛使用的FFHQ数据集[63]上进行，该数据集包含  $70k$  高分辨率的人脸图像，涵盖了多样化的年龄、性别、种族和表情的人脸图像，是生成任务的标准数据集之一。为了适应模型的输入需求并保持一致的训练质量，本研究将FFHQ数据集中的图像调整为  $512 \\times 512$  的分辨率。\n\n在测试阶段，为了对模型的性能进行严格评估，本研究从CelebA-HQ数据集[219]中随机挑选了100张图像作为测试集，确保测试图像的多样性和高质量。为避免训练集与测试集之间的重叠影响结果的客观性，本研究人工仔细检查了每一张测试图像，特别是确保测试集中没有任何图像与自增强名人数据集中的图像重叠。\n\n评价指标。本研究从文本一致性和人脸相似度两个方面对所提出的方法进行评估，以衡量模型在生成符合编辑提示的图像同时保持人脸身份特征上的表现。\n\n文本一致性指生成图像在多大程度上反映编辑提示中指定的语义信息，即生成结果是否准确符合文本描述。为此，使用CLIP模型的文本-图像嵌入空间计算生成图像与输入文本提示之间的相似性。具体地，CLIP模型将生成的图像和对应的编辑提示文本嵌入到相同的特征空间中，并通过余弦距离来衡量二者的相似程度。较小的余弦距离表示文本描述和生成图像的匹配度更高，从而表明生成图像在反映编辑提示语义方面的表现更好。\n\n人脸相似度用于衡量生成图像在多大程度上保留了输入人脸的身份特征。具\n\n体而言，采用ArcFace模型[220]提取输入图像和生成图像中裁剪的人脸区域的ID特征向量。通过计算输入面部和生成图像中裁剪面部的ID特征向量的余弦距离来衡量二者的相似度。较小的余弦距离表明生成图像在人脸ID的一致性上表现较好，成功保留了输入人脸的独特身份特征。\n\n此外，本研究还引入了编码时间作为评估指标，以量化获得学习人物身份所需的时间。编码时间用于衡量在输入图像上执行身份编码的速度，能够反映该方法在实际应用中的效率。为了减少生成过程中的随机性和提高评估的稳定性，本研究针对每个编辑提示和输入人脸身份生成四张图像，取其平均值作为最终评估结果。\n\n实现细节。本工作选择Stable Diffusion 2.1-base作为基础文本生成图像模型，以充分利用其在文本到图像生成任务中的强大性能。学习率和批量大小分别设置为0.00005和64。编码器共训练约60,000次。嵌入正则化权重  $\\lambda$  设置为0.0001。实验在配备八块A100-80G GPU的服务器上进行，大约需要一天时间进行训练。在推理阶段，为了进一步提高生成图像的质量和对输入文本的响应性，采用了DDIM采样器[215]进行去噪处理。采样步数设置为30。引导尺度（cfg）设为7.5，以强化生成图像的文本一致性，使生成图像更加符合输入提示的语义描述。\n\n![](images/581886898eff900106e1d98181ec3d8d9e9f5985a8daa2a69fd6f8ac963d3c56.jpg)  \n图5.4DreamIdentity与其他方法的定性对比\n\n# 5.4.2 主要实验结果\n\n在本节中，本研究将所提出的方法与几种现有的个性化文本生成图像方法进行了详细比较，包括基于优化的方法Textual Inversion[40]、DreamBooth[42]、以及高效个性化模型E4T[218]。其中，Textual Inversion和DreamBooth均依赖于对模型进行参数优化以实现个性化生成，而E4T通过对每张面部图像执行约15次迭代优化来生成个性化图像。为了提供更全面的比较，还将本方法与当前基于编码器的最新方法ELITE[44]进行了对比。ELITE通过直接编码输入图像来生成个性化图像，避免了反复优化的过程。为了确保实验的公平性，本研究使用单一的人脸图像作为输入对各个方法进行评估。在算法实现上，本研究采用了广泛使用的开源Diffusers代码库实现了Textual Inversion和DreamBooth，对于E4T和ELITE方法，本研究在原始论文的基础上进行了重新复现。\n\n表5.2  \n\n<table><tr><td>Methods</td><td>文本一致性↑</td><td>人脸相似度↑</td><td>编码时间↓</td></tr><tr><td>TI</td><td>0.213</td><td>0.326</td><td>20 min</td></tr><tr><td>DB</td><td>0.217</td><td>0.425</td><td>4 min</td></tr><tr><td>E4T</td><td>0.220</td><td>0.420</td><td>20 s</td></tr><tr><td>Elite</td><td>0.196</td><td>0.450</td><td>0.05 s</td></tr><tr><td>本方法</td><td>0.228</td><td>0.467</td><td>0.04 s</td></tr></table>\n\n![](images/1a8457fd54a44aa0d9ac2cd491696101f9adcc032d64d7c5c1562b8174cc75cb.jpg)\n\n![](images/9e18c71d38be4a3921b757ac637ae98e185ea9dceb111f7640852e008c13a840.jpg)\n\n![](images/0a2ad1cea7ea229dc6c91fc6a59df83f273e8497a6bf981c2fa55b32ff06bc7f.jpg)  \n输入\n\n![](images/0201052c3003699a611c32f97c977ae35ba74bd7f8386866a8d3c203ffad1ae9.jpg)\n\n![](images/ab6e19431f86232045387261a2a81c0f7a65e791700303cf8004645822ee4100.jpg)\n\n![](images/30e92558136798246eb91df1efe6ee55071fcf090fba86ad0f2006f01af63369.jpg)  \n无编辑性学习\n\n![](images/c3a0ff34af19702296d28eecc45ede488452489b3f038eeb908da904b34dc9e9.jpg)\n\n![](images/46f224054ab1f4c3a370ff6cc38352b1c7fc2cf08971ee6fc62c8eb0d9d6385f.jpg)\n\n![](images/2850a7bcfc7289fb1f5e8619c134f9c72ad14313f199332d5d6f1c9efd568c6d.jpg)  \n无重构学习\n\n![](images/5578825fc601bc645c09060b0356cc20e20ee31fe52a07dae07071e5ccc9708d.jpg)\n\n![](images/da0042697d95ff1799a39fa0e80fcdf88a3a093cabeb7e488771684c5b84a074.jpg)\n\n![](images/80c864471c274f8571cb0a4f46cbc5a875e64ca772cffff144c8101362555045.jpg)  \n本方法效果  \n图5.5 自增强编辑学习的作用\n\n定量和定性结果。如表5.2所示，本方法DreamIdentity在所有指标上都优于近期\n\n的方法，在文本一致性、人脸相似度和编码速度方面表现均更优。DreamIdentity通过编辑友好的词嵌入  $S^{*}$ ，在文本一致性上相比第二好的  $\\mathrm{E4T}^{[218]}$  提升了  $7\\%$ ，表明本方法在生成符合文本描述的图像上具有显著优势。同时，由于准确的身份表示，DreamIdentity在人脸相似度上比第二好的模型[44]提高了  $3.7\\%$ ，并且享有更好的可编辑性。\n\n在计算效率方面，由于DreamIdentity采用直接编码的方式表示输入身份，而非为每一个词嵌入  $S^{*}$  进行优化，额外的计算成本被有效控制，每个面部身份的编码时间仅为0.04秒。相比于传统基于扩散的文本生成图像方法需要的秒级计算时间，DreamIdentity的编码速度几乎可以忽略不计，相比优化的方案，显著提高了整体生成效率。\n\n图5.4中展示的定性结果进一步验证了DreamIdentity在文本一致性和可编辑性方面的优势。特别是在生成符合提示描述的图像方面，DreamIdentity明显优于基于编码器的方法ELITE。例如，DreamIdentity成功捕捉了第1行中提示的厨师装束、第2行中的Funko Pop风格，以及第4行中的雪景环境，充分展示了其文本驱动的多样化生成能力。相比之下，ELITE则表现出对输入图像的过拟合，未能有效响应文本提示的多样化需求，从而限制了生成图像的编辑性。\n\n# 5.4.3 消融实验结果\n\n在本节中，进行消融研究，以验证提出的自增强编辑性学习和  $M^2$  ID特征的有效性。\n\n![](images/7824701c8ce0d70093114eb82554068a289536df34c6309b9b0a91435755ed15.jpg)  \n图5.6 ID编码器和多尺度特征的有效性\n\n自增强编辑性学习。图5.5中编辑提示为“S*作为警察，直视摄像头”。“w/o edit”\n\n表 5.3 自增强编辑性学习的消融实验  \n\n<table><tr><td>Recon</td><td>self-aug</td><td>文本一致性↑</td><td>人脸相似度↑</td></tr><tr><td>✓</td><td></td><td>0.213</td><td>0.380</td></tr><tr><td></td><td>✓</td><td>0.216</td><td>0.348</td></tr><tr><td>✓</td><td>✓</td><td>0.228</td><td>0.467</td></tr></table>\n\n和“w/o recon”分别表示编码器在没有可编辑性学习目标和没有重建学习的情况下进行训练。可以看出可以看出，当模型仅依赖重构目标进行训练时，嵌入的可编辑性受到较大限制。具体而言，在这种情况下，模型缺乏对多样化提示的响应能力，输入身份难以得到灵活的编辑。例如，未进行自增强编辑性学习的模型无法根据文本提示将输入身份编辑为“警察”形象，这表明仅依靠重构训练无法满足对身份编辑的需求。此外，若仅利用自增强编辑数据集进行训练，尽管模型获得了一定的编辑性，但由于自增强数据集包含的身份数量有限（约1000个面部ID），导致人脸相似度下降，生成图像的身份保持效果不佳。\n\n通过结合重构数据（即FFHQ数据集）和生成的自增强数据集，模型在训练中获得了更全面的特征学习能力，既保留了面部相似度，又具备响应文本指令的编辑性能力。表5.3中的定量结果进一步证实了这一结论。具体来说，联合使用重构和自增强编辑性学习时，模型在文本一致性和人脸相似度上均取得了显著提升，其中文本一致性达到0.228，人脸相似度提高到0.467，这表明模型不仅能精确生成符合文本描述的图像，还能在生成过程中有效保持输入身份的特征。\n\n$M^2$  ID编码器。为了验证  $M^2$  ID编码器的有效性，本研究进行了消融实验，对比了不同特征编码方法的性能表现。如表5.4所示，当模型从通用的CLIP编码器切换为专门用于人脸识别的ID编码器时，模型在文本一致性和人脸相似度上的表现均得到了显著提升。具体来说，使用ID编码器使文本一致性从0.266提升至0.302，表明ID编码器能够在生成过程中更加准确地捕捉到身份特征，并在多样化的文本提示下保持一致。此外，将多尺度（MS）特征集成到模型中进一步提升了人脸相似度指标，使其达到了0.412，显示了多尺度特征在捕捉丰富的身份信息方面的优势。图5.6进一步展示了  $M^2$  ID编码器在保持身份一致性上的有效性。\n\n为了进一步提升身份相似度，本研究还在  $M^2$  ID编码器中引入了多词嵌入机制，允许使用多个词嵌入来表示身份特征。图5.7显示了多词嵌入对ID相似度有明显提升的作用。如表5.4所示，当嵌入数量增加到2时，人脸相似度提高了 $12\\%$  ，而文本一致性仅发生了  $0.4\\%$  的微小变化。这表明多词嵌入在提升ID一致性方面具有显著优势，同时并未显著影响文本对齐度。然而，进一步增加嵌入数量导致文本一致性下降了  $17\\%$  ，这一现象表明过多的词嵌入可能包含了超出ID\n\n表 5.4 多尺度多令牌编码器的消融实验  \n\n<table><tr><td>ID</td><td>MS</td><td>ME</td><td>文本一致性↑</td><td>人脸相似度↑</td></tr><tr><td></td><td></td><td></td><td>0.229</td><td>0.266</td></tr><tr><td>✓</td><td></td><td></td><td>0.228</td><td>0.302</td></tr><tr><td>✓</td><td>✓</td><td></td><td>0.229</td><td>0.412</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>0.228</td><td>0.467</td></tr></table>\n\n特征的冗余信息，从而削弱了生成过程中的可编辑性。基于这一观察，本研究选择将嵌入数量设为2，以在提升人脸相似度的同时避免可编辑性的下降。\n\n表 5.5 编码令牌数量对生成结果的定量影响  \n\n<table><tr><td>Emb Num</td><td>文本一致性↑</td><td>人脸相似度↑</td></tr><tr><td>1</td><td>0.229</td><td>0.412</td></tr><tr><td>2</td><td>0.228</td><td>0.467</td></tr><tr><td>3</td><td>0.188</td><td>0.472</td></tr></table>\n\n![](images/f7a02084f7cb300119ea15128f3c1870568fc373f810211b4d5f7ac83c9059ed.jpg)\n\n![](images/5f6152871f1425121ebe05cee94f782b70426099c9b7f3a9ac55ea862a1f39c7.jpg)  \n输入\n\n![](images/2efe1b9d4cfe9f2b73172c69902cac4e0e2313ec034546a80d2ea48047f74f11.jpg)\n\n![](images/ce74d35501e56f40c4b3831cd63f8a4f7e8191687217cc3544a39b15cd35fe61.jpg)  \nNUM=1\n\n![](images/fe615c446b29b67b3cdd3e316510503ae82ed203887b4b431d7f873d8462672a.jpg)\n\n![](images/d8fec540ffa04de90a79b7ef91bed41898fd56ea09e64b9be9f03bf90393bc96.jpg)  \nNUM=2\n\n![](images/95795025a330fa85849a138ee59435a5e24bf9d61172570791107c90a472b27c.jpg)\n\n![](images/413102e3969834b9c1e8ee352075d84be84891ac4b4b9cfe77be2b0db5cb3883.jpg)  \nNUM=3  \n图5.7 编码令牌数量对生成结果的影响\n\nID 保持场景切换。如图5.9所示，给定输入的面部 ID 及其在画布上的位置（由视线位置指示），可以生成一系列不同场景的图像，这些图像在身份信息和头部位置上保持一致，并借助 ControlNet[98]实现。生成的场景由文本描述指定，涵盖不同的配饰、发型、背景和风格。通过这种方法，可以实现“同一身份人物在任何地方”的效果。\n\n本研究通过构建ID保持的场景切换能力，实现了“同一身份人物在不同场景下”的生成效果。具体而言，给定输入的面部ID及其在画布中的位置，模型能够生成一系列不同场景下身份信息和头部位置上一致的图像。除了本研究方案用来控制人物身份外，ControlNet[98]用来精准控制头部位置。文本描述指定相应的场景，可以涵盖不同的配饰、发型、背景和风格。如图5.9所示，模型生成了\n\n![](images/8d812de998f40870c7d1c6ef78795de60290949cfa9400a4b793eae9c2c37a38.jpg)  \n图5.8 身份保持的场景切换与其他方案进行比较\n\n同一身份在不同场景下的图像，成功展示了身份保持和可编辑性的效果。这种灵活性使得生成结果能够适应各种主题需求，降低图像创作门槛\n\n此外，本研究进一步将所提出的方法与学术界常用的Instruct-Pix2Pix模型进行了对比，如图5.8所示。InstructPix2Pix[221]面临着身份保持的挑战，无法在不同场景变化中保持输入身份的特征一致性。而所提出的方案能够在不同场景下生成符合输入身份的图像，充分展示了本方法在身份一致性控制方面的优势。\n\n# 5.5 本章小结\n\n本研究提出了一种编辑友好的基于编码器的方法DreamIdentity，仅需一张人脸图像即可生成指定人物在新场景中的图像。针对在精确控制人脸身份和响应文本编辑指令方面面临的挑战，本研究引入了一种自增强编辑性学习机制。该机制通过构建自增强数据集，将多种编辑任务融入训练过程中，使得模型在处理不同文本提示时能够保持身份一致性的同时，具备强大的可编辑性。此外，本研究设计了创新性的  $M^2$  ID编码器，通过多尺度的身份感知特征，将人物身份信\n\n![](images/fd04638d95e2e85300a60b6ea228f4880dcb1c8b35272879c1d314b4080864f8.jpg)  \n图5.9 同一身份在不同场景下的生成结果\n\n息投影到多个词嵌入中，实现了对人物的精确表示，且几乎没有额外的时间开销。大量实验验证了所提出方法的有效性。与当前的优化方法和编码器方法相比，DreamIdentity在文本一致性、人脸相似度以及编码效率方面均表现出显著优势。实验结果表明，DreamIdentity不仅能够准确保持人物身份特征，还具备灵活的编辑能力，为在生成新场景图像中提供了高效、稳定的解决方案。\n\n本方案在以下几个方面存在局限性：1）由于模型是在真实人脸图像数据集上训练的，当输入图像属于风格化或域外分布（例如卡通风格或艺术化处理的面孔）时，模型的身份相似性表现会受到限制；2）在生成不符合输入人物性别的新场景时，模型的编辑性会出现偏差，生成效果可能不符合预期；3）由于模型可学习的参数有限，生成图像在某些输入人脸下的一致性和多样性仍有提升空间，表现为在复杂或极端输入条件下生成的图像可能不够稳定。\n\n针对上述局限性，未来工作可以从以下几个方向进行探索，以提升模型的适应性和一致性表现：\n\n1）扩大数据规模并增加多样性。目前模型的训练数据主要包含真实人脸图像，导致其在处理风格化或非现实场景时表现有限。为了提升模型在不同风格和域外图像（如动漫、手绘、超现实主义等）上的适应性，未来可以引入更多风格化和跨域数据。扩大数据规模和多样性将拓宽模型的输入分布，使其在生成域外风格的图像时仍能保持较好的身份一致性和图像质量。这种方式不仅提升了模型的泛化能力，还增强了其在多种艺术风格和视觉表现上的灵活性，从而提高鲁棒性。  \n2）丰富提示词的范围和多样性。当前模型主要使用符合输入身份和性别的提示词进行训练。然而，在实际应用中，用户可能会希望生成具有反常识或高度创意的场景（如反转性别、极端职业设定或非典型装束等）。为此，未来可以尝试在训练中加入“反常识”提示词，使模型学习如何处理文本与图像特征相互矛\n\n盾的情况。通过丰富提示词的类型，模型能够更好地在提示词和身份特征存在差异时仍保持一致性，并生成符合文本语义要求的高质量图像。这种多样性提示词训练将显著提升模型的编辑灵活性和在复杂场景中的文本一致性。\n\n3）引入更灵活的插件结构与可学习参数。当前模型的可学习参数数量有限，限制了模型在某些复杂输入条件下的生成一致性。未来可以设计更多可学习参数的插件模块，如可扩展的自适应层，使得模型能够在高效编辑的同时具备更强的学习能力。这种插件设计可以以模块化的方式逐步增加模型参数，适应不同输入人脸特征，从而提升在复杂条件下的生成一致性和稳定性。\n\n# 第6章 总结与展望\n\n# 6.1 全文总结\n\n为了高效生成符合我国文化内涵的宣传图像，以讲好中国故事，争夺网络空间话语权，进而维护我国网络空间安全与稳定，本文围绕可控文本生成图像技术展开相应研究，重点研究了位置、属性以及身份的精细化控制问题。通过对当前技术瓶颈的深入分析，本文提出了一系列创新性的方法。以下是本论文的主要研究内容和贡献总结。\n\n# 1. 基于引导的布局结构可控的文本生成图像\n\n在布局结构可控文本生成图像方面，本文提出了一种基于引导的信息扩展策略，通过引入背景布局信息补全前景布局，实现了更为全面的布局生成。这一方法有效地解决了当前布局补全方法中背景区域语义信息缺失的问题，从而降低了图像生成的难度。本文进一步利用预训练的大规模物体生成模型提供物体生成的先验知识，并设计了细粒度文本-布局交互模块，实现了从单个物体生成到复杂多物体场景生成的迁移。实验表明，所提出的方法显著提升了布局结构可控的文本生成图像的生成质量，有效减少了背景失真和物体变形等问题的发生。\n\n# 2. 基于学习的细粒度颜色可控的文本生成图像\n\n在细粒度颜色可控的文本生成图像方向上，本文创新性地提出了一种基于学习的颜色控制方法，克服了非学习方法在细粒度属性控制方面的局限性。具体而言，本文设计了一种自适应频率导向的颜色直方图，以更准确地表示用户的颜色需求，并提出了颜色一致性匹配奖励策略，通过迭代多次生成的图像与输入图像的颜色一致性损失，显式地提升了输入图像和输出图像之间的颜色一致性。实验结果表明，该方法在肤色控制任务中取得了显著的性能提升，并且成功扩展至发色控制任务，展示了良好的通用性。\n\n# 3. 基于人脸特征精确表达的身份可控的文本生成图像\n\n在身份可控的文本生成图像方面，本文提出了一种基于人脸特征提取器的多尺度多令牌人脸身份表示方法。通过利用多尺度特征表示人脸图像中的底层细节与高层语义信息，本文实现了高保真度的人脸身份生成。此外，本文设计了自增强的编辑学习方法，通过生成成对的名人编辑性数据，增强了生成图像的编辑性，从而提升了人脸身份可控性与文本描述之间的一致性。实验表明，该方法在保持人脸身份相似性的同时，有效避免了因重构目标而忽略文本信息的问题，提升了生成图像的整体质量。\n\n综上所述，本学位论文通过对文本生成图像模型中的位置可控性、属性可控性以及身份可控性三个关键问题的深入研究，提出了多个创新性的技术路线和\n\n方法。这些方法在实验中均展现出了优秀的性能，有助于生成符合中国形象和文化内涵的宣传图像，提升我国在网络空间中的话语权和影响力，进而维护我国网络空间的安全与稳定。\n\n# 6.2 研究展望\n\n随着可控文本生成图像技术的不断发展，其潜力和应用场景也在不断拓展。未来的研究可以考虑围绕以下几个关键方向进行深入探索，以进一步提升该技术的实用性。\n\n# 1. 可控文本图像生成的统一建模\n\n未来的研究可以进一步探索如何在一个统一的模型中处理多种控制信号，从而实现更强大的可控文本图像生成能力。这种统一建模方法不仅可以处理位置、结构、属性、身份、衣服配饰、风格等多种控制信号，还能够适应更多元化的控制需求，提升模型的泛化能力和应用范围。通过将多种控制信号整合在一个模型中，也可以减少模型切换和任务适配的成本，为实际应用提供更加高效的解决方案。其中挑战性可能是在于多种控制信息的联合学习以及信号之间如果存在冲突时该如何缓解。\n\n![](images/6b9dac3a20517f47efc0c64fcf3617dd47e8c7a8c1967fe065e30a3dfea984c9.jpg)  \n图6.1 可控文本图像生成统一建模\n\n从算法实现的角度，可以首先尝试将不同的控制信息独立表示，例如：使用离散布局序列表示位置信息，自适应颜色直方图表示颜色属性信息，基于人脸身份敏感的多尺度特征表示身份信息。对每类控制信息可以设计专属的可学习参数，或利用同一超网络生成控制信号相关的参数，从而以重构图像为目标进行联合学习。为了增强模型在控制条件缺失场景中的适应性，可以在训练时引入随机丢弃控制条件的策略。在第二阶段，引入非重构学习策略去解决控制信号冲突的问题。例如，通过对同一人脸身份的图像A和图像B进行配对训练，将图像A\n\n的身份信息与图像B的属性信息结合，以生成符合图像B属性要求的结果。这种基于非重构配对数据的组合方式，使得模型在处理不同控制信息（如人脸信息和属性信息）不一致的输入时，能够优先满足属性信息的要求，同时尽可能保持人脸信息的一致性，从而实现更加灵活和准确的控制效果。\n\n# 2.探索可控文本图像生成对识别任务的作用\n\n可控文本图像生成技术不仅能够生成高质量的图像，还为识别任务中的应用提供了新的可能性。通过生成带有特定控制信号的图像，模型能够创建多样化的训练样本集，从而提升识别模型在少样本甚至无样本场景下的识别能力。\n\n以语义分割任务为例，仅在领域A的数据上训练分割模型，在迁移到领域B时性能往往会显著下降，反映出模型在新领域中的泛化能力不足。为了解决这一问题，可以借助可控文本图像生成模型来生成领域B的配对数据，即分割图与领域B的图像。具体而言，可以对模型在领域B的数据上进行定向微调（例如使用LoRA进行轻量级适应性调整）。由于像ControlNet等结构化控制模型具有较强的泛化能力，微调后的模型可以根据领域B的分割图生成符合该领域特征的图像。随后，将这些生成的数据集融入分割模型的训练流程中，从而有效提升模型在领域B上的泛化能力。\n\n# 3. 基于多模态理解-生成统一的大模型进行研究\n\n![](images/96ac6217c0677026d68bb0892964bfc8c6438c2f841982e8b7aa8eb32d27c09e.jpg)  \n图6.2 从单模态文生图模型向多模态理解-生成图像演进\n\n随着多模态理解-生成统一的深入发展，未来的可控文本图像生成技术可能不再局限于“文本输入-图像输出”的传统模式，而是将面临多模态输入和输出的挑战。这意味着研究者需要探索在多模态生成大模型框架下的技术方案，实现在文本、图像、音频等多模态间的联合控制和生成。这一跨模态的研究方向将显著拓宽可控文本生成技术的应用边界，使其在更复杂的交互场景中发挥关键作用。\n\n在基于多模态理解-生成统一的模型研究中，核心在于如何构建多模态的可\n\n控训练数据集。研究者可以利用现有的专家可控模型生成目标图像，并结合大语言模型进行相应的文本描述扩充。例如，为了实现身份控制，用户可以通过不同的语言描述表达生成要求（如“生成与参考人脸相似的效果”或“基于参考人脸进行生成”）。进一步地，可以通过结合多个可控图像生成模型，构建多维度信息配对的数据集，例如结合参考图像A中的衣物风格和参考图像B的肤色特征来生成目标图像。这样，研究者能够系统性地生成复杂的配对数据，为多模态统一生成任务提供更丰富、更精准的数据支持。\n\n# 4. 可控文本生成图像的进一步拓展\n\n可控文本生成图像技术的应用潜力还可以进一步拓展到视频和3D生成领域。这一研究方向将极大地拓宽现有技术的适用场景，使其不仅能够生成高质量的静态图像，还能够生成具有时间连续性的动态视频以及富有空间立体感的3D模型。随着技术的进步，视频生成领域将面临新的控制需求，如对角色动作的精细控制、镜头运动的路径控制、以及动作速率的动态调整；而在3D生成领域中，视角控制、深度信息的准确生成等也成为了关键挑战。通过将控制信号扩展至视频和3D生成任务，可控文本生成技术能够为虚拟现实、增强现实等领域提供更丰富的内容生成工具，助力这些新兴技术的发展。\n\n# 参考文献\n\n[1] 徐国亮. 提升中国国际话语权的深刻意蕴与重要意义[EB/OL]. 2021. https://theory.gmw.cn/2021-11/28/content_35342932.htm.  \n[2] 李婧文, 李雅文. 深度合成技术应用与风险应对[J]. 网络与信息安全学报, 2023, 9(2): 184-190.  \n[3] 乔喆. 人工智能生成内容技术在内容安全治理领域的风险和对策[J]. 电信科学, 2023, 39(10): 136-146.  \n[4] XU T, ZHANG P, HUANG Q, et al. Attngan: Fine-grained text to image generation with attentional generative adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 1316-1324.  \n[5] DEVLIN J, CHANG M W, LEE K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[A]. 2018.  \n[6] RADFORD A, KIM J W, HALLACY C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PMLR, 2021: 8748-8763.  \n[7] ZHU M, PAN P, CHEN W, et al. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 5802-5810.  \n[8] HUANG M, MAO Z, WANG P, et al. Dse-gan: Dynamic semantic evolution generative adversarial network for text-to-image generation[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 4345-4354.  \n[9] ESSER P, KULAL S, BLATTMANN A, et al. Scaling rectified flow transformers for high-resolution image synthesis[C]//Forty-first International Conference on Machine Learning. 2024.  \n[10] XU J, LIU X, WU Y, et al. Imagereward: Learning and evaluating human preferences for text-to-image generation[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[11] NICHOL A, DHARIWAL P, RAMESH A, et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models[A]. 2021.  \n[12] ESSER P, ROMBACH R, OMMER B. Taming transformers for high-resolution image synthesis[C]/Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.  \n[13] REED S, AKATA Z, YAN X, et al. Generative adversarial text to image synthesis[C]// International conference on machine learning. PMLR, 2016: 1060-1069.\n\n[14] ZHANG H, XU T, LI H, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 5907-5915.  \n[15] ZHANG H, XU T, LI H, et al. Stackgan++: Realistic image synthesis with stacked generative adversarial networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(8): 1947-1962.  \n[16] YIN G, LIU B, SHENG L, et al. Semantics disentangling for text-to-image generation[C]// Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 2327-2336.  \n[17] RUAN S, ZHANG Y, ZHANG K, et al. Dae-gan: Dynamic aspect-aware gan for text-to-image synthesis[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 13960-13969.  \n[18] LIAO W, HU K, YANG M Y, et al. Text to image generation with semantic-spatial aware gan [C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 18187-18196.  \n[19] BROCK A, DONAHUE J, SIMONYAN K. Large scale gan training for high fidelity natural image synthesis[A]. 2018.  \n[20] KANG M, ZHU J Y, ZHANG R, et al. Scaling up gans for text-to-image synthesis[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10124-10134.  \n[21] SCHUHMANN C, VENCU R, BEAUMONT R, et al. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs[A]. 2021.  \n[22] RADFORD A, WU J, CHILD R, et al. Language models are unsupervised multitask learners [J]. OpenAI blog, 2019, 1(8): 9.  \n[23] RAMESH A, PAVLOV M, GOH G, et al. Zero-shot text-to-image generation[C]// International conference on machine learning. Pmlr, 2021: 8821-8831.  \n[24] YU J, XU Y, KOH J Y, et al. Scaling autoregressive models for content-rich text-to-image generation: Vol. 2[A]. 2022: 5.  \n[25] SAHARIA C, CHAN W, SAXENA S, et al. Photorealistic text-to-image diffusion models with deep language understanding[J]. Advances in neural information processing systems, 2022, 35: 36479-36494.  \n[26] RAMESH A, DHARIWAL P, NICHOL A, et al. Hierarchical text-conditional image generation with clip latents: Vol. 1[A]. 2022: 3.  \n[27] ROMBACH R, BLATTMANN A, LORENZ D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and\n\npattern recognition. 2022: 10684-10695.  \n[28] HONG S, YANG D, CHOI J, et al. Inferring semantic layout for hierarchical text-to-image synthesis[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7986-7994.  \n[29] LI W, ZHANG P, ZHANG L, et al. Object-driven text-to-image synthesis via adversarial training[C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 12174-12182.  \n[30] QIAO Y, CHEN Q, DENG C, et al. R-gan: Exploring human-like way for reasonable text-to-image synthesis via generative adversarial networks[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 2085-2093.  \n[31] ZHAO B, MENG L, YIN W, et al. Image generation from layout[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 8584-8593.  \n[32] KINGMA D P, WELLING M. Auto-encoding variational bayes[A]. 2013.  \n[33] SUN W, WU T. Image synthesis from reconfigurable layout and style[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 10531-10540.  \n[34] SUN W, WU T. Learning layout and style reconfigurable gans for controllable image synthesis [J]. IEEE transactions on pattern analysis and machine intelligence, 2021, 44(9): 5070-5087.  \n[35] BACK J, KIM S, AHN N. Webtoonme: A data-centric approach for full-body portrait stylization[M]//SIGGRAPH Asia 2022 Technical Communications. 2022: 1-4.  \n[36] GE S, PARK T, ZHU J Y, et al. Expressive text-to-image generation with rich text[C]// Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7545-7556.  \n[37] TAI Y W, JIA J, TANG C K. Local color transfer via probabilistic segmentation by expectation-maximization[C]//2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05): Vol. 1. IEEE, 2005: 747-754.  \n[38] LEVIN A, LISCHINSKI D, WEISS Y. Colorization using optimization[M]//ACM SIGGRAPH 2004 Papers. 2004: 689-694.  \n[39] PALSSON S, AGUSTSSON E, TIMOFTE R, et al. Generative adversarial style transfer networks for face aging[C]//Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018: 2084-2092.  \n[40] GAL R, ALALUF Y, ATZMON Y, et al. An image is worth one word: Personalizing text-to-image generation using textual inversion[A]. 2022.  \n[41] DONG Z, WEI P, LIN L. Dreamartist: Towards controllable one-shot text-to-image generation via positive-negative prompt-tuning[A]. 2022.  \n[42] RUIZN, LI Y, JAMPANI V, et al. Dreambooth: Fine tuning text-to-image diffusion models for\n\nsubject-driven generation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 22500-22510.  \n[43] KUMARI N, ZHANG B, ZHANG R, et al. Multi-concept customization of text-to-image diffusion[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 1931-1941.  \n[44] WEI Y, ZHANG Y, JI Z, et al. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 15943-15953.  \n[45] MA Y, YANG H, WANG W, et al. Unified multi-modal latent diffusion for joint subject and text conditional image generation[A]. 2023.  \n[46] SHI J, XIONG W, LIN Z, et al. Instantbooth: Personalized text-to-image generation without test-time finetuning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8543-8552.  \n[47] VASWANI A. Attention is all you need[A]. 2017.  \n[48] VAN DEN OORD A, VINYALS O, et al. Neural discrete representation learning[J]. Advances in neural information processing systems, 2017, 30.  \n[49] RAZAVI A, VAN DEN OORD A, VINYALS O. Generating diverse high-fidelity images with vq-vae-2[J]. Advances in neural information processing systems, 2019, 32.  \n[50] LEE D, KIM C, KIM S, et al. Autoregressive image generation using residual quantization [C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11523-11532.  \n[51] ZHENG C, VUONG T L, CAI J, et al. Movq: Modulating quantized vectors for high-fidelity image generation[J]. Advances in Neural Information Processing Systems, 2022, 35: 23412-23425.  \n[52] YU J, LI X, KOH J Y, et al. Vector-quantized image modeling with improved vqgan[A]. 2021.  \n[53] GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial nets[J]. Advances in neural information processing systems, 2014, 27.  \n[54] RADFORD A, METZ L, CHINTALA S. Unsupervised representation learning with deep convolutional generative adversarial networks[A]. 2015.  \n[55] ZHANG H, GOODFELLOW I, METAXAS D, et al. Self-attention generative adversarial networks[C]//International conference on machine learning. PMLR, 2019: 7354-7363.  \n[56] ARJOVSKY M, CHINTALA S, BOTTOU L. Wasserstein generative adversarial networks [C]//International conference on machine learning. PMLR, 2017: 214-223.  \n[57] GULRAJANI I, AHMED F, ARJOVSKY M, et al. Improved training of wasserstein gans[J].\n\nAdvances in neural information processing systems, 2017, 30.  \n[58] MAO X, LI Q, XIE H, et al. Least squares generative adversarial networks[C]/Proceedings of the IEEE international conference on computer vision. 2017: 2794-2802.  \n[59] NOWOZIN S, CSEKE B, TOMIOKA R. f-gan: Training generative neural samplers using variational divergence minimization[J]. Advances in neural information processing systems, 2016, 29.  \n[60] LIM J H, YE J C. Geometric GAN[A]. 2017.  \n[61] MIYATO T, KATAOKA T, KOYAMA M, et al. Spectral normalization for generative adversarial networks[A]. 2018.  \n[62] KARRAS T, LAINE S, AITTALA M, et al. Analyzing and improving the image quality of stylegan[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 8110-8119.  \n[63] KARRAS T, LAINE S, AILA T. A style-based generator architecture for generative adversarial networks[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 4401-4410.  \n[64] KARRAS T, AITTALA M, LAINE S, et al. Alias-free generative adversarial networks[J]. Advances in neural information processing systems, 2021, 34: 852-863.  \n[65] VAN DEN OORD A, KALCHBRENNER N, KAVUKCUOGLU K. Pixel recurrent neural networks[C]//International conference on machine learning. PMLR, 2016: 1747-1756.  \n[66] HO J, JAIN A, ABBEEL P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.  \n[67] SOHL-DICKSTEIN J, WEISS E, MAHESWARANATHAN N, et al. Deep unsupervised learning using nonequilibrium thermodynamics[C]/International conference on machine learning. PMLR, 2015: 2256-2265.  \n[68] CHANG H, ZHANG H, JIANG L, et al. Maskgit: Masked generative image transformer [C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11315-11325.  \n[69] LIU Q. Rectified flow: A marginal preserving approach to optimal transport[A]. 2022.  \n[70] QIAOT, ZHANG J, XUD, et al. Mirrorgan: Learning text-to-image generation by redescription[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 1505-1514.  \n[71] ZHANG H, KOH J Y, BALDRIDGE J, et al. Cross-modal contrastive learning for text-to-image generation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 833-842.  \n[72] LIN TY, MAIRE M, BELONGIE S, et al. Microsoft coco: Common objects in context[C]//\n\nComputer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer, 2014: 740-755.  \n[73] DING M, YANG Z, HONG W, et al. Cogview: Mastering text-to-image generation via transformers[J]. Advances in neural information processing systems, 2021, 34: 19822-19835.  \n[74] WU C, LIANG J, JI L, et al. Niwa: Visual synthesis pre-training for neural visual world creation[C]//European conference on computer vision. Springer, 2022: 720-736.  \n[75] GAFNI O, POLYAK A, ASHUAL O, et al. Make-a-scene: Scene-based text-to-image generation with human priors[C]//European Conference on Computer Vision. Springer, 2022: 89-106.  \n[76] HO J, SALIMANS T. Classifier-free diffusion guidance[A]. 2022.  \n[77] GU S, CHEN D, BAO J, et al. Vector quantized diffusion model for text-to-image synthesis [C]/Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10696-10706.  \n[78] FENG Z, ZHANG Z, YU X, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10135-10145.  \n[79] CHEN J, YU J, GE C, et al. Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis[A]. 2023.  \n[80] CHEN J, GE C, XIE E, et al. Pixart-  $\\sigma$ : Weak-to-strong training of diffusion transformer for 4k text-to-image generation[A]. 2024.  \n[81] PODELL D, ENGLISH Z, LACEY K, et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis[A]. 2023.  \n[82] LIU X, ZHANG X, MA J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.  \n[83] CHANG H, ZHANG H, BARBER J, et al. Muse: Text-to-image generation via masked generative transformers[A]. 2023.  \n[84] RAFFEL C, SHAZEER N, ROBERTS A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. Journal of machine learning research, 2020, 21(140): 1-67.  \n[85] ZHANG Z, XIE Y, YANG L. Photographic text-to-image synthesis with a hierarchically-nested adversarial network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 6199-6208.  \n[86] CHENG J, WU F, TIAN Y, et al. Rifegan: Rich feature generation for text-to-image synthesis from prior knowledge[C]/Proceedings of the IEEE/CVF conference on computer vision and\n\npattern recognition. 2020: 10911-10920.  \n[87] BETKER J, GOH G, JING L, et al. Improving image generation with better captions[J]. Computer Science. https://cdn.openai.com/papers/dall-e-3.pdf, 2023, 2(3): 8.  \n[88] CHEN W, HU H, SAHARIA C, et al. Re-imagen: Retrieval-augmented text-to-image generator[A]. 2022.  \n[89] TOSHEV A, SZEGEDY C. Deeppose: Human pose estimation via deep neural networks [C]/Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 1653-1660.  \n[90] MARTINEZ J, HOSSAIN R, ROMERO J, et al. A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2640-2649.  \n[91] YANG L, SONG Q, WANG Z, et al. Parsing r-cnn for instance-level human analysis[C]// Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 364-373.  \n[92] ZHOU B, ZHAO H, PUIG X, et al. Scene parsing through ade20k dataset[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 633-641.  \n[93] KIRILLOV A, MINTUN E, RAVI N, et al. Segment anything[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4015-4026.  \n[94] ISOLA P, ZHU J Y, ZHOU T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.  \n[95] WANG T C, LIU M Y, ZHU J Y, et al. High-resolution image synthesis and semantic manipulation with conditional gans[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8798-8807.  \n[96] PARK T, LIU M Y, WANG T C, et al. Semantic image synthesis with spatially-adaptive normalization[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 2337-2346.  \n[97] SUSHKO V, SCHÖNFELD E, ZHANG D, et al. You only need adversarial supervision for semantic image synthesis[A]. 2020.  \n[98] ZHANG L, RAO A, AGRAWALA M. Adding conditional control to text-to-image diffusion models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 3836-3847.  \n[99] QIN C, ZHANG S, YU N, et al. Unicontrol: A unified diffusion model for controllable visual generation in the wild[A]. 2023.  \n[100] ZAVADSKI D, FEIDEN J F, ROTHER C. Controlnet-xs: Designing an efficient and effective\n\narchitecture for controlling text-to-image diffusion models[A]. 2023.  \n[101] MOU C, WANG X, XIE L, et al. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models[C]//Proceedings of the AAAI Conference on Artificial Intelligence: Vol. 38. 2024: 4296-4304.  \n[102] JIANG Z, MAO C, PAN Y, et al. Scedit: Efficient and controllable image diffusion generation via skip connection editing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8995-9004.  \n[103] HAM C, HAYS J, LU J, et al. Modulating pretrained diffusion models for multimodal image synthesis[C]//ACM SIGGRAPH 2023 Conference Proceedings. 2023: 1-11.  \n[104] HU M, ZHENG J, LIU D, et al. Cocktail: Mixing multi-modality control for text-conditional image generation[C]/Thirty-seventh Conference on Neural Information Processing Systems. 2023.  \n[105] ZHAO S, CHEN D, CHEN Y C, et al. Uni-controlnet: All-in-one control to text-to-image diffusion models[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[106] AVRAHAMI O, HAYES T, GAFNI O, et al. Spatext: Spatio-textual representation for controllable image generation[C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 18370-18380.  \n[107] LI Y, LIU H, WU Q, et al. Gligen: Open-set grounded text-to-image generation[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22511-22521.  \n[108] XUE H, HUANG Z, SUN Q, et al. Freestyle layout-to-image synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 14256-14266.  \n[109] ZHENG G, ZHOU X, LI X, et al. Layoutdiffusion: Controllable diffusion model for layout-to-image generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22490-22499.  \n[110] JIA C, LUO M, DANG Z, et al. Ssmg: Spatial-semantic map guided diffusion model for free-form layout-to-image generation[C]//Proceedings of the AAAI Conference on Artificial Intelligence: Vol. 38. 2024: 2480-2488.  \n[111] QI Z, HUANG G, HUANG Z, et al. Layered rendering diffusion model for zero-shot guided image synthesis[A]. 2023.  \n[112] HOE J T, JIANG X, CHAN C S, et al. Interactdiffusion: Interaction control in text-to-image diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6180-6189.  \n[113] CHEN K, XIE E, CHEN Z, et al. Geodiffusion: Text-prompted geometric control for object detection data generation[A]. 2023.\n\n[114] VOYNOV A, HERTZ A, ARAR M, et al. Anylens: A generative diffusion model with any rendering lens[A]. 2023.  \n[115] CHENG J, LIANG X, SHI X, et al. Layoutdiffuse: Adapting foundational diffusion models for layout-to-image generation[A]. 2023.  \n[116] GIAMBI N, LISANTI G. Conditioning diffusion models via attributes and semantic masks for face generation[A]. 2023.  \n[117] REN J, XU C, CHEN H, et al. Towards flexible, scalable, and adaptive multi-modal conditioned face synthesis[A]. 2023.  \n[118] LIU X, REN J, SIAROHIN A, et al. Hyperhuman: Hyper-realistic human generation with latent structural diffusion[A]. 2023.  \n[119] ZHANG J, LI S, LU Y, et al. Jointnet: Extending text-to-image diffusion for dense distribution modeling[A]. 2023.  \n[120] BALAJI Y, NAH S, HUANG X, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers[A]. 2022.  \n[121] KIM Y, LEE J, KIM J H, et al. Dense text-to-image generation with attention modulation[C]// Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7701-7711.  \n[122] HE Y, SALAKHUTDINOV R, KOLTER J Z. Localized text-to-image generation for free via cross attention control[A]. 2023.  \n[123] ZHAO P, LI H, JIN R, et al. Loco: Locally constrained training-free layout-to-image synthesis [A]. 2023.  \n[124] GANDIKOTA R, MATERZYNSKA J, ZHOU T, et al. Concept sliders: Lora adaptors for precise control in diffusion models[A]. 2023.  \n[125] BAUMANN S A, KRAUSE F, NEUMAYR M, et al. Continuous, subject-specific attribute control in t2i models by identifying semantic directions[A]. 2024.  \n[126] LIU Z, LUO P, WANG X, et al. Large-scale celebfaces attributes (celeba) dataset[J]. Retrieved August, 2018, 15(2018): 11.  \n[127] HUANG L, CHEN D, LIU Y, et al. Composer: Creative and controllable image synthesis with composable conditions[A]. 2023.  \n[128] VOYNOV A, CHU Q, COHEN-OR D, et al. p+: Extended textual conditioning in text-to-image generation[A]. 2023.  \n[129] ZHANG Y, DONG W, TANG F, et al. Prospect: Prompt spectrum for attribute-aware personalization of diffusion models[J]. ACM Transactions on Graphics (TOG), 2023, 42(6): 1-14.  \n[130] WANG Z, WEI W, ZHAO Y, et al. Hifi tuner: High-fidelity subject-driven fine-tuning for\n\ndiffusion models[A]. 2023.  \n[131] HOULSBY N, GIURGIU A, JASTRZEBSKI S, et al. Parameter-efficient transfer learning for nlp[C]//International conference on machine learning. PMLR, 2019: 2790-2799.  \n[132] HU E J, SHEN Y, WALLIS P, et al. Lora: Low-rank adaptation of large language models[A]. 2021.  \n[133] VALIPOUR M, REZAGHOLIZADEH M, KOBYZEV I, et al. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation[A]. 2022.  \n[134] CHAVAN A, LIU Z, GUPTA D, et al. One-for-all: Generalized lora for parameter-efficient fine-tuning[A]. 2023.  \n[135] CHEN H, ZHANG Y, WANG X, et al. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation: Vol. 3[A]. 2023.  \n[136] GU Y, WANG X, WU J Z, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[137] SMITH J S, HSU Y C, ZHANG L, et al. Continual diffusion: Continual customization of text-to-image diffusion with c-lora[A]. 2023.  \n[138] GUO Y, YANG C, RAO A, et al. Animatediff:Animate your personalized text-to-image diffusion models without specific tuning[A]. 2023.  \n[139] XIANG C, BAO F, LI C, et al. A closer look at parameter-efficient tuning in diffusion models [A]. 2023.  \n[140] YEH S Y, HSIEH Y G, GAO Z, et al. Navigating text-to-image customization: From lycoris fine-tuning to model evaluation[A]. 2023.  \n[141] AVRAHAMI O, ABERMAN K, FRIED O, et al. Break-a-scene: Extracting multiple concepts from a single image[C]//SIGGRAPH Asia 2023 Conference Papers. 2023: 1-12.  \n[142] CAI Y, WEI Y, JI Z, et al. Decoupled textual embeddings for customized image generation [C]//Proceedings of the AAAI Conference on Artificial Intelligence: Vol. 38. 2024: 909-917.  \n[143] LI Y, LIU H, WEN Y, et al. Generate anything anywhere in any scene[A]. 2023.  \n[144] MOTAMED S, PAUDEL D P, VAN GOOL L. Lego: Learning to disentangle and invert concepts beyond object appearance in text-to-image diffusion models[A]. 2023.  \n[145] JIN C, TANNO R, SASEENDRAN A, et al. An image is worth multiple words: Learning object level concepts using multi-concept prompt learning[A]. 2023.  \n[146] SAFAEE M, MIKAEILI A, PATASHNIK O, et al. Clic: Concept learning in context[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6924-6933.  \n[147] HUA M, LIU J, DING F, et al. Dreamtuner: Single image is enough for subject-driven\n\ngeneration[A]. 2023.  \n[148] TEWEL Y, GAL R, CHECHIK G, et al. Key-locked rank one editing for text-to-image personalization[C]//ACM SIGGRAPH 2023 Conference Proceedings. 2023: 1-11.  \n[149] QIU Z, LIU W, FENG H, et al. Controlling text-to-image diffusion by orthogonal finetuning [J]. Advances in Neural Information Processing Systems, 2023, 36: 79320-79362.  \n[150] HAN L, LI Y, ZHANG H, et al. Svdiff: Compact parameter space for diffusion fine-tuning [C]/Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7323-7334.  \n[151] VORONOV A, KHOROSHIKH M, BABENKO A, et al. Is this loss informative? faster text-to-image customization by tracking objective dynamics[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[152] FEI Z, FAN M, HUANG J. Gradient-free textual inversion[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 1364-1373.  \n[153] CHEN W, HU H, LI Y, et al. Subject-driven text-to-image generation via apprenticeship learning[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[154] LI J, LI D, SAVARESE S, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models[C]//International conference on machine learning. PMLR, 2023: 19730-19742.  \n[155] ARAR M, GAL R, ATZMON Y, et al. Domain-agnostic tuning-encoder for fast personalization of text-to-image models[C]//SIGGRAPH Asia 2023 Conference Papers. 2023: 1-10.  \n[156] ABBEEL P, NG A Y. Apprenticeship learning via inverse reinforcement learning[C]// Proceedings of the twenty-first international conference on Machine learning. 2004: 1.  \n[157] ZHOU Y, ZHANG R, GU J, et al. Customization assistant for text-to-image generation [C]/Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 9182-9191.  \n[158] ZHOU Y, ZHOU D, CHENG M M, et al. Storydiffusion: Consistent self-attention for long-range image and video generation[A]. 2024.  \n[159] CAOM, WANG X, QIZ, et al. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing[C]/Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 22560-22570.  \n[160] VALEVSKI D, LUMEN D, MATIAS Y, et al. Face0: Instantaneously conditioning a text-to-image model on a face[C]//SIGGRAPH Asia 2023 Conference Papers. 2023: 1-10.  \n[161] XIAO G, YIN T, FREEMAN W T, et al. Fastcomposer: Tuning-free multi-subject image generation with localized attention[A]. 2023.  \n[162] CHEN L, ZHAO M, LIU Y, et al. Photovverse: Tuning-free image customization with text-to-\n\nimage diffusion models[A]. 2023.  \n[163] LI X, HOU X, LOY C C. When stylegan meets stable diffusion: a w+ adapter for personalized image generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 2187-2196.  \n[164] LI Z, CAO M, WANG X, et al. Photomaker: Customizing realistic human photos via stacked id embedding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8640-8650.  \n[165] ACHLIOPTAS P, BENETATOS A, FOSTIROPOULOS I, et al. Stellar: Systematic evaluation of human-centric personalized text-to-image methods[A]. 2023.  \n[166] PENG X, ZHU J, JIANG B, et al. Portraitbooth: A versatile portrait model for fast identity-preserved personalization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 27080-27090.  \n[167] HYUNG J, SHIN J, CHOO J. Magicapture: High-resolution multi-concept portrait customization[C]/Proceedings of the AAAI Conference on Artificial Intelligence: Vol. 38. 2024: 2445-2453.  \n[168] SALIMANS T, GOODFELLOW I, ZAREMBA W, et al. Improved techniques for training gans[J]. Advances in neural information processing systems, 2016, 29.  \n[169] SZEGEDY C, VANHOUCKE V, IOFFE S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.  \n[170] HEUSEL M, RAMSAUER H, UNTERTHINER T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[J]. Advances in neural information processing systems, 2017, 30.  \n[171] BINKOWSKI M, SUTHERLAND D J, ARBEL M, et al. Demystifying mmd gans[A]. 2018.  \n[172] HINZ T, HEINRICH S, WERMTER S. Semantic object accuracy for generative text-to-image synthesis[J]. IEEE transactions on pattern analysis and machine intelligence, 2020, 44 (3): 1552-1565.  \n[173] WAH C, BRANSON S, WELINDER P, et al. The caltech-ucsd birds-200-2011 dataset[M]. California Institute of Technology, 2011.  \n[174] HESSEL J, HOLTZMAN A, FORBES M, et al. Clipscore: A reference-free evaluation metric for image captioning[A]. 2021.  \n[175] VINYALS O, TOSHEV A, BENGIO S, et al. Show and tell: A neural image caption generator [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3156-3164.  \n[176] PAPINENI K, ROUKOS S, WARD T, et al. Bleu: a method for automatic evaluation of\n\nmachine translation[C]//Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002: 311-318.  \n[177] BANERJEE S, LAVIE A. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments[C]//Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 2005: 65-72.  \n[178] VEDANTAM R, LAWRENCE ZITNICK C, PARIKH D. Cider: Consensus-based image description evaluation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 4566-4575.  \n[179] HUANG K, SUN K, XIE E, et al. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation[J]. Advances in Neural Information Processing Systems, 2023, 36: 78723-78747.  \n[180] WU X, SUN K, ZHU F, et al. Better aligning text-to-image models with human preference: Vol. 1[A]. 2023.  \n[181] KIRSTAIN Y, POLYAK A, SINGER U, et al. Pick-a-pic: An open dataset of user preferences for text-to-image generation[J]. Advances in Neural Information Processing Systems, 2023, 36: 36652-36663.  \n[182] ZHANG S, WANG B, WU J, et al. Learning multi-dimensional human preference for text-to-image generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8018-8027.  \n[183] LIANG Y, HE J, LI G, et al. Rich human feedback for text-to-image generation[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 19401-19411.  \n[184] LEE T, YASUNAGA M, MENG C, et al. Holistic evaluation of text-to-image models[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[185] NILSBACK M E, ZISSERMAN A. Automated flower classification over a large number of classes[C]//2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE, 2008: 722-729.  \n[186] ZHOU Y. Generative adversarial network for text-to-face synthesis and manipulation[C]// Proceedings of the 29th ACM International Conference on Multimedia. 2021: 2940-2944.  \n[187] XIA W, YANG Y, XUE J H, et al. Tedigan: Text-guided diverse face image generation and manipulation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 2256-2265.  \n[188] JIANG Y, YANG S, QIU H, et al. Text2human: Text-driven controllable human image generation[J]. ACM Transactions on Graphics (TOG), 2022, 41(4): 1-11.  \n[189] WANG Z J, MONTOYA E, MUNECHIKA D, et al. Diffusiondb: A large-scale prompt\n\ngallery dataset for text-to-image generative models[A]. 2022.  \n[190] SCHUHMANN C, BEAUMONT R, VENCUR, et al. Laion-5b: An open large-scale dataset for training next generation image-text models[J]. Advances in Neural Information Processing Systems, 2022, 35: 25278-25294.  \n[191] MINWOO BYEON H K S L W B, Beomhee Park, KIM S. Coyo-700m: Image-text pair dataset[EB/OL]. 2022. https://github.com/kakaobrain/coyo-dataset.  \n[192] CHEN K, CHOY C B, SAVVA M, et al. Text2shape: Generating shapes from natural language by learning joint embeddings[C]//ACCV. 2018: 100-116.  \n[193] JIANG Y, HUANG Z, PAN X, et al. Talk-to-edit: Fine-grained facial editing via dialog[C]// ICCV. 2021: 13799-13808.  \n[194] HU K, LIAO W, YANG M Y, et al. Text to image generation with semantic-spatial aware gan [A]. 2021.  \n[195] CAESAR H, UIJLINGS J, FERRARI V. Coco-stuff: Thing and stuff classes in context[C]// CVPR. 2018: 1209-1218.  \n[196] DENG J, DONG W, SOCHER R, et al. Imagenet: A large-scale hierarchical image database [C]//CVPR. 2009: 248-255.  \n[197] SUN W, WUT. Learning layout and style reconfigurable gans for controllable image synthesis [A]. 2020.  \n[198] GUPTA K, LAZAROW J, ACHILLE A, et al. Layout transformer: Layout generation and completion with self-attention[C]//ICCV. 2021: 1004-1014.  \n[199] DUMOULIN V, SHLENS J, KUDLUR M. A learned representation for artistic style[A]. 2016.  \n[200] KARRAS T, AILA T, LAINE S, et al. Progressive growing of gans for improved quality, stability, and variation[A]. 2017.  \n[201] TAO M, TANG H, WU S, et al. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis[A]. 2020.  \n[202] KLEIN G, KIM Y, DENG Y, et al. Opennmt: Open-source toolkit for neural machine translation[C]/Proceedings of ACL 2017, System Demonstrations. 2017: 67-72.  \n[203] HOLTZMAN A, BUYS J, DUL, et al. The curious case of neural text degeneration[A]. 2019.  \n[204] LIU L, FU Q, HOU F, et al. Flexible portrait image editing with fine-grained control[A]. 2022.  \n[205] AFIFI M, BRUBAKER M A, BROWN M S. Histogram: Controlling colors of gan-generated and real images via color histograms[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 7941-7950.  \n[206] GUO Z, WU Y, CHEN Z, et al. Pulid: Pure and lightning id customization via contrastive\n\nalignment[A]. 2024.  \n[207] CHEN Z, FANG S, LIU W, et al. Dreamidentity: Enhanced editability for efficient face-identity preserved image generation[C]//Proceedings of the AAAI Conference on Artificial Intelligence: Vol. 38. 2024: 1281-1289.  \n[208] WANG Q, BAI X, WANG H, et al. Instantid: Zero-shot identity-preserving generation in seconds[A]. 2024.  \n[209] YE H, ZHANG J, LIU S, et al. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models[A]. 2023.  \n[210] HUANG M, MAO Z, LIU M, et al. Realcustom: Narrowing real text word for real-time open-domain text-to-image customization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7476-7485.  \n[211] WANG H, WANG Q, BAI X, et al. Instantstyle: Free lunch towards style-preserving in text-to-image generation[A]. 2024.  \n[212] QI T, FANG S, WU Y, et al. Deadiff: An efficient stylization diffusion model with disentangled representations[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8693-8702.  \n[213] CLARK K, VICOL P, SWERSKY K, et al. Directly fine-tuning diffusion models on differentiable rewards[A]. 2023.  \n[214] ALAYRAC J B, DONAHUE J, LUC P, et al. Flamingo: a visual language model for few-shot learning[J]. Advances in neural information processing systems, 2022, 35: 237:16-23736.  \n[215] SONG J, MENG C, ERMON S. Denoising diffusion implicit models[A]. 2020.  \n[216] BHAT A, JAIN S. Face recognition in the age of clip & billion image datasets[A]. 2023.  \n[217] DOSOVITSKIY A, BEYER L, KOLESNIKOV A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[A]. 2020.  \n[218] GAL R, ARAR M, ATZMON Y, et al. Designing an encoder for fast personalization of text-to-image models: Vol. 2[A]. 2023.  \n[219] LIU Z, LUO P, WANG X, et al. Deep learning face attributes in the wild[C]//Proceedings of International Conference on Computer Vision (ICCV). 2015.  \n[220] DENG J, GUO J, XUE N, et al. Arcface: Additive angular margin loss for deep face recognition. in 2019 IEEE[C]//CVPR. 2018: 4685-4694.  \n[221] BROOKS T, HOLYNSKI A, EFROS A A. Instructpix2pix: Learning to follow image editing instructions[A]. 2022.",
    "chunked": true,
    "vectorizeStatus": "success",
    "chunkCount": 187
  },
  {
    "id": "9592a001-f9e1-4749-8315-a21e61bdb73c",
    "title": "基于注意力修正机制的组合式文生图算法研究_罗羚玮",
    "fileName": "基于注意力修正机制的组合式文生图算法研究_罗羚玮.pdf",
    "fileType": "pdf",
    "fileSize": 8500375,
    "uploadTime": "2025-12-29T21:42:07.204136",
    "parsed": true,
    "parseStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251229214207_基于注意力修正机制的组合式文生图算法研究_罗羚玮.pdf",
    "markdownContent": "# 杭州量子科技大学\n\n# 硕士学位论文\n\n# (专业学位)\n\n![](images/54c1ef18b9bdc038df5ef3162b765f42993de3f44ea3fc0e3c7bd5e75922a43f.jpg)\n\n# 论文题目：基于注意力修正机制的组合式文生图算法研究\n\n作者姓名： 罗羚玮\n\n指导教师： 顾晓玲 副教授\n\n专业学位类别： 工学\n\n专业领域： 计算机技术\n\n所在学院： 计算机学院\n\n完成时间： 2025年5月20日\n\n# 杭州电子科技大学硕士学位论文\n\n# 基于注意力修正机制的组合式文生图算法研究\n\n研究生：罗羚玮\n\n指导教师：顾晓玲 副教授\n\n2025年5月\n\n# Dissertation Submitted to Hangzhou Dianzi University\n\nfor the Degree of Master\n\n# Research on Compositional Text-to-Image Generation Algorithm Based on Attention Correction Mechanism\n\nCandidate: Luo Lingwei\n\nSupervisor: Prof. Gu Xiaoling\n\nMay, 2025\n\n# 摘要\n\n随着人工智能技术的不断进步，基于扩散模型的图像生成技术已成为研究热点。然而，扩散模型在组合式生成任务中仍面临诸多挑战，出现两类问题：属性绑定问题及空间交互问题。前者包括实体缺失、实体泄露、属性互换、属性泄露四类子问题，后者包括空间位置错误、交互错误问题。这些问题限制了扩散模型在其下游任务中应用的有效性。因此，本研究聚焦于基于扩散模型注意力修正机制的组合式文生图算法，展开了以下两部分的研究：\n\n针对属性绑定问题，本文提出了一种基于交叉注意力损失优化的组合式生成算法。该方法的提出基于交叉注意力机制是扩散模型提示文本特征与图像特征融合对齐的关键步骤。该算法首先借助代码提示链（Chain-of-Code，COC），通过大语言模型生成提示文本中对应标记文本的布局。随后，提取出当前去噪步骤的交叉注意力图。依据布局，提出两种新颖的损失：基于图像块导向的交叉注意力损失（Patch-Oriented Cross Attention Loss, PCA）和基于区域导向的交叉注意力损失（Region-Oriented Cross Attention Loss, RCA），根据阈值判断结果来对潜在空间向量进行优化。同时，在指定的去噪时间步上，进行迭代优化得到更好的潜在空间向量。最终实现，利用现实的布局条件，引导交叉注意力汇聚在指定布局区域中，从而提升扩散模型生成图像与提示文本之间的语义一致性。\n\n针对空间交互问题，本研究提出了基于编辑修正的多条件组合式文生图算法。该算法首先分析了多种文生图模型在组合式生成任务中的空间位置约束能力差异，并引入多模态可控文生图模型作为基模型。在此基础上，为进一步加强空间结构约束，提出了两种新颖的基于注意力机制的结构优化损失：基于自注意力的结构强化损失（Structural Reinforcement Self Attention Loss，SRSA）和基于交叉注意力的结构相似度损失（Structural Similarity Cross Attention Loss，SSCA）。最后，为防止去噪过程中的交叉注意力分布错误累加导致的生成图像错误，引入了检测修正机制。利用基于CLIP图文相似度检测目标区域生成图像后，借助图像编辑方法对生成图像进行修正，从而进一步保证图像生成效果。\n\n综上所述，本研究围绕无训练模式下提高扩散模型组合式生成任务能力设计了两个使用新颖的算法框架，并进行了大量的实验分析，验证本文提出方法的有效性和实用性，还证明了本文方法具有强大的鲁棒性和可泛化性。\n\n关键词：组合式生成，扩散模型，注意力机制，无训练，图像编辑\n\n# Abstract\n\nWith the continuous advancement of artificial intelligence technology, image generation technology based on diffusion models has become a research hotspot. However, diffusion models still face many challenges in combinatorial generation tasks, and two types of problems arise: attribute binding problem and spatial interaction problem. The former includes four sub-problems: entity missing, entity leakage, attribute interchange, and attribute leakage, while the latter includes spatial position error and interaction error. These problems limit the effectiveness of diffusion models in their downstream tasks. Therefore, this study focuses on the training-free compositional text-to-image synthesis algorithm based on the attention mechanism of the diffusion model, aiming to improve the performance of the diffusion model in compositional text-to-image generation tasks by manipulating the attention mechanism, and carries out the following two parts of research.\n\nFor the attribute binding problem, this paper proposes a compositional text-to-image generation algorithm based on cross-attention optimization loss. The proposed method is based on the cross-attention mechanism is the key module in the fusion alignment of the prompt text embedding and image features of the diffusion model. The algorithm first generates the layout of the token is need to pay attention to in the prompt through the large language model (LLM) with the help of the Chain-of-Code (CoC). Subsequently, in the denoising step, the cross-attention map of the current denoising step is proposed. According to the layout, two novel losses are proposed: the patch-oriented cross attention loss (PCA) and the region-oriented cross attention loss (RCA). The latent space vector is optimized according to the threshold judgment result, so that the denoising direction of each step is more in line with the text semantics. At the same time, at the specified denoising time step, iterative optimization is performed to obtain a better latent space vector. After that, the layout conditions are used to guide the cross attention to converge in the specified layout area, thereby improving the semantic consistency between the image generated by the diffusion model and the prompt text.\n\nFor spatial interaction issues, this paper proposed a multi-conditional compositional text-to-image generation algorithm based on editing correction. The algorithm first analyzes the differences in the spatial position constraint capabilities of\n\ntext-to-image models in the compositional text-to-image generation task, and introduces a multi-condition controllable text-to-image model as the base model. On this basis, in order to further strengthen the spatial structure constraints, two novel structural optimization losses based on attention mechanisms are proposed: the structural reinforcement self attention loss (SRSA) and the structural similarity cross attention loss (SSCA). Finally, in order to prevent the generated image errors caused by the accumulation of cross-attention distribution errors during the denoising process, a detection correction mechanism is introduced. After the CLIP image-text similarity is used to detect the target area and generate the image, the generated image is corrected with the help of image editing methods to further ensure the image generation effect.\n\nIn summary, this study designed two novel algorithm frameworks around improving the combinatorial generation task capability of the diffusion model in the untrained mode, and conducted a large number of experimental analyses to verify the effectiveness and practicality of the proposed method, and also proved that the proposed method has strong robustness and generalizability.\n\nKeywords: Compositional Text-to-Image Generation, Diffusion Model, Attention Mechanism, Training-Free, Image Editing\n\n# 目录\n\n# 第1章绪论 1\n\n1.1 研究背景和意义  \n1.2 国内外研究现状 3\n\n1.2.1 基于微调训练的组合式文生图算法 3  \n1.2.2 基于无训练的组合式文生图算法 5\n\n1.3 本文研究内容 ..... 8  \n1.4 本文组织结构 ..... 10\n\n# 第2章 相关理论基础 12\n\n2.1 扩散模型 ..... 12\n\n2.1.1 DDPM模型原理 12  \n2.1.2 基于扩散模型的文生图模型 13\n\n2.2 组合式文生图相关技术 17\n\n2.2.1 注意力机制 ..... 17  \n2.2.2 多实例文生图算法 ..... 18\n\n2.3 布局生成算法 ..... 22\n\n2.3.1 布局预测器生成布局 ..... 22  \n2.3.2 大语言模型生成布局 ..... 23\n\n2.4 评估数据集及评估指标 ..... 25\n\n2.4.1 数据集 ..... 25  \n2.4.2 评估指标 ..... 25\n\n2.5 本章小结 ..... 27\n\n# 第3章 基于交叉注意力损失优化的组合式文生图算法 ..... 28\n\n3.1 引言 28  \n3.2 方案设计 31\n\n3.2.1 基于大语言模型的布局生成 32  \n3.2.2 基于交叉注意力的图文一致性损失 34\n\n3.3 实验结果分析 37\n\n3.3.1 实验设置 37  \n3.3.2 定量评估 38  \n3.3.3 定性评估 ..... 40  \n3.3.4 消融实验 43  \n3.3.5 算法局限性 46\n\n3.4 本章小结 47\n\n# 第4章 基于编辑修正的多条件组合式文生图算法 48\n\n4.1 引言 48  \n4.2 多模态可控文生图模型分析 50  \n4.3 方案设计 ..... 53\n\n4.3.1 基于注意力的结构优化损失 ..... 54  \n4.3.2 基于CLIP相似度检测的图像编辑修正模块 ..... 56\n\n4.4 实验结果分析 59\n\n4.4.1 实验设置 59  \n4.4.2 定量评估 ..... 59  \n4.4.3 定性评估 61  \n4.4.4 消融实验 64  \n4.4.5 算法局限性 67\n\n4.5 本章小结 69\n\n第5章 总结与展望 70\n\n5.1 工作总结 70  \n5.2 未来展望 ..... 71\n\n参考文献. 73\n\n# 第1章 绪论\n\n# 1.1 研究背景和意义\n\n生成式人工智能（Artificial Intelligence Generated Content，AIGC）是指利用人工智能生成高质量的内容，包括文本、图像、视频等内容。可控图像生成技术是AIGC领域的关键组成部分，以其精准操控模型生成前所未有的图像的能力，备受关注。文生图（Text-to-Image Generation）是当前可控图像生成领域核心的研究方向之一，能够将文本描述的细腻情感与丰富想象转化为栩栩如生的视觉图像。尤其是扩散模型出现后，这项技术因其生成高质量和多样性结果方面的卓越表现引起了广泛的关注，并催生了一系列基于扩散模型的文生图大模型，如Imagen[1]、DALL·E3[2]、Stable Diffusion（SD）[3]，为多个行业带来了广泛的应用前景，如Midjourney<sup>1</sup>、Adobe Firefly<sup>2</sup>、文心一格<sup>3</sup>、万兴爱画<sup>4</sup>等一系列商业产品，在个人艺术创作、社交媒体图片内容等多方面，利用其创造个性化的作品内容。同时，文生图模型在图生视频领域中也具有强大的作用，如可灵AI<sup>5</sup>使用文生图模型获得高质量符合条件图片后用于视频生成，能够有效降低视频内容制作的门槛，提升内容生产的效率，为影视文娱行业发展助力。\n\n显然，从当前的科技发展态势来看，根据文本生成高质量相符的图像已经成为计算机视觉领域一项至关重要的任务，对AIGC技术的产业化落地有着不可忽视的深刻影响。尽管如此，当前开源扩散模型，如在经过数十亿个文本图像对的广泛训练后，展现出了显著的性能改进的当前经典的、先进的开源扩散模型SD，仍存在着一些不可忽视的生成问题，特别是在组合式生成任务中，往往难以保证生成图片与提示文本之间的高度一致性。这一问题如果得不到有效解决，将会对一系列依赖于SD模型原有生成能力的下游任务造成严重的负面影响。例如，在图像编辑领域，编辑工作因生成的图片与用户期望存在较大偏差而变得异常困难。\n\n组合式生成任务是指提示文本中存在多个物体及其对应的属性，即提示文本中包含多个属性-实体对，这类任务对模型的理解能力和创造力提出了极高的要求。虽然SD在单实例图像生成方面取得了显著成效，但在处理包含复杂布局和丰富属性的多实例文本提示时，特别是现实世界中不常出现的组合，其生成的图\n\n像与文本提示的语义之间存在显著的不对齐现象，并且出现实体缺失、属性互换、属性泄露、空间布局错误等问题。如图 1.1 所示，具体问题包括：实体缺失，即在生成的图像中忽略了某些对象；实体泄露，即在生成的图像中某些对象被另一些对象所替代，或含有另一些对象的明显特征；属性互换，即两个或两个以上对象的属性相互交换；属性泄露，即一个对象的属性错误地绑定到另一个对象；空间布局错误，即在生成的图像的空间关系或布局与提示文本中所给定的不符；交互动作错误，即生成图像与文本提示中所给定的交互信息不符。\n\n![](images/f25f38e6ce9544befd34eee532245f8d2de0fb8240e5836bfe41466d581ac53e.jpg)  \nA brown horse and a red orange\n\n![](images/fa75886f75db8a62208bb686631e4f3ac1cc295c63c1821153b225208e2d092a.jpg)  \nA yellow frog and a green fly\n\n![](images/4246c9a8b5be59455a89bfbdfd372b62a5c11cc9b8e9825f6f1f776e978626e7.jpg)  \nA blue bear and a brown boat\n\n![](images/ad6d6275c6aa6c84f0510905a284b39d33d77e61e3d5d98e31de64b1e956d1ff.jpg)  \n(a) 实体缺失  \nA green tree and brown grass  \n(d) 属性泄露  \n图1.1扩散模型生成错误分类示例\n\n![](images/509deaa88d8f7aed7d7180b6fc3f5e0898635c839e61aa939f48291d438237b9.jpg)  \n(b) 实体泄露  \nA desk on the top of a cat  \n(e)空间信息错误\n\n![](images/37d78c5b61824cc9f53383e63b2600ee69c7f9d4f2d6729b76408fe1e6bcc7a1.jpg)  \n(c)属性交换  \nA dog is licking its owner's face and showing affection  \n(f) 交互错误\n\n因此，为了进一步提升生成模型的能力开展深入研究，提高扩散模型的多实例生成任务中的可控能力，增强生成图片与条件文本的图文一致性，缓解实体缺失、实体泄露、属性互换、属性泄露、空间布局错误等情况的出现。本研究对扩散模型在生成图片与给定提示文本之间一致性效果的增强进行一项深入而细致的探究。首先，聚焦于系统性地分析导致图文匹配问题的根源，包括模型对文本语义理解的局限性、模型各模块自身的局限性等，通过细致剖析这些关键因素，能够更准确地定位问题的症结所在，为后续提出针对性的改进措施奠定坚实基础。其次，在算法层面进行优化与创新。扩散模型的核心在于其逐步去噪的过程。深入探索去噪过程中的动态变化，特别是如何在该过程中更有效地融入文本提示信息，即注意力机制利用，通过对比注意力图与文本语义的关联，据此调整模型去噪方向，以确保生成的图像能够精准反映提示文本意图。\n\n# 1.2 国内外研究现状\n\n当前扩散模型存在着一些问题，其原因主要可以归因于两个方面：一方面是扩散模型在生成过程中，随着时间的推移，遗漏了某些提示文本所传达的信息；另一方面是扩散模型无法将正确的文本的信息映射到图像中某些正确的位置上。而这些错误最初来源于文本生成图像扩散模型的训练范式。文本条件仅作为去噪损失的附加信息，在没有明确学习文本中每个概念的明确指导下，扩散模型很容易无法正确理解提示中的概念。\n\n在当前提升组合式生成能力领域，国内外学者在不同维度上都进行了深入而又富有创新性的研究。其中，对原始扩散模型架构进行升级是一个重要的探索方向。通过引入更先进的网络结构、更丰富有效的训练数据、更高效的训练策略等，使生成模型在保持图像质量的同时，更准确地捕捉并反映提示文本中的关键信息，从而实现了生成图像与文本之间更加紧密的联系。在此同此，引入多模态大语言模型等一系列工具在训练过程中对生成图像进行反馈、筛选等方式成为当下的一种新的流行趋势。此外，利用额外控制条件（如布局条件）对生成过程进行约束，也是提升组合式生成能力的重要手段之一。通过引入这些控制条件，模型能够更加精确地控制生成图像的位置、大小等。同时，去噪模块中的注意力机制也因为其独有的优势而受到广泛注意，利用注意力机制使得模型更加关注于提示文本中的关键信息，从而有效减少噪声干扰，提升生成图像的质量和准确性。这一机制的应用不仅提升了生成模型的性能，更为理解视觉系统的工作原理提供了新的视角和启示。\n\n综上所述，本章深入剖析这些算法的原理、方法以及实际应用，更好地理解当前提升组合式生成任务能力的研究进展和未来去洗，为进一步提升组合式生成能力提供有益的参考和启示。\n\n# 1.2.1 基于微调训练的组合式文生图算法\n\n在提高扩散模型生成能力的方面，最为传统且根本的方法无疑是对模型架构进行持续的优化与升级，同时采用更高质量的训练数据集，以确保模型能够获得更精准、高效地拟合数据分布能力。这一策略的核心在于，通过构建更加复杂且强大的模型结构，以及提供更加丰富和真实的训练样本，来增强模型捕捉和再现图像细节的能力。如PixArt - α等[4]-[5]使用Diffusion Transformer（DiT）[6]及其变体结构[7]取代了基于卷积的U-Net结构[8]，这一操作极大地提升了模型的灵活性与扩展性。同时，采用更高质量、更多数量的训练数据并设计三阶段的训练策略，从而实现低训练成本下高质量的图像生成。在此基础上，后续出现的PixArt - Σ[9]、\n\nPixArt -  $\\Delta^{[10]}$  进一步推动了该方向的进展。不仅在先前的基础上对数据集进行扩展，提高数据集的美学质量，还设计了从弱到强的训练策略。具体而言，利用SDXL[11]的VAE模块，在此基础上对模型进行微调，此外，采用“位置编码插值”技巧和没有KV压缩的低分辨率预训练模型上应用KV压缩，获得高质量的模型初始状态，加速微调过程。\n\n在训练过程中改进扩散模型组合式生成能力的方法主要分为两种：一种是加入额外的控制模块，用来指定图像中的高级特征[12]-[22]。如 ControlNet[12]利用初始置为0的同样规模的U-Net进行训练，并将其作为额外输入条件（如边缘图、关键点、分割图等）的特征提取器，将提取出来的特征与原始U-Net模块进行融合；LayoutLLM-T2I[21]利用大语言模型依据提示文本获取布局分布，在GLIGEN[16]模型的基础上，在门控注意力层与交叉注意力层中间添加额外的可训练关系意图注意力层，实现实体之间生成正确的交互信息；BlobGEN[22]同样的思想，在GLIGEN的基础上，添加额外的可训练的注意力层，实现进一步的图文匹配度提升，但该文中使用特殊表示Blob，能够表达丰富的布局信息和具体描述；T2I-Adapter[13]利用一个额外可训练的由4个特征提取块和3个下采样块组成网络，实现额外控制条件的特征提取。与ControlNet不同的是，T2I-Adapter可以同时接受多种不同的条件输入，只需要将不同训练后的Adapter输出进行连接即可完成操作。同时，Composer[14]也支持多重引导条件进行图像生成，在该文中提出引导不仅仅是依赖于条件控制输入，更应该依赖于条件的组合性质。将输入条件按照性质划分为全局条件和局部条件，构建出联合训练策略。具体来说，在训练不同条件时，通过Dropout[23]的值范围来控制对网络优化的影响。\n\n另一种训练方式通过引入奖励机制[24]-[34]，来激励生成更加贴合提示文本的图像。在强化学习的框架下，DDPO_SF[24]率先采用了一套相对完备的强化学习方法，为这一领域树立了标杆。随后，DPOK[29]则是在此基础上进行了有益的拓展，巧妙地结合了策略优化和KL正则化，这一创新举措有效地缓解了因奖励机制不当而可能引发的过拟合问题，进一步提升了模型的稳定性和性能。此外，还有一系列研究采用另外的新颖方式，即尝试利用基于用户反馈的强化学习机制。这些研究从人类反馈中汲取经验，通过最大化奖励来不断优化生成过程，了解符合人类视觉效果的图像分布，从而实现了图像生成准确率的显著提升[33]-[34]。另一种实现方式也值得关注，即去噪过程中直接反向传播奖励函数的梯度[30]-[32]，这种梯度传播方式，为奖励机制在图像生成训练中的有效应用开辟了新的途径，使得模型能够更加精准地捕捉到奖励信号，进而生成更加符合期望的图像作品。\n\n同时，当前新的一种增强组合式生成能力的训练趋势是引入检测分割模型（如GroundingDINO[35]、Segment Anything[36]）、多模态大语言模型（如BLIP[37]-\n\n[38]、 $\\mathsf{LLaMa}^{[39]}$ 、 $\\mathsf{GPT}^{[40]}$ 等）大模型工具来对训练过程进行干预，在训练过程中对生成图像进行反馈，从而进一步增强扩散模型的生成能力，使训练过程更加高效、生成图像更加真实可信。CoMat[41]引入三个模块来提高扩散模型对某些缺失物体生成概率，并借助LoRa微调技术[42]来加速训练过程。首先引入图像字幕系统，准确识别出生成图像中未能生成的提示文本的部分概念。在该模型的监督下，让扩散模型重新访问文本标记以搜索被忽略的条件信息，并为先前被忽略的文本概念加强重要性，以获得更好的文本-图像对齐。其次，利用注意力激活损失进一步提高部分文本标记低激活值。最后，设计一种新的对抗损失，通过使用鉴别器来区分与训练和微调之后扩散模型生成的图像，防止微调过程由图像字幕模型和属性与实体之间关系的先验知识主导导致过拟合现象。DreamSync[43]则是利用视觉语言模型可以识别生成的图像和用户的输入文本之间的细粒度差异的能力对模型进行反馈，从而改进文生图扩散模型。其过程主要可以分为四步：示例、评估、过滤和微调。给定文本使扩散模型生成一批对应图像，使用PaLM  $2^{[44]}$ 根据图文生成问题，后利用BLIPVQA问答模型[38]及美学模型VILA[45]对其进行评估，过滤出几张高质量的图像作为训练集，并使用LoRa技术进行微调。\n\n基于微调训练的组合式文生图算法展现出了显著优势，在提升生成图像与提示文本匹配度方面表现卓越。该算法不仅图像生成速度快捷高效，而且在处理复杂提示任务、空间交互任务表现提升尤为突出，所生成图像的质量也得到了有效增强。然而，这类方法也面临着一系列挑战。其中，最为突出的问题在于微调模型所需的高质量训练数据获取难度极大，这直接导致了微调过程在时间、空间以及金钱成本上的高昂投入。此外，该算法在适应多样化提示文本任务方面仍存在一定局限性，难以做到对各类任务均能完美适配。\n\n# 1.2.2 基于无训练的组合式文生图算法\n\n扩散模型中文本编码器本身固有的一些局限性，致使其在解析复杂提示时面临挑战，特别是CLIP文本编码器难以洞悉提示文本深层次的结构特征[46]。此外，文本编码器内采用的因果注意掩码，会导致的序列后续标记的语义信息与前置标志的语义相互混杂，进而造成信息融合问题[47]。这些不足在一定程度上影响了U-Net去噪模块中实现文本与生成图像的匹配程度。Structured Diffusion[48]使用从文本解析树中获取的多个属性-对象对的文本向量序列来改进组合性，其中每个文本向量序列都强调，结构化语言表示中来自多个层次的一个实体或实体的联合。OPT2I[49]根据提示文本生成一张图片后，将生成图片于提示文本进行一致性度量后，例如CLIP Score[50]或者是戴维森场景图分数（DSG）[51]，借助LLM[40]迭代地改进了用户提供的文本提示，通过对提示文本的优化，弥合训练文本与提示文\n\n本之间的误差。在每次优化迭代中，上下文示例都会被更新，以包括迄今为止找到的最佳解决方案。Detector Guidance[47]则是引入了T5文本编码器[52]，利用了其强大的先验知识来协助图文语义对齐。\n\n在  $\\mathrm{P2P}^{[53]}$  出现后的一系列研究中充分展示了交叉注意力机制作为连接文本信息与图像信息之间的桥梁，在促进文本与生成图像对齐方面的重要性。其中，注意力图中每个图像块对应注意力值的高低表明了该图像块生成其对应物体的概率高低。若注意力值过低，会导致物体无法生成，造成一系列错误。因此，Attend-and-Excite[54]提出一种最大化最小注意力激活值损失来指导去噪过程中潜在变量的变化，以此来提升物体的生成概率。在该研究的基础上，一些研究[55]-[57]提出在增强注意力的同时，要减少不同主体之间高注意力区域的重叠，防止多个物体合并成一个单一实体或者是一个物体的生成被另一个物体生成所抑制导致的物体缺失，即最终生成的图像无法区分这两个物体，只能选择两者中的一个。与此同时，还有一些研究[46],[58]-[59]提出通过不同属性-文本对之间及其内部的对抗损失，即利用注意力图之间的相似度作为损失，加强同一实体属性堆的注意力图相似度，降低不同文本属性对之间的注意力图相似度。该操作能够减少不同主体之间重叠，加强主体与其对应属性的注意力绑定，从而生成与提示文本对应的高质量图片。\n\n在仅依赖注意力图来获得增强损失或分离损失的情况下，模型无法确切知晓所提升的某个图像块的注意力值是否精确对应于目标物体所在的确切区域。这种不确定性导致模型倾向于在整个图像范围内随机选择增强区域，这种做法虽然在一定程度上能够提升某些区域的注意力值，但也很容易导致不同主体的高注意力区域发生重叠，进而模糊了图像中各个主体的边界。为了克服这一难题，后续的研究工作开始探索引入显式的布局控制策略。这一策略的核心目标在于明确并强化那些应当被增强的区域范围，从而确保模型在提升注意力值时能够有的放矢，避免不必要的重叠和模糊。在实现布局控制的过程中，对注意力图的操作主要采取了两种策略。第一种策略是直接对注意力图进行修改[60]-[71]。例如，Dense[60]、eDiff-I[61]等模型通过为每个对应的主体生成相应的布局掩膜，随后对这些掩膜进行应用，对相应文本的注意力图进行掩码处理。这样，在处理去噪步骤时，模型就能够更加专注于那些与目标主体相关的区域。SimM[65]模型则是通过将注意力图中最高值区域的注意力值与目标区域中的注意力值进行交换，从而有效地提高了目标对象的生成概率，进一步增强了图像的生成效果。另一种策略则是利用布局条件区域内的注意力图来推导相应的潜在向量的优化损失[66]-[70]。例如，RealCompo[67]模型借助了两种扩散生成模型，根据布局条件分别计算得到相应的优化损失。然后，该模型对这两个潜在变量进行优化处理，并采取一定的加权权\n\n重对它们进行融合。该融合结果作为下一步去噪步骤的输入变量，能够更加准确地反映目标物体的特征和结构，从而进一步提升图像生成的质量和准确性。\n\n同时，一些研究还揭示出自注意力在这过程中有至关重要的角色[64],[69]-[70]。自注意力机制衡量的是图像块之间的关联程度，对于图像结构的生成具有不可或缺的重要性。当图像块间的自注意力出现偏差时，同样会导致特征融合出现问题。因此，在利用交叉注意力图进行掩膜操作或依据其计算优化损失的同时，也需要对自注意力图施加更为严格的约束，或通过利用自注意力图所得到的损失，如Attention Refocusing[69]来进一步增强图像生成的效果，从而提升整体性能。\n\n此外，还有一个有趣的研究方向是在噪声维度上进行优化或组合。在扩散模型生成图像的过程中，给定一个文本提示以及不同的随机种子，所获的生成图像千差万别，在此其中，并不是所有的随机种子都能够生成与文本对应的图像。 $\\mathrm{INITNO}^{[72]}$ 将这些问题式归因于无效的初始噪声的存在。INNTO利用设计的交叉注意反应得分和自我注意冲突得分，将初始潜在空间正确划分有效区域和无效区域。让来自有效区域的噪声输入到文生图扩散模型时，会得到语义上合理的图像。同时，由于去噪后的潜在编码还有大量可解释性的信息，具有可加性，在一定程度上能够进行缩放不会损失其包含的表示。因此，Compositional[73]是在基于能量的模型的视角下理解扩散模型，利用可加性，通过使用文本提示来合成图像中的不同组件来实现概念连接，即直接将分割提示的预测的噪声相加。Isolated Diffusion[74]关键思想是分离各种概念的去噪过程来缓解相互干扰。将复杂的文本提示分割成一组更简单的文本提示，包括一个基本主题及其对应属性。利用分割条件下预测的噪声的线性组合进行去噪过程。而Noise Collage[75]则是通过多个并行的布局文本扩散模型生成提示文本中的每一个实体，此时的文本提示对对某一实体的详细描述，并结合注意力图最大值区域裁剪出对应实体的潜在向量。最终将获得的每一实体块加权拼接到有全局文本为条件的去噪后的潜在向量上，从而避免条件不匹配，将正确的对象放在正确的位置。\n\n当前最新的流行趋势是在生成图像完成后，利用多模态大语言模型、目标检测等方式[76]-[80]，找到生成图像中与提示文本不符合的部分，根据大语言模型的提示，使用图像编辑、个性化生成等多种形式重新生成或修正。Divide and Conquer[76]借助Agent技术，将多种模型进行集成，让模型根据输入条件自动从工具库中选取适当的模型使用，并利用反馈验证机制对生成图像进行修正。DivCon[76]和SLD[79]使用目标检测模型对生成图片进行检测，不同的是后续修正步骤。DivCon利用 inpainting技术对模型生成错误或者缺失的部分进行重新生成，SLD则是利用图像编辑步骤，按照LLM给定的编辑顺序一步步修正。\n\nMuLan[80]则是对文本进行分解，每一步生成过程中只生成一个物体，并使用多模态模型进行检测，确保每一步的生成正确性。\n\n基于无训练的组合式文生图算法，其核心优势在于能确保生成图像与提示文本的语义信息高度契合、精准一致。该算法还具备出色的兼容性，可无缝衔接至任意模型，且能适配各类任务的提示文本，全程无需额外的训练时间与成本投入。不过，该算法的缺点也较为突出。无论采用优化损失函数来调整潜在空间向量，还是融合多种模型生成的潜在空间向量，或是借助多模态大语言模型检测生成图像并依据其修改意见进行编辑，这些操作都会致使图像生成速度变慢，还可能在一定程度上造成图像质量下降。\n\n# 1.3 本文研究内容\n\n本文的研究围绕着提升扩散模型组合式生成能力的算法展开，旨在探索一种无训练的简便方式提升扩散模型生成图像与提示文本一致性能力。扩散模型的组合式生成任务面临着以下两类问题：属性绑定问题及空间交互问题。属性绑定问题包含四个子问题：（1）实体缺失，即生成的图像中遗漏了提示文本中明确指出的部分主体，导致图像内容不完整；（2）实体泄露，即当提示文本包含两种或多种主体时，生成的图像中主体类别数量少于文本描述，信息表达不充分；（3）属性互换，即在提示文本存在两个或更多属性-主体对的情况下，图像中的某个主体被赋予了本应属于另一个主体的属性，造成混淆；（4）属性泄露，类似于属性互换，但表现为图像中某一主体的属性与提示文本中另一主体的属性相混淆，而非完全互换。空间交互问题包括两个子问题：（1）空间位置错误，即生成图像中的两个及以上的主体之间的相对位置关系与提示文本中的不符，空间布局错乱；（2）交互关系错误，即生成图像中的两个及以上的主体之间的交互关系与提示文本中的不一致，逻辑关系错误。\n\n因此，本研究创新性地提出了两种无训练算法框架，如图1.2所示，旨在提升扩散模型的组合式文生图生成能力。这两种框架各自侧重解决不同的问题，通过不同的策略和技术手段，有效应对实体缺失、实体泄露、属性互换、属性泄露、空间信息错误以及交互关系错误等挑战，从而显著提升图像生成的质量与准确性，使生成的图像更加忠实地反映提示文本的内容与意图。在T2I-Compbench数据集上的大量实验一致地证明了本研究提出的两种算法框架在解决属性绑定问题和空间交互问题上的优越性。\n\n为解决属性绑定问题，本研究提出的基于注意力损失优化的组合式生成算法的目标在于利用扩散模型去噪过程中的交叉注意图来提升扩散模型组合式生成任务中的图文一致性。通过使用显式布局表示，在扩散模型中重新分配交叉注意。\n\n首先，利用大语言模型（LLMs）的代码链（CoC）提示技术来解释文本，并生成具有空间一致性的对象布局。在这些布局的基础上，分析不同词语注意力图上的注意力值分布情况，对注意力分布特性进行观察、分析、归纳后设计两个创新的损失函数：面向补丁的交叉注意（Patch-oriented Cross-Attention, PCA）损失和面向区域的交叉注意（Region-oriented Cross-Attention, RCA）损失。PCA损失强调了关注提示中所有令牌的图像补丁的高激活值，而RCA损失则侧重于提高布局内的平均注意力，从而增加了在指定区域内生成对象的可能性。这两个损失函数在去噪过程中修改了扩散模型去噪过程中潜在变量的值，使其每一步的去噪方向都更加朝着符合文本语义的方向前进。该方法的主要创新点在于：\n\n(1) 利用 LLM 作为布局生成器, 通过 CoC 提示来生成精确的布局以提高其性能。  \n(2) 引入两种新的损失函数，在基于生成的布局的扩散模型中重新分配交叉注意。  \n(3) 将本文方法与多个基线模型进行了综合的实验评估，同时将本文方法与其余基线模型进行结合，证明了它在增强生成图像和不同文本提示之间的对齐方面的优越性以及较强的鲁棒性。\n\n![](images/386450c22bd78982be882284095b971c88e1a87b86dd93ac647ac7410db59d53.jpg)  \n图1.2 研究内容关系\n\n为解决空间交互问题，本研究提出的基于注意力修正的多条件组合式生成算法的目的在于加强组合式生成任务中的空间位置任务生成准确率。同时利用注意力机制进一步加强属性绑定等任务的图文一致性。首先，分析先前的一些无训练优化方式来提升组合式生成能力的方位在空间位置任务上表现不佳的原因，由此提出借助当前多模态可控图像生成模型能力来优化该任务基本生成能力。其次，引入两种基于注意力的结构优化损失，包括基于自注意力的结构强化损失、基于交叉注意力的结构相似度损失。最后，提出一种直接修正的方式对生成图像进行编辑修正，减少因无法达到优化效果的生成错误现象。该方法的主要创新点在于：\n\n(1) 深入探讨、分析空间位置生成错误的具体原因，引入有效的多模态可控文生图模型加强空间位置条件约束能力。\n\n(2) 引入了基于注意力图的两种结构强化损失，以此来加强图像空间结构的正确性。同时，引入基于CLIP相似度检测的图像编辑修正模块对生成后图片再修正完善，提升模型生成图文一致性的能力。  \n(3) 将本文方法与多个基线模型进行了综合的实验评估，证明了它在增强空间位置任务上的有效提升，以及在属性绑定任务上的进一步优化。\n\n# 1.4 本文组织结构\n\n本文各章的安排如图 1.3 所示：\n\n![](images/3c0c246ab3525e98ff7f1be822e12ac21c21ee14d22f22a2a6be808bdcb2bd6a.jpg)  \n图1.3 本文组织结构示意图\n\n第一章：绪论。本章首先介绍了研究的背景和意义。其次，系统性的回顾了国内外在提升扩散模型在生成图文一致性能力上的研究现状，包括了基于微调训练的组合式文生图算法、基于无训练的组合式文生图算法，以及在上述两种算法中起到重要作用的多种形式布局生成算法。最后，对本文研究内容进行简要总结。\n\n第二章：相关理论基础。本章首先介绍了扩散模型，包括DDPM、SD、ControlNet、GLIGEN等一系列扩散模型理论基础及当前流行的扩散模型的架构模型。其次，详细介绍了先进的提升组合式生成能力的算法研究，包括Attend-and-Excite、SynGen等无训练算法。最后，介绍了本文实验所使用的数据集，以及当下对提升组合式生成能力研究常用的评价指标。\n\n第三章：基于注意力损失优化的组合式生成算法。本章首先对提出算法的动机进行详细地分析。其次对算法的设计进行详细描述。最后，利用T2I-Compbench数据集进行了大量的定量和定性实验，同时将本文方法与大量其他方法进行结合实验，验证本章方法在提升扩散模型组合式生成中属性绑定任务上的有效性。\n\n第四章：基于编辑修正的多条件组合式生成算法。本章中，首先对稳定扩散模型在空间位置生成准确性能较差的原因进行分析。其次，对算法的设计进行详细的描述。最后，利用T2I-Compbench数据集进行了大量的定量和定性实验，验证本章方法在提升空间位置控制方面得到有效的提升，同时也在属性绑定等任务上有进一步的效果提升。\n\n第五章：总结与展望。本章对基于注意力机制的提升扩散模型组合式生成能力算法研究工作进行总结。首先，概述所提出算法的核心思想和方法。其次，详细阐述了本研究在提升扩散模型组合式生成能力领域的创新点，并客观分析了研究中存在的布局和局限性。最后，展望了未来在提升扩散模型组合式生成能力领域中的研究方向。\n\n# 第2章 相关理论基础\n\n# 2.1 扩散模型\n\n本章节首先介绍扩散模型的基本原理，以及在此基础上提出的进一步研究研究，如加速推导、增添条件控制等。随后，详细介绍当前基于扩散模型的流行架构，如Stable Diffusion、DALL-E2等。此外，详细概述多模态可控模型如ControlNet、GLIGEN等备受关注的有效模型，它们通过精细控制生成过程，为生成用户所期待的图像提供了更高的可控性。\n\n# 2.1.1 DDPM模型原理\n\n扩散模型（Diffusion Model，DM）[81]-[84]是时下常用的一类生成模型，其本质是一种概率生成模型。扩散建模包括去噪扩散概率模型（Denoising Diffusion Probabilistic Models，DDPM）[81]和基于分数的生成模型[83]。DDPM分为前向过程和反向过程，两个过程都是一个参数化的马尔科夫链。\n\n在前向过程中逐步向数据中添加噪声，直到数据变成高斯噪声，如图 2.1 从右向左过程，即  $x_0 \\rightarrow x_T$  过程。从原始图像  $x_0$  开始，第  $t$  步在  $x_{t-1}$  的基础上添加噪声得到  $x_t$ ，直到  $T$  步后  $x_T$  完全变成高斯噪声。然后在反向过程中，逐步从高斯噪声中学习，将噪声图像还原为原始数据，如图 2.1 从左向右过程，即  $x_T \\rightarrow x_0$  过程。首先给定高斯噪声  $x_T$ ，通过逐步去噪，直至将原始数据  $x_0$  恢复。\n\n![](images/c00e5914ac0e246f5912f1d166553bc0c9caee76e8f6f7de659f99d3c65fe97f.jpg)  \n图2.1DDPM模型（图片来自文献[81]）\n\n对于具体数学表示来说，前向加噪过程中  $x_{t}$  只与  $x_{t - 1}$  相关，因此逐步向数据中添加高斯噪声的过程可以被条件概率分布所表示为：\n\n$$\nq \\left(x _ {t} \\mid x _ {t - 1}\\right) = N \\left(x _ {t}; \\sqrt {1 - \\beta_ {t}} x _ {t - 1}, \\beta_ {t} \\mathrm {I}\\right) \\tag {2.1}\n$$\n\n其中,  $\\beta_{t} \\in (0,1)$  表示  $x_{t}$  的方差, 且  $\\beta_{t}$  随着时间步的增加, 逐渐增大。随着  $t$  增大,令  $x_{t}$  的分布逐渐趋向于标准正态分布。同时, 利用重参数化技巧, 令  $\\alpha_{t} = 1 - \\beta_{t}, \\bar{\\alpha}_{t} = \\prod_{i=1}^{t} \\alpha_{i}$ , 可将  $x_{t}$  表示为以下形式:\n\n$$\nx _ {t} = \\sqrt {\\bar {\\alpha} _ {t}} x _ {0} + \\sqrt {1 - \\bar {\\alpha} _ {t}} \\epsilon , \\epsilon \\sim N (0, I) \\tag {2.2}\n$$\n\n因此，最终将式（2.1）转化为：\n\n$$\nq \\left(x _ {t} \\mid x _ {0}\\right) = N \\left(x _ {t}; \\sqrt {\\bar {\\alpha} _ {t}} x _ {0}, (1 - \\bar {\\alpha} _ {t}) \\mathrm {I}\\right) \\tag {2.3}\n$$\n\n在反向去噪过程中，如果已知真实分布  $q(x_{t}|x_{t - 1})$  ，则还原原始数据轻而易举。但是，在生成过程中，真实分布难以获得。因此，使用模型  $p_{\\theta}$  来学习分布  $q(x_{t}|x_{t - 1})$\n\n$$\np _ {\\theta} \\left(x _ {0: T}\\right) = p \\left(x _ {T}\\right) \\prod_ {t = 1} ^ {T} p _ {\\theta} \\left(x _ {t - 1} \\mid x _ {t}\\right) \\tag {2.4}\n$$\n\n$$\np _ {\\theta} \\left(x _ {t} \\mid x _ {t - 1}\\right) = N \\left(x _ {t - 1}; \\mu_ {\\theta} \\left(x _ {t}, t\\right), \\Sigma_ {\\theta} \\left(x _ {t}, t\\right)\\right) \\tag {2.5}\n$$\n\n在训练过程中，模型的目标是最大化对数似然，即优化  $x_0 \\sim q(x_0)$  下的  $p_\\theta(x_0)$  的交叉熵，此时，使用 VLB 来优化负对数似然函数：\n\n$$\n- \\log p _ {\\theta} (x _ {0}) \\leq \\operatorname {E} _ {q (x _ {1: T} | x _ {0})} \\left[ \\log \\frac {q (x _ {1 : T} | x _ {0})}{p _ {\\theta} (x _ {0 : T})} \\right] \\tag {2.6}\n$$\n\n随后，通过化简推导、变形，最终优化函数表示为：\n\n$$\nL _ {s i m p l e} = \\operatorname {E} _ {x _ {0} \\sim q (x _ {0}), \\epsilon \\sim N (0, 1), t \\sim U (1, T)} \\left[ \\left\\| \\epsilon - \\epsilon_ {\\theta} \\big (\\sqrt {\\bar {\\alpha} _ {t}} x _ {0} + \\sqrt {1 - \\bar {\\alpha} _ {t}} \\epsilon , t \\big) \\right\\| ^ {2} \\right] \\quad (2. 7)\n$$\n\n在该基础上，随后提出一系列研究对DDPM模型进行优化增强[82]-[85]。如 $\\mathrm{DDIM}^{[82]}$ ，其特点是不再限制扩散过程必须是一个马尔卡夫链，这使得DDIM可以采用更小的采样步数来加速生成过程。同时，DDIM从一个随机噪声  $x_0$  加噪到初始去噪向量  $x_{T}$  的过程是一个确定性的过程。DDPM是一种无条件生成模型，随后出现了Classifier Guidance[84]、Classifier-Free Guidance[85]等一系列条件图像生成方法，为后来进一步的技术研究提供坚实的基础。\n\n# 2.1.2 基于扩散模型的文生图模型\n\n在图像生成技术的浪潮中，OpenAI凭借其强大的研发实力，推出了两款备受瞩目的文生图模型——GLIDE[86]与DALL-E2[87]。随后，Google推出具有相似架构和出色性能的Imagen[1]模型。这些生成模型为文生图领域增添了新的活力。Stable Diffusion模型则以其独特的潜在扩散架构，为图像生成技术带来全新视角。\n\nOpenAI率先提出GLIDE模型，沿用了ADM架构。它通过将输入文本编码成Token Embeddings，并融入U-Net的每一个注意力块和残差块中，实现了文本引导的图像生成。此外，GLIDE还使用了无条件引导技术，使模型具有无条件生成能力。紧随其后，提出的DALL-E2利用  $\\mathrm{CLIP}^{[88]}$  提取的文本特征，使用级联式架构生成高分辨率图片。首先，利用CLIP模型将输入条件文本转换到文本-图像的统一特征空间。其次，第一阶段是用过先验模型将文本特征与图像特征进行\n\n对齐，此时使用了Decoder-only的Transformer模型直接预测向量。最后，第二阶段利用扩散模型将视觉特征转化为生成图片。DALL-E2模型如图2.2所示。\n\n![](images/746d4d8f01b29e04385671fe2f8d2a91ccd8a2cee1b52f5b998a9d7e5a3011e0.jpg)  \n图2.2DALL-E2模型架构（图片来自于文献[87]）\n\n与DALL-E2模型结构思想类似的还有Imagen模型。与之不同，Imagen使用T5-XXL作为文本编码器，结合了条件扩散模型，引入了新的Efficient U-Net架构等方式能够处理复杂文本描述，使生成的图像与文本的契合度更高。同时，Imagen提出在这一框架下扩展文本编码器的效果，比扩展扩散模型更重要。\n\n![](images/0d855f4c6f5cd71fa923730889884399f43ef4585faa6099824bd05e20ee2ce9.jpg)  \n图2.3StableDiffusion模型架构（图片来自于文献[3]）\n\nStable Diffusion 模型引入交叉注意力机制[89]来实现提示信息的理解和融合，其本身不是一个专门为文生图设计的模型，但它在文本引导的图像生成方面表现出了强大的能力。同时，先前研究的扩散模型是直接在像素空间中进行运行，其所需的训练和推理过程耗费巨大且速度慢。因此，为了能在有限的计算资源上实现训练和推理，SD使用编码器将图像映射到一个低维潜在空间中进行计算，最后使用解码器将潜在空间向量转变为图片。SD模型架构如图2.3所示。SD模型\n\n与先前扩散模型的主要区别在于引入了 Auto Encoder 和 Decoder 模块，让模型不再是在像素空间中之间运算，而是通过 Encoder 模块，将原始图片降维到一个较小的潜在变量  $z$  上进行运算后，通过解码器解码获得生成图片。具体而言，给定一张图像  $x \\in R^{H \\times W \\times 3}$ ，通过编码器  $\\varepsilon$  编码  $x$  到潜在空间表示  $z = \\varepsilon(x)$ ，随后解码器  $D$  从  $z$  中重构出图像，即  $\\tilde{x} = D(z) = D(\\varepsilon(x))$ ，其中  $z \\in R^{h \\times w \\times c}$ 。\n\n![](images/4242a87d9b569970e6ab129d09e0d74424fe3c7d6dc76a6f25a01686ebc9fb81.jpg)  \n(a) Stable Diffusion  \n(b) ControlNet  \n图2.4 ControlNet模型结构图（图片来自于文献[12]）\n\n为实现生成与提示信息相对应的图像，最直接的问题是如何让模型理解提示。此时引入了条件编码器  $\\tau_{\\theta}$  。SD可以使用分割图、文本、图像等多种提示信息生成图像。SD中使用CLIP文本编码器将文字串转换成计算机能够理解的具有输入文字信息的语义向量，作为后续去噪模块的重要控制输入。SD以U-Net结构作为去噪模块，并在其中加入了注意力机制，以此实现生成与提示文本一致的图像。注意力机制中包含两种：交叉注意力和自注意力。其中，交叉注意力层为文本提示为生成条件，从而实现文本到图像的融合。自注意力则是为实现全局信息控制。SD的采样阶段从随机噪声图像开始，在U-Net中利用注意力机制控制每一步去噪的方向，经过多次迭代，最终生成与文本提示语义一致的高质量图像。\n\n由于仅仅通过提示文本描述，难以精准地表达复杂的布局、姿态等，单纯的文生图模型无法生成与用户期望一致的图片。因此，多模态可控图像生成模型应\n\n运而生，可通过除文本外的额外控制条件，如布局、关键点、分割图等，不仅能够根据文本描述生成图像，还实现了对图像生成过程的精细控制，为图像创作带来了更多的可能性和创意空间。下文将介绍几种具有代表性的多模态可控生成模型，包括 ControlNet、T2i-Adapter、GLIGEN、MIGC 等。\n\nControlNet 通过复制预训练扩散模型中的 U-Net 模块结构与参数，并利用额外控制条件对其进一步训练，从而巧妙地将其转化为一个专注于提取输入控制条件特征的强大工具。其模型架构如图 2.4 所示。同时，ControlNet 巧妙地采用了一种名为“零卷积”的独特技术，其具体结构如图 2.5 所示。零卷积层是一种具有初始化为零的权重和偏置的  $1 \\times 1$  卷积层。这一设计能够在训练初期确保 ControlNet 与初始网络在功能上完全等价，从而为模型提供了一个极为有利的初始化条件。这有助于模型更快地收敛，还进一步提升了其在学习和适应新添加的控制条件方面能力。\n\n![](images/52361db1905547cc4db15884c6f44e401bdddd673b1a8dce9bc5587896569b1a.jpg)  \n(a) Before\n\n![](images/d3c954918a5af3ad7f04978f3c3b25a7bc7cdaa8a2a60ff001950102c07839dc.jpg)  \n(b) After  \n图2.5 零卷积层图示（图片来自于文献[12]）\n\nT2I-Adapter 则不同，其主要解决的问题是“对齐”问题，即内部知识和外部控制信号应该对齐。因此，T2I-Adapter 是在预训练的扩散模型的基础上，添加外部的一个小模型，即由 4 个特征提取块和 3 个下采样块组成的网络，利用相对少量的数据来学习这种对齐，实现额外控制条件的特征提取，为预训练模型提供了额外的指导。此时，根据不同条件训练多种不同适配器，实现多条件可控生成。\n\nGLIGEN 基于并扩展了现有的预训练的文本到图像扩散模型的功能，使它们也能够以接地输入为条件。接地输入包括边框布局信息和其相应的文本标签信息。为了保留预训练模型的庞大先验概念知识，冻结了它的所有权重，并通过门控机制将接地信息注入到新的可训练层中，即冻结自注意力层和交叉注意力层的权重，并在两层中间添加一个新的门控自注意力层，如图 2.6 所示。此时，门控自注意允许视觉特征利用边界框信息，并且由此产生的接地特征被视为残差。GLIGEN 具体的模型结构如所示。GLIGEN 在采样步骤的前半部分使用完整模型（所有层），而在后半部分仅使用原始层（不含门控变压器层），可以产生准确反映接地条件的结果，同时具有高图像质量。\n\n![](images/f34a6bfff15815203486624275dfbd6b7901a94279b8b4d9a7a75ef4e933288e.jpg)  \n图2.6 GLIGEN门控自注意力结构（图片来自于文献[16]）\n\n# 2.2 组合式文生图相关技术\n\n扩散模型由于文本编码器的缺陷、训练数据集及训练范式的误差，最终导致生成图像与提示文本语义不符的情况。这主要可分为两类问题：属性绑定问题及空间交互问题。前者包括实体缺失、实体泄露、属性互换、属性泄露四类子问题，后者包括空间位置错误、交互错误问题。\n\n当前主流的解决组合式生成任务算法中主要为两大类：一种利用模型结构优化、训练范式修正、高质量训练数据等多种方式对扩散模型进行微调实现性能提升；另一种则是借助注意力机制、反馈修正等即插即用无训练的方式，引导扩散模型生成与提示文本相符的图像。在这两种方案中，对注意力机制的利用都是重中之重，影响着最终生成的质量效果。本章将对当前主流的算法研究进行阐述。\n\n# 2.2.1 注意力机制\n\n扩散模型作为一种强大的生成模型，其中的核心机制武义是注意力机制。深入探讨扩散模型中的注意力机制，解析其如何在复杂的特征空间中捕捉关键信息，对本文的研究任务有着重要作用。本章节以文生图形式的SD模型，对其中注意力机制的原理进行详细阐述。\n\n具体来说，使用一个预训练CLIP文本编码器对文本提示  $w = (w_{1}, w_{2}, \\ldots, w_{n})$  进行处理获得文本向量特征  $c = f_{CLIP}(w) \\in R^{n \\times e}$ ，此时  $e = 77$  为特征嵌入维度。从  $c$  中通过线性映射获得键  $K \\in R^{n \\times d}$  和值  $V \\in R^{n \\times d}$  。给定一组通过大小为  $h' \\times w'$\n\n的特征图计算得到的查询块  $Q \\in R^{n_q \\times d}$ , 此时  $n_q = h' \\times w'$  。交叉注意力图推导为:\n\n$$\nA ^ {c} = \\operatorname {s o f t m a x} \\left(\\frac {Q K}{\\sqrt {d}}\\right) \\in [ 0, 1 ] ^ {n _ {q} \\times n} \\tag {2.8}\n$$\n\n此时，通过文本向量获得  $K$  、  $V$  与图像特征图获得的  $Q$  进行计算，是为了利用图像块对文本信息做注意力实现逐步的文本特征提取和耦合，实现把文本信息注入到图像信息中，使得 SD 模型学习到文本与图像之间的特征对应关系。在第  $t$  步去噪过程中，通过对  $A^c$  索引及维度重塑，可以利用  $A_i^c \\in [0,1]^{h' \\times w'}$  来表示一个单词标记  $w_i$  与特征图中的每一个空间位置之间的注意力图。\n\n自注意力机制是为了便于全局信息的使用而存在。它将每一个空间位置上的特征传播到分辨率为  $h^{\\prime} \\times w^{\\prime}$  的特征图中的一个相似区域。与交叉注意力一样的计算公式，但于交叉注意力机制不同的是，公式中所使用的  $Q$ 、 $K$ 、 $V$  都是从大小为  $h^{\\prime} \\times w^{\\prime}$  的特征图计算得到的。这是为了可以将输入图像的不同部分的图像块进行交互，从而实现特征的整合和全局上下文的引入，同时还能够在不考虑位置的情况下捕捉特征之间的关系，有助于帮助模型建立捕捉图像全局关系的能力，理解不同位置的像素之间的依赖关系，从而更好地理解图像的语义。自注意力图可被推导得出  $A^{s} \\in [0,1]^{n_{q} \\times n_{q}}$ 。同样地，可以使用  $A_{p}^{s} \\in [0,1]^{h^{\\prime} \\times w^{\\prime}}$  来表示与像素  $p$  相关的所有像素的相关联程度。\n\n在获得原始的交叉注意力图之后，要经过一个高斯平滑函数之后，才可以进行下一步操作。由于高斯平滑滤波器对于服从正态分布的噪声非常有效，因此它常被用于消除图像中的高斯噪声。同时，在进行边缘检测、特征提取等图像处理任务前，通常需要对图像进行平滑处理，以减少噪声对后续处理的影响。高斯平滑操作是一个二维卷积操作，用于“模糊”图像，可以有效地去除高频噪声，提高图像的信噪比。同时，由于高斯平滑能够保留图像的边缘信息，因此它不会导致图像的边缘模糊化。高斯平滑类似于均值滤波器（假如  $3 \\times 3$  ，则全部相加取均值，即成为中间点的像素值），但它使用不同的核表示高斯驼峰的形状。二维高斯平滑函数可以表示为：\n\n$$\nG (x, y) = \\frac {1}{2 \\pi \\sigma^ {2}} e ^ {\\frac {x ^ {2} + y ^ {2}}{2 \\sigma^ {2}}} \\tag {2.9}\n$$\n\n因此，本文中要进行损失计算的注意力图通过下式获得：\n\n$$\nG a u s s i a n S m o o t h i n g (A) \\rightarrow A \\tag {2.10}\n$$\n\n# 2.2.2 多实例文生图算法\n\n当前，组合式文生图算法为解决复杂场景的语义对齐问题，主要形成了两种技术路线：其一是基于模型微调的参数化方法，通过引入额外控制条件（如空间约束）或重构损失函数，针对性提升模型对组合式文本描述的解析能力；其二是\n\n基于去噪过程优化的非参数化方法，聚焦于扩散模型核心的交叉注意力机制——该机制作为文本-图像跨模态特征融合的桥梁，其权重分布直接决定了去噪路径的语义导向。前者虽能通过参数调整实现精准控制，但存在计算成本高、泛化性受限的瓶颈；后者则通过动态优化潜在空间向量，以即插即用的方式规避模型微调，在保持生成自由度的同时显著降低时空开销。\n\n在模型微调范式下，针对组合式提示中细粒度语义对齐的挑战，TokenComposer[90]创新性地提出标记级监督机制。该方法通过联合优化扩散模型的原始去噪目标与基于语义标记的接地目标，构建双路径训练框架。具体来说，首先，利用预训练的图像理解模型（如分割模型）来获取文本中的每个名词标记相对应的二进制分割图。在训练过程中，提出两种损失  $L_{token}$  和  $L_{pixel}$  对交叉注意力图进行优化，前者确保文本标记与图像区域对应起来，后者用于约束激活区域是否在分割图所指定的区域内。具体模型图如图2.7所示。\n\n![](images/ea21a2936b284c46753f4ed3b442c4043245644bbd37f132be635f6bc7cfcb3c.jpg)  \n图2.7 TokenComposer模型架构图（图片来自文献[90]）\n\nDreamSyn[43]通过引入反馈机制并采用迭代自训练的方法对模型进行训练。首先，根据给定文本生成一批图像，并利用多模态模型和美学评估模型进行图文匹配度评估，并从中选取最优的图片，利用选取的最佳图片使用LoRA技术对模型进行微调。以此循环迭代，DreamSyn不断提升文本-图像对齐度与美学质量。\n\n与先前研究如多模态大语言模型反馈、注意力图修正等修正方式不同，MIGC[17]采用了“分而治之”的思想，将复杂的多实例任务拆解为多个简单的单实例生成子任务。每个子任务只关注一个实例的生成，从而降低了问题的复杂性。在处理子而避免了文本泄露问题。在得到每个子任务的解后，MIGC根据用户输入的布局信息，对每个实例的信息进行显式的空间掩码过滤，确保了不同实例的属性在融合过程中不会相互泄露，从而保证了最终生成图像的准确性和布局正确性。最后整合每个子任务的解，将多个单实例生成的结果融合成一个完整的图像，同时保持每个实例的属性和位置准确性。其具体模型架构如图2.8所示。\n\n![](images/7221b02adcabcea3dd379fb694c2f325f531ce47196c499d13428dc12ef89d64.jpg)  \n图2.8 MIGC模型结构图（图片来自文献[17]）\n\n从最初  $\\mathrm{P2P}^{[53]}$  提出交叉注意力机制是控制文本与图像之间关系的关键，通过操控该机制可以实现图像的编辑。交叉注意力层能够融合视觉和文本嵌入，为每个文本标记产生空间注意力图，从而影响生成图像的结构和外观。因此，利用注意力机制来提升生成图像和提示文本的一致性是顺其自然的。当前基于即插即用的组合式图像生成算法中主要的优化方向分为以下三种：其一，对初始噪声进行优化，给扩散模型一个良好的先验区间或是对去噪后潜在向量进行拼接替换；其二：使用注意力机制对潜在空间向量进行优化或对注意力图直接进行修改；其三，在图像生成完成后，借助大模型，对生成图像进行编辑修改。\n\nAttend-and-Excite 强调在文生图模型的去噪过程中实时干预，根据交叉注意力图设计的损失动态调整潜在空间向量，努力确保所有主题标记都能被模型所关注，以提高生成图像的语义准确性。其根据交叉注意力图设计损失的核心思想是提高需要关注的文本标记中的最大的注意力图值，以此来提升物体生成概率。具体而言，检索出所需要关注的文本标记的交叉注意力图，分别找到图中的最大值。随后，在所有需要关注的文本标记的注意力最大值的中，找到其中的最小值，如果该值小于设定的阈值，则根据该损失值对潜在空间向量进行优化。同时，该优化过程主要集中于去噪前期，且在某些时间步上，会进行迭代优化，直至符合阈值约束或者是达到事先设定的最大迭代轮数后退出。多种措施的提出，均是因为在去噪过程前期，图像的大致结构已经形成，随后去噪步骤则是为了在这前期过程中增添细节信息。故在去噪前期就要对潜在空间向量进行优化，在后续步骤不进行优化是为了保证生成图片的质量。该研究还提出在缓解实体缺失问题的同时，属性绑定问题也会随着实体缺失问题改善而提升，即属性绑定的解决依赖于物体缺失的解决。具体模型图如图 2.9 所示。\n\n在Attend-and-Excite模型的基础上，后续的一系列研究不仅致力于提升最小激活注意力值，还探索了如何更有效地将不同属性文本对的激活值区域进行分散，以减少不同文本标记间的区域重叠现象。一部分研究引入了额外的控制因素，如\n\nBounded Attention[70]引入布局信息，通过在初始阶段增强布局内的注意力图的注意力值，有效限制不同主体之间的信息流动。这一过程确保每个主体的注意力能够集中在其对应的边界框内，从而最大限度地减少因相似性而导致的语义混淆。在后续生成阶段，Bounded Attention采用注意力掩膜机制，进一步减小语义泄漏的风险。这一机制确保每个主体的特征在生成过程中保持独立，避免了不同主体之间的视觉特征混合。这种方法通过精细控制信息流动，不仅提升了图像的整体质量，也增强了对用户输入的响应能力，使得生成图像更好反映给定文本提示。\n\n![](images/ba7e8b6e68280ffe0fe9e3cec064fdf6ad8eeed43387b3eb9a5f52a9e449fc22.jpg)  \n图2.9Attend-and-Excite模型图（图片来自文献[54]）\n\n另一部分研究巧妙地避免了使用额外的控制条件，而是引入对比学习的思想，通过计算不同属性文本内部及相互之间的注意力图相似度，以此作为对抗损失来优化区域划分，实现了自然且有效的区域分隔。Conform[58]利用对比学习的理念，通过构建正负样本对来优化文本到图像生成。具体而言，将文本提示中的主体及其属性分组，形成正样本对（如颜色和对象）和负样本对（如不同对象的颜色），采用InfoNCE损失函数增强正样本对的吸引和对负样本对的排斥，同时，将注意力地图从时间步长  $t + 1$  损失计算，有效加倍的令牌计数用于计算损失函数，创建对基于注意地图从相同的时间步，以及跨时间步使得模型能够在生成过程中保持对特定对象和属性的关注，减少注意力的散布效应。\n\n# 2.3 布局生成算法\n\n在多模态可控图像生成领域中，布局预测技术正逐步成为连接抽象描述与具象图像生成之间的桥梁。近年来，随着深度学习技术的不断发展，特别是大模型的出现，为图像生成中的布局生成提供了新的解决方案。当前主流的布局生成预测方法主要分为两类：训练一种布局预测器[62][76]或者是利用大语言模型根据提示文本推理[63][69]得到。\n\n# 2.3.1 布局预测器生成布局\n\n当前的布局预测器通常是基于深度学习架构的复杂模型，它们具备从输入的文本提示、标签集合或其他形式的信息中解析并预测图像中各元素精确位置和尺寸的能力。这一方法的显著优势在于其高效性与准确性，尤其是在处理包含多层次、多元素交互的复杂布局信息时表现得尤为突出。相较于传统方法，布局预测器不仅加快了图像生成的速度，还显著提升了生成图像的质量与真实性，使得从文本到图像的转化过程更加流畅自然。然而，布局预测器的强大功能并非凭空而来，其背后隐藏着对大量高质量标注数据的依赖。这些数据不仅需要精确标注每个元素的位置、大小，还需反映元素间的相互关系，这对于模型的训练至关重要。此外，面对多样化的图像生成任务，如室内场景设计、自然风光合成，往往需要针对性地设计和调整布局预测器，以适应不同任务对布局结构、元素种类及交互方式的特定要求。这种定制化需求无疑增加了模型开发与部署的复杂度。\n\n早期的研究尝试利用条件生成对抗网络[28]（Conditional Generative Adversarial Networks, GANs）来解决这一问题。GANs通过引入对抗训练机制，使得生成的图像在视觉上更加接近真实世界的样本，从而在多个图像生成任务中取得了显著成效。特别是在结合文本描述进行图像合成时，条件GANs能够根据给定的文本提示生成与之匹配的图像内容。然而，当面对包含多个交互对象的复杂场景时，这些方法的局限性便显露无遗。尤其是在文本描述未能明确界定各交互对象的具体属性和相互关系时，模型往往难以准确捕捉并再现这些复杂的场景结构，导致生成的图像在逻辑连贯性和视觉真实性上大打折扣。\n\n为了克服这一挑战，研究者们开始探索更为精细的布局表示方法。LayoutVAE[91]便是在这一背景下应运而生的一种创新方案。它不再局限于简单的对象标签输入，而是能够处理一组对象标签集合，预测每个类别的实例数量以及整个场景的空间布局。这种从全局视角出发的布局预测策略，有效提升了模型对复杂场景结构的理解能力。随后，NDN[92]（Neural Design Network）通过整合图卷积网络（Graph Convolutional Network, GCN）[93]和条件变分自编码器\n\n（Conditional Variational Autoencoder, VAE），进一步推动了布局生成技术的发展。NDN能够从用户指定的约束条件出发，自动生成符合这些条件的设计布局，从而实现了从抽象需求到具体设计方案的快速转化。这一过程中，GCN的引入使得模型能够更好地捕捉元素间的关联信息，而条件VAE则确保了生成的布局既符合用户预期又具有一定的创新性。\n\n鉴于某些输入信息可能不足以充分描述用户想要的画面细节，研究者们开始探索使用场景图（Scene Graph, SG）作为中间文本表示[94]。场景图是一种结构化的信息表示方式，它不仅能够明确对象间的空间关系，还能捕捉对象的属性信息。随着场景图复杂性的增加，一些研究开始尝试从数据中学习规范图表示[95]，以提高复杂场景布局生成的准确性和多样性。这种方法通过引入额外的语义信息，增强了模型对复杂场景结构的解析和生成能力。\n\nText2Scene[96]则是另一种基于文本描述生成物体布局的方法，它属于文本到图像生成领域的一个重要分支。该方法利用长短时记忆网络（Long Short-Term Memory, LSTM）作为文本编码器，提取文本特征，并通过递归式布局生成器按顺序生成物体布局。在布局生成阶段完成后，系统会将预裁剪的物体元素精确地放置在画布上，并利用图像修复技术补充边缘区域，最终合成一幅完整且和谐的图像。值得注意的是，Text2Scene在整个过程中无需使用生成对抗网络，这在一定程度上简化了模型结构并降低了计算成本。尽管该方法在多物体场景的图像生成质量上取得了显著提升，但仍存在改进空间，尤其是在处理极端复杂场景和精细纹理生成方面。\n\n近期，LayoutTransformer[97]的提出为布局生成技术带来了新的突破。LayoutTransformer采用了一种名为LT-Net的布局变压器网络架构，它能够接收场景图作为输入，并唯一地编码语义特征，充分利用这些特征的共现和隐式关系。这种编码方式不仅增强了模型对复杂场景结构的理解能力，还允许用户通过操作这些概念上多样化但看似合理的布局输出来实现定制化设计。此外，LT-Net的解码器能够将编码的上下文特征转换为保持自监督关系一致性的边界框，这些边界框不仅准确反映了元素的位置和大小，还保持了元素间的空间关系。通过将生成的布局分布拟合到高斯混合模型中，LT-Net能够生成具有空间多样性的布局，从而进一步提升了图像生成的多样性和真实性。\n\n# 2.3.2 大语言模型生成布局\n\n当前, 随着多模态大语言模型在视觉-语言联合表征和跨模态推理能力上的突破性进展, 以 KOSMOS-2.5[98]、MiniGPT-4[99]为代表的先进模型, 通过融合视觉感知与语言理解的双通道架构, 实现了从 “语义理解-空间推理-视觉生成” 的全\n\n链路自动化处理，在自然语言理解和文本生成任务方面展现高超能力，不仅能够准确理解复杂多变的自然语言描述，还能基于这些理解生成出既符合语法规则又富有逻辑性和连贯性的文本内容。因此，利用大语言模型根据输入的文本提示自动地生成图像布局的应用也是当前的主流趋势。这种方法依赖于大语言模型的理解能力和生成能力，其优势在于它能够捕捉到文本描述中的细微差别，并生成与之相匹配的布局信息，从而实现更加精细的图像生成控制。在本文研究内容中，需要借助大语言模型生成布局信息。如Attention Refocusing直接使用GPT-4从文本提示中预测布局，不需要其他额外操作。然而，这种方法也面临着计算量大、生成速度慢以及可能存在的语义理解偏差等问题。为此，一些研究[101]-[100]利用更加明确的提示词和框架方式提升模型的能力，如Reason on Layout[101]利用LLM和思维链（Code of Thought, CoT）[102]框架生成空间一致的布局。\n\n大语言模型在推理任务中执行少镜头提示能力是通过给定一个由三部分组成的提示，即输入、思维链和输出。思维链是一系列生成输出的中间自然语言推理步骤，即 CoT 提示就是通过解构任务指令和上下文示例中的解构任务过程，为 LLM 提供了一个推理框架，而无需对模型的参数进行迭代修改。这种思维链的指导模型是为了进一步加强对语义的解耦能力，使得模型可区分文本提示中的主体对象、装饰元素、风格限定等不同语义层次。这种方法在具有有限的基于实例的学习的场景中特别有效。它的功能是使用直接的教学句子来提示 LLM 按顺序处理信息，或者通过展示一系列的例子来说明一步一步[101]-[103]的推理过程。\n\n简而言之，思维链是解决推理任务时人类思维过程遵循的一系列典型步骤。它可以帮助将一个问题分解成一系列的子问题，然后逐个解决这些子问题，从而得出最终的答案。CoT在语义推理任务方面表现出色，但在涉及数值或符号推理的问题上往往表现出困难。因此有研究提出代码链（Code of Code, CoC）[104]，其关键思想在于，鼓励大型语言模型将复杂的程序或逻辑问题中的各个语义子任务，以一种灵活且易于理解的伪代码形式进行格式化。这种伪代码不仅保留了原始问题的逻辑结构，还以更加明确和机器可读的方式呈现了问题的各个组成部分。通过该方式，CoC能够利用大型语言模型在自然语言理解和生成方面的优势，同时克服其在直接处理数值或符号推理时的不足。更重要的是，CoC引入了一个解释器组件，该组件能够明确捕获并处理伪代码中可能存在的未定义或模糊行为。这一特性对于确保推理过程的准确性和完整性至关重要，因为它能够及时发现并纠正潜在的逻辑错误或计算失误。一旦解释器识别出这些问题，它会将这些未定义的行为或错误传递给模拟大型语言模型，模型则可以根据这些信息进一步调整和优化其推理过程。通过上述列举的一些方法，将其应用于布局生成时，提升语言模型生成准确性，使LLM能够更有效地指导模型更加关注提示文本的空间关系。\n\n# 2.4 评估数据集及评估指标\n\n# 2.4.1 数据集\n\n先前的研究大多集中在某一个子问题上，如Attend-and-Excite解决实体缺失，Structure Diffusion解决属性绑定，并提出了他们自己的数据集和定量指标来评估他们的方法。然而，对于组合文本到图像生成问题具体定义和衡量标准，并未达成共识。T2I-CompBench[25]是一个用于开放世界组合文本到图像生成的综合性基准数据集。该数据集旨在解决当前文本到图像生成模型在复杂场景中组合不同属性和关系的挑战。本文研究目的是解决扩散模型组合式生成任务难题。因此，该数据集的内容非常吻合本文研究目的，且数据集包含任务丰富、衡量指标多样。T2I-CompBench包含1800个测试用组合文本提示，分为三大类和六个子类，每一子类中都包含300条文本提示。T2I-Compbech的任务具体如下：\n\n(1) 属性绑定（Attribute Binding）：由于模型在文本提示符中有多个属性和多个对象时，倾向于混淆属性和对象之间的关联。因此，该类别中的每个文本提示符至少包含两个对象和两个属性，并且模型应该将这些属性与正确的对象绑定，以生成复杂的场景。该类别包含三个子类别，分别是：颜色绑定（Color Binding）；形状绑定（Shape Binding）；纹理绑定（Texture Binding）。  \n(2) 对象关系（Object Relationships）：当在一个复杂场景中组合对象时，对象之间的关系是一个关键的因素。因此，该类别中的每个文本提示包含至少两个对象之间具有指定关系的对象。根据关系的类型，该类别由空间关系（Spatial Relationships）和非空间关系（Non-Spatial Relationships）两个子类别组成。  \n(3) 复杂组合（Complex Compositions）：该类别中的每个文本提示包含两个以上的对象或两个以上的子类别。例如，提示文本中包含两个属性对象及其关系。\n\n# 2.4.2 评估指标\n\n组合文本到图像模型的评估是一个巨大的挑战。现有的文本到图像生成的度量可以分为保真度评估、对齐评估。传统的指标如  $\\mathrm{IS}^{[105]}$  和  $\\mathrm{FID}^{[106]}$  常用来评估合成图像的保真度。为了评估图像-文本对齐情况，通常采用  $\\mathrm{CLIP}^{[88]}$  和 BLIP 字幕[37][38]的文本-图像相似度。然而，由于图像字幕的模糊性和组合视觉-语言理解的困难，这两种指标在组合性评价中都表现不佳。因此，根据不同类别的组合提示提出了不同的评估指标。\n\n针对属性绑定任务评估，提出了解纠缠 BLIP-VQA 以克服模糊的属性对应。BLIP 字幕模型和 CLIP 文本向量的文本相似度评估的主要局限性在于，BLIP 字幕模型并不总是描述每个对象的详细属性。例如，BLIP 字幕模型可能会将一个图像描述为“a room with a table, a chair, and a curtains”，而生成这个图像的文本提示符是“a room with yellow curtains and a blue chair”。因此，仅仅是单纯地比较文本-文本的相似性容易导致歧义和混淆。因此，利用 BLIP VQA 的视觉问题回答能力来评估属性绑定。如考虑到由文本提示“a green bench and a red car”生成的图像，利用“green bench？”和“red car？”两个问题来提问。通过显式地将复杂的文本提示分解为两个独立的问题，其中每个问题只包含一个属性-主体对，避免了 BLIP-VQA 的混淆。BLIP-VQA 模型将生成的图像和根据文本拆分的几个问题作为输入，将回答“是”的概率作为一个问题的分数。通过乘以每个问题回答“是”的概率来计算总分数。\n\n针对空间关系任务评估，提出基于UniDet模型[107]的评价体系。大多数多模态视觉语言模型在推理空间关系方面表现效果不尽人意，如模型无法正确判断“Object A on the right of Object B”是否是正确。因此，引入了一种基于UniDet目标检测模型的空间关系评价度量。首先使用UniDet来检测生成的图像中的对象具体位置，获得对应的边界盒位置。然后，比较两个边界盒的中心位置来确定两个物体之间的空间关系。将两个对象A、B中心坐标位置表示为  $(x_{1},y_{1})$  和 $(x_{2},y_{2})$  。如果  $x_{1} < x_{2}$  且  $|x_{1} - x_{2}| > |y_{1} - y_{2}|$ ，并且两个边界框之间的重叠区域（IoU）低于0.1的阈值，则“Object A on the left of Object B”。其他空间关系“right”、“top”和“bottom”也有类似评估。若不满足上述条件，其分数使用式（2.11）计算得到，其中  $IoU_{threshold} = 0.1$\n\n$$\ns c o r e = \\frac {I o U _ {\\text {t h r e s h o l d}}}{I o U _ {A B}} \\tag {2.11}\n$$\n\n此外，对于空间关系中的“near”、“next”、“on the side of”，则是计算两个边界盒的中心位置，根据阈值判断是否符合位置关系，若不满足条件，则利用式（2.12）计算得到，其中distanceThreshold为事先设定的阈值，本文中distanceThreshold=150。\n\n$$\n\\operatorname {s c o r e} = \\frac {\\text {d i s t a n c e} _ {\\text {t h r e s h o l d}}}{\\max  \\left(x _ {\\text {d i s t a n c e}}, y _ {\\text {d i s t a n c e}}\\right)} \\tag {2.12}\n$$\n\n针对非空间关系任务的评估，提出了一种基于CLIP Score的评估体系。这一体系的提出，源于当前在评估物体之间复杂且微妙的交互关系时，现有的许多模型都显得力不从心，难以准确捕捉并验证这些关系。而CLIP模型能够从文本和图像中抽取关键信息，并进行有效的匹配和比对，在一定程度上具备处理复杂文本和判断交互条件的能力。然而，尽管CLIP模型表现出色，但在处理某些极其\n\n复杂或细微的交互关系时，仍然难以避免较大的误差。这是因为现实世界中的物体交互关系往往错综复杂，超出了模型当前的学习和理解范畴。为了进一步提升评估的准确性和可靠性，在第三章基于注意力损失优化的组合生成算法的实验验证中，还特别添加了人类用户的评估环节。人类用户以其独特的感知能力和理解能力，能够捕捉到模型可能忽视的细节和微妙之处，从而实现对生成图像与提示文本之间匹配程度的更为全面和准确的判定。\n\n# 2.5 本章小结\n\n本章深入探讨了关于扩散模型的基本原理及经典模型、提升组合式生成能力的关键理论算法以及评估数据集和评估指标。提升扩散模型的组合式生成能力是一项基础性工程，影响了基于扩散模型的下游任务，包括图像编辑、个性化生成等多个关键领域。\n\n在理论方面，本章详细介绍了有关扩散模型的相关知识。首先是扩散模型的理论基础DDPM，以及在此基础上衍生得到的DDIM等加速采样或是条件增加控制模型。随后，介绍了基于扩散模型的文生图模型，包括当前最流行的开源模型Stable Diffusion和其他具有较强生成能力的模型，如DALL-E2。最后，介绍了多条件可控的文生图模型，如ControlNet、T2I-Adapter能够接受额外的条件，如布局图、关键点图等，实现更加可控的图像生成。\n\n在关键技术方面，本章深入探讨了提升扩散模型组合式生成能力的关键机制和算法。首先，详细介绍注意力机制的原理，表明注意力机制在文本与图像信息的重要作用。其次，介绍基于微调的组合式生成算法研究和基于即插即用的组合式生成算法研究，前者包括利用注意力机制、使用多模态视觉模型反馈等形式，后者则是利用注意力图获得损失优化潜在空间向量的多种实现方式。\n\n最后，详细介绍本文实验所使用的评估数据集和评估指标。T2I-Compbench内包含的多种任务形式的提示文本，能够更加全面地展现出当前的组合式生成任务存在问题。同时，针对不同的任务，使用不同的评估指标来精确的验证生成图像与提示文本的匹配程度。\n\n综上所述，本章通过对本文研究的理论基础、关键技术和评估数据集及指标进行了深入研究的探索，提供了全面而系统的知识体系，为该领域的研究和应用奠定了坚实的基础。\n\n# 第3章 基于交叉注意力损失优化的组合式文生图算法\n\n# 3.1 引言\n\n最近的研究工作表明，文生图扩散模型存在着的主要问题为模型实际生成图像内容与文本提示中表达的预期内容对齐有关。先前一些研究针对该问题给出量化数据[108]，以稳定扩散模型（Stable Diffusion, SD）为例的文生图模型，对于单个主体对象始终能够成功，并且单个实体存在的属性在现实世界中不常出现的属性也能够正确生成，如图3.1（a-c）所示。然而，它们很难同时生成两个对象，如图3.1（d-f）所示。当出现的两个物体不包含其对应属性时，出现部分实体泄露问题，如图3.1（d）所示，猫和狗的耳朵呈现相同的姿态。特别是在提示文本中给出现的对象添加相应的属性时，SD模型生成出错的情况增加，如图3.1（e-f）所示，当提示文本中出现训练集或是现实世界中较少出现的情况，SD模型生成错误的情况尤为突出。因此，当提示文本中包含两个及以上的对象且在每个对象均含有对应的属性条件，文生图模型难以生成与提示文本相符合的图像问题的存在极大地影响到了扩散模型在实际生活中的有效应用。\n\n![](images/07666e9b44c168e36cc16329f733a64df571776ae9756d91148c4be4a078c2ab.jpg)  \nA dog\n\n![](images/e5f7404215f749c05322f84d0d747654bc000049a56c2b943b61cd4ec7f271db.jpg)  \n(a)\n\n![](images/0ff9995e67825afcd1e672c2d0f711a6b79ef0e3c51ee0a9b7aa6542d826ef80.jpg)  \nA black dog\n\n![](images/412c4a720afd10c264849bd7dde475a215a6d9ae5a75d4d5e8596e587c7e6822.jpg)  \n(b)\n\n![](images/0f6bab585749cba123ae8efb73f7e8dbf415a0b1d421ee17293672e211a94fcc.jpg)  \nA blue dog\n\n![](images/5d4f904be3a1f2144a86132ecf960645145fa39ca88d20d0238ac389efbfd7ce.jpg)\n\n![](images/9712e4a4cc3cb0f55c214c51136029dbecaac48bbee8ea3ab1d43dc2aa9522e9.jpg)  \nA dog and a cat\n\n![](images/45b238c63b278b175f983eaac4490ffdaf03ca98370383ad99c9257c3b2373ea.jpg)  \n(d)\n\n![](images/76ae800ea708c19530d7a94d584e2dd1118c9f3d918dea5ad755fcf77e6bbdf1.jpg)  \nA black dog and a white cat\n\n![](images/4a824d5ea8d87626d27c54c0f0923d4b38cd53ce679f7b65fcdb381254f3376d.jpg)  \n(e)\n\n![](images/aefc8644784faf52e6686a3e2af5752cc53655f5b2adc964d8d60a67e9872707.jpg)  \n(f)  \n图3.1 SD模型在不同提示文本生成示例\n\n![](images/b4094a35721ec7128a4adc67687055e05f6c5cf4ce655ff7c8bb2dee3f4d5a1e.jpg)  \nA blue dog and a red cat\n\n将提示文本中出现多个属性-对象对的生成任务称之为组合式生成任务。在现实生活场景中，以组合式生成任务形式提示文本是更为经常出现的。因此，改善扩散模型组合式生成任务的必不可少的重要任务。本文的研究任务致力以无训练的方式来解决该问题，提高文生图扩散模型的可控性。\n\n先前的研究表明，交叉注意力机制在文本与图像特征的融合中发挥着至关重要的作用。在这些研究的基础上，观察发现，注意力图中注意力值分配不当可能导致某些类别无法生成，主要可以归纳为以下几种情况：\n\n![](images/99dbb49975192fb90e512fb6696446eb3ad4627259fc323a2b1cfa6b8546ea80.jpg)  \n图3.2 属性泄露示例交叉注意力图可视化\n\n首先，注意区域和边界缺乏明确的约束，常常导致注意力被错误地分配到不相关的位置，导致错误的属性绑定，如图3.2所示（提示文本为“a red cake and a blue suitcase”），属性“blue”从去噪初始阶段就没有和其对应的实体“suitcase”注意在相同的区域，而是在一些无关的区域，同时“cake”的对应属性“red”在初始阶段属性对应的区域就蔓延到了“suitcase”所在的区域，在这两种因素的导致下，最终导致属性泄露。或者是多个主体的注意力重叠范围过大，导致模型在最后无法分辨该区域到底要生成哪个物体，最终导致实体缺失或者是实体特征融合等问题出现，如图3.3所示（提示文本为“a yellow frog and a green fly”），两个实体“frog”和“fly”，两个实体的注意图的重叠区域几乎一致，但是从图中明显可以看出，“frog”的注意图的注意力值明显高于“fly”（注意力图中，颜色越接近于红色表明注意力值越高，越接近于蓝色表明注意力值越低），从而模型会更倾向于生成更高注意力值所表示的文本信息，最终导致实体泄露。因此，如果没有清晰的指导，注意力可能会偏离相关区域，影响模型生成的最终效果。\n\n其次，在交叉注意图中存在忽视初始注意值较低区域的倾向。这种现象源于模型认为低注意力值意味着该区域不重要。然而，这些区域可能仍然蕴含着关键信息，如图3.4所示（提示文本为“a blue bench and a green cake”），注意力图中“cake”的注意力在最初阶段存在较高注意力值区域，但是此时与实体\n\n“bench”的高注意值区域重叠，且被覆盖，因此，随后去噪过程中“cake”的注意力值一直未能出现高注意力值区域且分布分散，导致模型忽视了该语义的生成。\n\n![](images/f134ba79b64c5adeb6ddf16bf5b5f2b494fd88c617e2437057edbb370b11a758.jpg)\n\n![](images/0a310e35bec209ed651905d95b1c76e86a0968993cf2ec465b2af7ae87293671.jpg)  \n图3.3 实体泄露示例交叉注意力图可视化  \n图3.4 实体缺失示例交叉注意图可视化\n\n最后，在调整交叉注意图时，未能充分考虑名词和修饰语在语义上的不同重要性。先前的最大化最小注意力值的选取，是从所有需要关注的文本标记，包括名词和修饰词中选取最小的值来进行优化，但是从全局来看修饰词的注意力值普遍低于名词的注意力值，即在一个图像块的注意力的所有通道中，会将大部分注意力分配给名词的所在的通道，且分布较散，从而易导致生成错误，如图3.2-图3.4所示，提示文本中对应属性的交叉注意力图的值均比实体的交叉注意值要小，且分布更为分散，这可能是由于常见背景空间中都需要颜色属性的注意力支持，才能够生成更符合实际的图片。同时Attend-and-Excite提出属性绑定的错误会随\n\n着实体缺失的问题改善而改善。因此，要首先确保实体，即名词，具有较高的注意力值，保证物体的生成。如果这种差异化的重要性未得到足够重视，则可能会影响整体的注意力分配和特征融合效果。\n\n因此，基于上述分析，本文提出利用显式布局来重新分配交叉注意力的一种新的优化方法。首先，利用大型语言模型（Large Language Model, LLM）的代码链（Code of Code, CoC）提示来解释文本和生成对象布局。在此布局的基础上，引入了基于交叉注意力值的两个创新的损失函数来修改去噪过程中的潜在空间向量。具体来说，引入了面向图像补丁的交叉注意（Patch-oriented Cross-Attention, PCA）损失，它鼓励为布局中的每个标记分配至少一个图像补丁，同时强调关注提示中所有标记的图像补丁的高激活值。此外，作为补充，面向区域导向交叉注意（Region-oriented Cross-Attention, RCA）损失侧重于提高布局内的平均注意力，以增加区域内生成对象的概率。此外，PCA和RCA的损失都通过区分名词和修饰词的损失词来考虑名词和修饰词的不同意义。本章主要贡献总结如下：\n\n(1) 利用LLM作为布局生成器，并通过CoC提示进行精确的布局生成来提高其性能，约束对应标记文本的注意力在其指定的位置，减少出现泄露的概率。  \n(2) 引入两种新的损失函数，在基于生成的布局的扩散模型中重新分配交叉注意以此来优化潜在空间向量，使去噪方向朝更加符合提示文本语义的方向前进。  \n(3) 将本文方法与多个基线模型进行了综合的实验评估，同时将本文方法与其余基线模型进行结合，证明了它在增强生成图像和不同文本提示之间的对齐方面的优越性以及较强的鲁棒性。\n\n# 3.2 方案设计\n\n本章提出的基于注意力损失优化的组合式生成算法研究框架如图3.5所示。该算法为无训练模型框架，输入为单一的提示文本。首先，通过使用了COC提示链的大语言模型根据提示文本生成与之对应的布局。随后，在扩散模型去噪过程中的前 $\\tau$ 步，使用本章提出的两种基于交叉注意力图的优化损失函数，即PCA损失、RCA损失，对潜在空间向量 $z_{t}$ 进行优化。最终的优化损失函数根据阈值判断，选择名词损失或修饰词损失。通过这一过程，在柔性布局约束的基础上，引导相应的标记文本向量聚集在LLM给定的布局中，从而提升相应标记文本的生成概率，增强模型的组合式生成能力。同时，该框架可以无缝衔接到其他拥有注意力机制的扩散模型，有效地提升不同模型在组合式生成任务中的图文一致性。\n\n![](images/af8dfbfeea1f21e4cb98a9be975dc9d9309d7ce49512c9c035474104533ef090.jpg)  \n图3.5 基于注意力损失优化的组合式生成算法框架图\n\n# 3.2.1 基于大语言模型的布局生成\n\n从文本中生成图像需要一个具有强大推理能力的文本编码器，能够理解从文本线索中推断出的对象之间的复杂关系，但当前的文本编码器能力均无法正确实现该功能。因此，需要一些额外的控制条件，如布局、关键点等辅助表示提示文本中包含的正确语义信息。同时，本章提出的理念和方法也需要借助额外的辅助控制条件来帮助标记文本对应的注意力值区域在其符合文本语义的布局上。本文中采用布局条件作为额外输入的控制条件。\n\n先前的一些布局生成任务研究方法主要集中两个方面：一方面，利用更好的模型结构或参数设计来微调现有的布局生成器；另一方面，使用高质量数据集，端到端地训练新的文本到布局生成器来生成符合提示文本语义的布局。但是，这两种方法都涉及到微调或训练，需要耗费大量的时空开销，同时难以保证新模型能够满足所有场景下的布局需求，模型的泛化能力难以得到保证。当前鉴于LLM具有强大的推理和分析能力，展现出了巨大的优势，无需像传统方法一样，针对每一个场景任务训练一个适配的新模型，而是利用高质量示例样本以及代码提示链等优化创新机制，就能够实现不同任务场景中的正确结果。因此，通过在LLM中使用代码提示链实现从文本中预测布局是一种便捷且高效的方法，能够大大减少训练时间，并提升完成任务的准确性。\n\n代码链（Code of Code, CoC）是思想链（Code of Thought, CoT）的有效扩展，用于增强 LLM 的代码驱动推理。它的核心概念在于引导 LLM 在处理复杂任务时，将原本抽象、庞大的程序逻辑巧妙地拆解为一个个语义清晰、功能明确的子任务，并将这些子任务以灵活多变的伪代码形式进行构建。随后，在运行时，可以通过代码解释器和 LMulator（LLM 的代码模拟器）显式地识别和处理这些伪代码段。实时记录和分析执行结果。在实现过程中，预测布局之前，会手动向 LLM\n\n提供与布局生成相关的伪代码和注释。这一步旨在说明布局背后的规则和原则，从而协助LLM能够正确理解所要完成的任务。最后，LMulator根据注释提供的指导来执行代码，在此过程中不断地进行交互和反馈，从而预测出准确布局。\n\n![](images/13486c90c986cb59481eb03a718c72627f8f332b3e4d299147b94655eceb9fe8.jpg)  \n图3.6 CoC提示生成布局流程\n\n使用代码提示链，将布局生成任务根据 CoC 的逻辑逐步分解为四个递进式的子任务，每一个步骤都使用到前一个步骤获得的生成结果，最终得到与组合式生成任务文本提示对齐的布局。如图 3.6 所示，其具体步骤为：\n\n步骤1：利用LLM来根据输入的文本提示符来解析对象。\n\n步骤2：确定文本提示符所指示的对象的数量。\n\n步骤3：生成对每个对象的位置的描述。首先，基于文本提示符分析图像中描述的对象之间的空间关系。若没有明确说明，则推断出最有可能出现情况。然后指定图像中每个对象所定位的区域，如左、右等，以获得最大的特异性。\n\n步骤4：创建布局。考虑到对象的数量和位置描述，在图像中为每个对象生成一个连贯的布局。布局格式应为  $(x_{1},y_{1},x_{2},y_{2})$  ，其中  $(x_{1},y_{1})$  表示左上角坐标 $(x_{2},y_{2})$  表示右下角坐标。\n\n值得注意的是，生成的布局被定义为边界框，每个边界框对应于一个描述其内容的文本短语。例如，在短语“red dog”中，“red”和“dog”令牌共享相同的布局（边界框）。如图3.7所示，展示了一个使用CoC提示进行布局预测的推理结果的示例。\n\n# Prompt: A frog on the top of a cup\n\n```jsonl\nAttribute Dictionary:   \n{\"frog\":{ \"number\":1, \"location description\":\"The location relationship is mentioned; the frog is on top of the cup. Considering the cup as a base, the frog should be positioned above it. Since it's on top, it should be located in the upper part of the picture.\" } \"cup\": { \"number\":1, \"location description\":\"The cup is mentioned to be the base for the frog. It should occupy a significant portion of the lower part of the picture to provide a stable base for the frog on top.\" }   \nLayout Dictionary:   \n{\"frog-0\":(70,5,185,110), \"cup-0\":(25,110,230,256) }\n```\n\n![](images/76f69a2ffe338a8c4de99158fef8dbcbe88f5128c886317ece80aaa23353d67b.jpg)  \n图3.7 CoC提示布局预测示例\n\n# 3.2.2 基于交叉注意力的图文一致性损失\n\n在本节中，使用如图3.6所示的CoC提示的LLM获得的布局和经过式(2.10)高斯平滑函数的交叉注意力图上提出两个引导损失：面向补丁的交叉注意（PCA）损失和区域导向的交叉注意（RCA）损失，它们在去噪过程中在基于SD的交叉注意层的注意力分布来优化潜在空间向量。对于每个损失的作用，具体而言，PCA的丢失促进了在布局中的每个令牌中分配至少一个图像补丁，同时也强调了关注提示中所有令牌的图像补丁的高激活值。此外，作为补充，RCA损失侧重于提高布局内的平均注意力，以增加区域内生成对象的概率。在此同时，PCA和RCA的损失都通过区分名词和修饰词的损失词来解决名词和修饰词的不同重要性。\n\n图像补丁导向的交叉注意力（PCA）损失的设计含义具有双重强调作用，其一是鼓励在布局中的每个令牌上分配至少一个图像补丁，其二为强调提高关注提示符中所有令牌的图像补丁的激活值。这种强调是至关重要的，因为这些区域的\n\n低注意值可能会导致多实例生成挑战，如对象遗漏或属性泄漏。当检查像“two black dogs”这样的特定文本提示时，标记“dog”和标记“black”都与多个（两个）布局相关联。因此，PCA损失首先识别出在每个令牌的各自布局中具有最高注意值的补丁。其次，从提取的最高注意的补丁中选择最小注意值。最终，计算出所有令牌的这些最小注意值的总和。对于仅与一个布局相关联的令牌，将执行最大计算操作后获得的最小注意值直接添加到求和公式中。此外，在优化过程中，PCA损失区分了名词和修饰词的损失词，为名词量身定制的PCA损失主要用于确保对象的生成，为修饰词定制的PCA损失则是进一步确保正确属性的生成。\n\n针对名词量身定制的PCA损失主要用于确保对象的生成，定义如下：\n\n$$\nL _ {p} ^ {(n)} = 1 - \\frac {1}{k _ {n}} \\sum_ {i \\in G _ {n}} \\min  _ {q \\in B _ {i}} \\max  \\left(A _ {i} ^ {n} \\odot M _ {i} ^ {q}\\right) \\tag {3.1}\n$$\n\n其中， $G_{n}$  表示由  $k_{n}$  个名词令牌组成的集合， $B_{i}$  表示文本标记  $i \\in G_{n}$  的关联布局， $A_{i}^{n}$  表示在时间  $t$  上令牌  $i$  的交叉注意力映射， $M_{i}^{q}$  表示  $B_{i}$  中第  $q$  个布局的二进制掩码。在公式中，max 计算了交叉注意力图  $A_{i}^{n}$  的布局  $q$  内的最大图像补丁注意力值。\n\n针对修饰词量身定制的PCA损失用于优化与实体对象相关属性，定义如下：\n\n$$\nL _ {p} ^ {(m)} = 1 - \\frac {1}{k _ {m}} \\sum_ {j \\in G _ {m}} \\min  _ {q \\in B _ {j}} \\max  \\left(A _ {j} ^ {m} \\odot M _ {j} ^ {q}\\right) \\tag {3.2}\n$$\n\n其中， $G_{m}$  表示由  $k_{m}$  个修饰词标记组成的集合， $B_{j}$  表示文本标记  $j \\in G_{m}$  的关联布局， $A_{j}^{m}$  表示在时间  $t$  时令牌  $j$  的交叉注意力映射， $M_{j}^{q}$  表示  $B_{j}$  中第  $q$  个布局的二进制掩码。在公式中，max 计算了交叉注意力图  $A_{j}^{m}$  的布局  $q$  内最大图像补丁注意力值。\n\n区域导向的交叉注意（RCA）损失是为了解决当其相关布局中的某些主题标记在原始注意图中的最高注意补丁中表现出较低的初始值时，PCA 损失可能无法提高注意值，从而导致物体遗漏等问题。为了解决这一限制，提出了 RCA 损失来提高布局内的平均注意力。这一改进旨在提高在去噪过程中在区域内生成对象的可能性，并减轻对象遗漏问题。同样，RCA 损失在优化过程中区分了名词和修饰语的损失词。\n\n针对名词量身定制的RCA损失主要用于增强对象的生成，定义如下：\n\n$$\nL _ {r} ^ {(n)} = 1 - \\frac {1}{k _ {n}} \\sum_ {i \\in G _ {n}} \\min  _ {q \\in B _ {i}} \\operatorname {m e a n} \\left(A _ {i} ^ {n} \\odot M _ {i} ^ {q}\\right) \\tag {3.3}\n$$\n\n其中,  $G_{n}$  表示由  $k_{n}$  个名词令牌组成的集合,  $B_{i}$  表示文本标记  $i \\in G_{n}$  的关联布局,  $A_{i}^{n}$  表示在时间  $t$  上令牌  $i$  的交叉注意力映射,  $M_{i}^{q}$  表示  $B_{i}$  中第  $q$  个布局的二进制掩码, mean 是为了计算布局  $q$  内的平均注意力。\n\n针对修饰词量身定制的RCA损失用于改进与实体对象相关属性。定义如下：\n\n$$\nL _ {r} ^ {(m)} = 1 - \\frac {1}{k _ {m}} \\sum_ {j \\in G _ {m}} \\min  _ {q \\in B _ {j}} \\operatorname {m e a n} \\left(A _ {j} ^ {m} \\odot M _ {j} ^ {q}\\right) \\tag {3.4}\n$$\n\n其中,  $G_{m}$  表示由  $k_{m}$  个修饰词标记组成的集合,  $B_{j}$  表示文本标记  $j \\in G_{m}$  的关联布局,  $A_{j}^{m}$  表示在时间  $t$  时令牌  $j$  的交叉注意力映射,  $M_{j}^{q}$  表示  $B_{j}$  中第  $q$  个布局的二进制掩码, mean 是为了计算布局  $q$  内的平均注意力。\n\n用上文所提出的两种基于交叉注意力的损失来引导修改获得的损失引导修改采样过程中的某一区间范围内时间步上的预测噪声。具体而言，基于PCA和RCA损失，在优化过程中，首先最小化两个名词损失，以优化噪声样本  $z_{t}$ ，在早期去噪阶段优先进行对象生成。当两个名词损失不能满足阈值条件时，通过最小化两个修饰语损失来优化噪声样本  $z_{t}$ ，目的是细化与对象相关的属性。如果修饰语损失也不能满足阈值条件，恢复到最小化名词损失来优化噪声，并根据需要在两者之间交替进行。计算公式如下所示：\n\n$$\nz _ {t} - \\alpha_ {t} \\nabla_ {z _ {t}} L _ {\\text {t o t a l}} \\rightarrow \\hat {z} _ {t} \\tag {3.5}\n$$\n\n其中， $\\alpha$  表示在去噪过程中控制优化影响的步长， $L_{total}$  被定义为：\n\n$$\nL _ {t o t a l} = \\left\\{ \\begin{array}{l} L _ {p} ^ {(n)} + L _ {r} ^ {(n)}, \\left(L _ {p} ^ {(n)} \\geq 1 - \\gamma_ {p} ^ {(n)}\\right) \\vee \\left(L _ {r} ^ {(n)} \\geq 1 - \\gamma_ {r} ^ {(n)}\\right) \\\\ L _ {p} ^ {(m)} + L _ {r} ^ {(m)}, \\left(L _ {p} ^ {(m)} \\geq 1 - \\gamma_ {p} ^ {(m)}\\right) \\vee \\left(L _ {r} ^ {(m)} \\geq 1 - \\gamma_ {r} ^ {(m)}\\right) \\end{array} \\right. \\tag {3.6}\n$$\n\n其中， $\\gamma_p^{(n)}$  为  $L_{p}^{(n)}$  阈值， $\\gamma_r^{(n)}$  为  $L_{r}^{(n)}$  阈值， $\\gamma_p^{(m)}$  为  $L_{p}^{(m)}$  阈值， $\\gamma_r^{(m)}$  为  $L_{r}^{(m)}$  阈值。\n\n在该过程中，在不同时间步骤上的不同词性的设置了不同的阈值，其中名词阈值要高于修饰词阈值，时间步越大，阈值越高。这种设置阈值的原因在于，通过现有的一些研究以及实验结果表明，名词标记的注意区域相对集中，而修饰标记的注意区域则相对分散。并且，在进行阈值判断时，首先使用名词的阈值进行判断，当满足名词的阈值之后，在使用修饰词的阈值及逆行盘。总之，名词的阈值判断优先级总是高于修饰词的。同时，SD模型在去噪前期主要生成图像的结构，而在后期主要是用于增加优化图像的细节内容。因此，为名词损失分配一个较高的阈值，为修饰词损失分配一个较低的阈值，同时这些损失在前几个时间步中的阈值增长较快，而在其后增长较慢是为了位置保持先前生成图像结构减少变动，并且这种渐进式的更新策略可以防止潜在空间向量  $z_{t}$  变得不符合数据集分布，同时促进更加准确的生成。\n\n同时，在该过程中对当前时间步上的预测噪声进行优化之后，判断当前时间步是否在实现设置的迭代时间步集合中。当  $t \\in T_{iteration}$  时，判断在该时间步上的优化次数是否达到最大迭代优化次数  $\\tau$ ，若未达到则重新进行优化。\n\n$$\n\\hat {z} _ {t} \\rightarrow z _ {t} \\tag {3.7}\n$$\n\n当  $t \\notin T_{\\text {iteration}}$  或者是  $t \\in T_{\\text {iteration}}$  且在该时间步上达到最大迭代优化次数，则跳出优化环节，直接进行迭代去噪，直到获得最终潜在变量  $z_0$  。\n\n# 3.3 实验结果分析\n\n# 3.3.1 实验设置\n\n本章节通过其API使用GPT-3.5模型[109]从文本提示中生成边界框，提供了全面的指导和示例，包括诸如最大边界盒大小和期望的输出格式等细节，以促进优化布局的生成。使用官方的Stable Diffusion v2.1文生图模型，该模型采用CLIP ViT-L/14模型中的预训练文本编码器。使用线性调度率设置比例因子  $\\alpha_{t}$  该线性调度率从20开始并线性衰减到最小值10，使用  $s = 7.5$  来执行对潜在空间向量  $z_{t}$  的无分类器引导[110]。用于平滑交叉注意力图的高斯滤波器的内核大小为3,标准差为  $\\sigma = 0.5$  。用于获得优化损失的交叉注意力图的维度为  $16\\times 16$  所有的图像生成的去噪步骤均为50步，而应用PCA和RCA损失的时间步数  $t^{\\prime}$  设置为30步。进行迭代优化的时间步集合  $T_{iteration} = \\{0,5,10,20,30\\}$  ,在该集合内的时间步上的最大迭代次数设置为25，其余的时间步上的最大优化次数均为1。同时，为了确保公平的比较，本章所进行比较的模型均是无训练形式的提升组合式生成任务能力算法，使用的基模型都是Stable Diffusion v2.1。同时进行图像生成时，基线方法和本章方法都使用固定种子42，为每个文本提示生成10张图像。\n\n在先前分析中，观察到名词标记的注意区域往往相对集中，而修饰标记的注意区域则相对分散，建立了一个较高的名词损失阈值和一个较低的修饰标记损失阈值。此外，随着时间步长的增加，这些阈值也相应地增加。经过广泛的实验，确定了不同时间步长  $t$  的不同损失的阈值  $\\gamma_{p}^{(n)}, \\gamma_{r}^{(n)}, \\gamma_{p}^{(m)}, \\gamma_{r}^{(m)}$  ，具体如下所示：\n\n当  $t = 0: \\gamma_p^{(n)} = 0.15, \\gamma_r^{(n)} = 0.10, \\gamma_p^{(m)} = 0.08, \\gamma_r^{(m)} = 0.05$ ;\n\n当  $t = 5:\\gamma_p^{(n)} = 0.40,\\gamma_r^{(n)} = 0.15,\\gamma_p^{(m)} = 0.2,\\gamma_r^{(m)} = 0.08;$\n\n当  $t = 10: \\gamma_p^{(n)} = 0.60, \\gamma_r^{(n)} = 0.60, \\gamma_p^{(m)} = 0.30, \\gamma_r^{(m)} = 0.30$ ;\n\n当  $t = 20: \\gamma_p^{(n)} = 0.90, \\gamma_r^{(n)} = 0.70, \\gamma_p^{(m)} = 0.45, \\gamma_r^{(m)} = 0.35$ ;\n\n当  $t = 30: \\gamma_p^{(n)} = 0.95, \\gamma_r^{(n)} = 0.80, \\gamma_p^{(m)} = 0.48, \\gamma_r^{(m)} = 0.40$ ;\n\n其中， $\\gamma_p^{(n)}$  为  $L_{p}^{(n)}$  阈值， $\\gamma_r^{(n)}$  为  $L_{r}^{(n)}$  阈值， $\\gamma_p^{(m)}$  为  $L_{p}^{(m)}$  阈值， $\\gamma_r^{(m)}$  为  $L_{r}^{(m)}$  阈值。\n\n本小节将本章提出的算法框架与当前先进的无训练形式的提升组合式生成任务的算法框架进行了全面比较。基线方法包括：Stable Diffusion[3]（表格中表示为SD）、Composable Diffusion[73]（表格中表示为Composable）、Attend-and-Excite[54]（表格中表示为A&E）、Layout Diffusion[63]（表格中表示为LD）、Divide&Bind[55]（表格中表示为D&B）、Initno[72]、Conform[58]、SynGen[46]。上述的无训练方法中包含了当前主流的以无训练方式提升扩散模型组合式生成方式的大部分类型。\n\n# 3.3.2 定量评估\n\n在本章节中，对与八种基线方法进行了详尽的比较，所使用的基模型为最新的Stable Diffusion v2.1版本。为了更直观展示本章提出方法与基线方法之间的定量比较结果，在表1中进行了详细的列示。通过对表1的深入分析，提炼出了一些关键见解，阐述了本章所提出方法具备的有效性及其在特定任务中的优势。\n\n表 1 基于注意力机制损失优化的组合式生成算法 (SD v2.1) 定量结果  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>BLIP-VQA↑</td></tr><tr><td>SD</td><td>0.5450</td><td>0.4348</td><td>0.5103</td><td>0.1565</td><td>0.3116</td><td>0.3576</td></tr><tr><td>Composable</td><td>0.4837</td><td>0.4057</td><td>0.4732</td><td>0.1553</td><td>0.3093</td><td>0.3934</td></tr><tr><td>A&amp;E</td><td>0.5311</td><td>0.4288</td><td>0.5443</td><td>0.1617</td><td>0.3099</td><td>0.3548</td></tr><tr><td>SynGen</td><td>0.7438</td><td>0.5358</td><td>0.6855</td><td>0.1656</td><td>0.3095</td><td>0.4384</td></tr><tr><td>LD</td><td>0.4526</td><td>0.4138</td><td>0.4511</td><td>0.1580</td><td>0.3034</td><td>0.3797</td></tr><tr><td>D&amp; B</td><td>0.4874</td><td>0.4070</td><td>0.4774</td><td>0.1526</td><td>0.3087</td><td>0.3993</td></tr><tr><td>Conform</td><td>0.7494</td><td>0.5164</td><td>0.6939</td><td>0.1852</td><td>0.3095</td><td>0.4539</td></tr><tr><td>Initno</td><td>0.4747</td><td>0.3975</td><td>0.4630</td><td>0.1428</td><td>0.3050</td><td>0.3866</td></tr><tr><td>Our</td><td>0.6451</td><td>0.4999</td><td>0.6703</td><td>0.1631</td><td>0.3087</td><td>0.4319</td></tr><tr><td>A&amp;E+PCA</td><td>0.6014</td><td>0.4931</td><td>0.6347</td><td>0.1505</td><td>0.3060</td><td>0.4258</td></tr><tr><td>A&amp;E+ RCA</td><td>0.6172</td><td>0.4920</td><td>0.6527</td><td>0.1505</td><td>0.3072</td><td>0.4176</td></tr><tr><td>A&amp;E + Our</td><td>0.6320</td><td>0.5074</td><td>0.6558</td><td>0.1527</td><td>0.3065</td><td>0.4169</td></tr><tr><td>Conform+Our</td><td>0.7630</td><td>0.5368</td><td>0.6480</td><td>0.1514</td><td>0.3086</td><td>0.4587</td></tr><tr><td>SynGen+Our</td><td>0.7743</td><td>0.5789</td><td>0.7212</td><td>0.1527</td><td>0.3074</td><td>0.4379</td></tr></table>\n\n首先，在比较的八种基线方法中，本章提出的方法在属性绑定任务上的表现显著优于其他方法。这一点尤其重要，因为属性绑定任务是能够评估文生图模型在处理实体缺失、实体泄露、属性互换和属性泄露等复杂任务中的能力的关键指标。尽管本章的方法在CLIP分数上未能达到最高水平，但这可能是CLIP分数可能并不足以全面捕捉语义信息中存在的交互关系。早期研究早已强调[111]-[112]，尽管CLIP分数提供了一个强大的平均语义表示，但在复杂提示文本的任务中，CLIP分数往往无法准确评估属性与对象之间的绑定关系。同时，通过实验数据可以观察到大部分提升组合式生成的无训练算法在准确生成空间位置方面存在一定的问题，这可能与SD模型无法理解提示文本中的位置关系所导致的，仅仅\n\n是通过简单的注意力损失函数的柔性引导，无法真正帮助模型理解到正确的空间关系。Conform在空间位置上的引导表现较为突出的原因可能是其利用了结合跨时间步的注意力图引导，利用多个时间步的注意力分布信息，帮助注意力图在一定时间步上的分布信息结构一致，这可能是促进空间结构信息正确的重要因素。\n\n其次，为进一步验证本章提出的PCA损失和RCA损失的泛化能力，故在本节中评估了所提出插件的有效性。具体而言，本小节比较了在使用和不使用这些损失时，现有模型的性能表现。为了实现这一目标，本小节将PCA和RCA损失整合到几种基线模型中，包括Attend-and-Excite、Conform和SynGen。\n\n研究结果表明，这种整合显著改善了这些基线方法在属性绑定任务上的表现，显示了本章方法的有效性，同时也证明在本章方法与SynGen等利用对抗损失增强属性-实体对之间的注意力图相似性这个方法上具有较强的互补作用，能够进一步提升属性绑定的任务性能。然而，值得注意的是，当同时合并这两种损失时，通过定量数据观察到基线模型在空间关系处理上的表现有所下降。这可能是由于在处理空间任务时，本章的方法与基线方法之间存在某种程度的损失冲突。这一发现提示在设计损失函数时，需要更加细致地考虑不同损失函数设计的具体解决目标以及相互之间的影响。此外，本小节还进行了两个消融实验，以评估PCA和RCA损失在组合式生成任务中的影响。在这两个实验中，A&E + PCA表示将PCA损失集成到Attend-and-Excite的原始损失中，而A&E + RCA则指的是合并RCA损失。实验结果显示，这两种损失的引入显著提升了Attend-and-Excite在属性绑定任务中的性能。这些结果不仅验证了本章提出方法的有效性，还突显了其在泛化能力上的优势，表明本章方法面对多样化生成任务，能保持较高性能。\n\n表 2 Non-Spatial Relationship 的用户评估  \n\n<table><tr><td>Method</td><td>Non-Spatial Relationships</td><td>Method</td><td>Non-Spatial Relationships</td></tr><tr><td>Stable Diffusion</td><td>73.9%</td><td>Conform</td><td>74.9%</td></tr><tr><td>Composable</td><td>78.0%</td><td>GLIGEN</td><td>81.6%</td></tr><tr><td>A&amp;E</td><td>79.9%</td><td>TokenCompose</td><td>60.2%</td></tr><tr><td>Divide&amp;Bind</td><td>76.0%</td><td>Initno</td><td>80.8%</td></tr><tr><td>SynGen</td><td>76.6%</td><td>LayoutDiffusion</td><td>75.2%</td></tr></table>\n\n为了进一步证明本文方法在对象非空间任务上的有效性，解决CLIP分数在复杂多实体提示文本中不足以全面捕捉语义信息中存在的交互关系，导致无法准确评估属性与对象之间的绑定关系。本小节还进行了一项涉及20名参与者的用户研究，旨在全面了解本章提出的方法相对于其他基线方法的性能。在这项研究中，每个参与者都要求随机选取的25个提示，随后选取每个模型生成10张图片中的其中一张，在本章提出的方法和某个基线模型生成两两比较，选取最符合提\n\n示文本的语义的其中一张。这种两两比较的方法，能够有效地捕捉参与者对不同模型生成结果的直观感受。实验结果如表2显示，与基线方法相比，本章提出的方法在文本语义相似度上表现出显著的优势。具体而言，参与者更倾向于选择本章方法生成的图片，认为其更符合提示文本的语义。这一结果不仅反映了本章方法在捕捉复杂语义关系方面的能力，表明了其在多实体提示文本处理中的优越性。此外，这项用户研究的结果还表明，参与者在评估图片时，往往会考虑图片中各个元素之间的关系和相互作用，而不仅仅是单个对象的特征。这进一步强调了在设计视觉生成模型时，考虑到语义层面的交互关系的重要性。\n\n总结来看，本章提出的方法在多个方面展现了其优越性。首先，在处理属性绑定任务时，相较于基线方法，提出的方法能够显著提升模型的性能。这一发现为今后的研究提供了新的视角，表明针对特定任务设计的损失函数能够有效改善模型的表现。其次，通过对PCA和RCA损失的有效整合，揭示了在复杂生成任务中，如何通过优化损失函数来提升模型的泛化能力。此外，消融实验的结果进一步支持了本文的方法论，证明了在组合式生成任务中，适当的损失函数设计能够带来显著的性能提升。\n\n# 3.3.3 定性评估\n\n图3.8展示了本小节对基线方法与本章所提出的方法进行了定性比较。经过分析，Conform和SynGen在应对多样化文本输入时展现了较强的适应性，能够有效理解并生成符合上下文的回应，并在定性和定量评估上均表现出竞争力，取得了令人满意的结果。然而，其他基线方法在处理不同文本提示时常常面临诸多挑战，表现出明显的不足，出现各种各样的问题。如图3.8第一行，提示文本为“a black jacket and a brown hat”所示，SD、Composable、Attn-Exct、Innito生成的图片为棕色的夹克衫和黑色的帽子，出现了明显的属性互换问题，Layout-Diffusion生成了黑色的夹克衫和黑色的帽子，出现黑色属性泄露问题，Innito则是只生成了黑色的夹克衫，而未生成帽子，出现实体缺失问题。如图3.8第五行所示（提示文本为“The woodcarver is creating a sculpture of a bird from a block of wood”），大部分模型都无法捕捉到复杂提示文本中的全部语义信息，而本章提出的方法，能够较好的捕捉到提示文本中所有信息，并对其进行较为准确地生成。\n\n综上所述，通过对几个具体案例进行深入分析，详细展示了本章提出的方法与基线方法之间在性能上的显著差异。这些案例涵盖了不同的应用场景，旨在全面评估新方法的有效性。分析结果表明，本章提出的方法在多个维度上表现出更优越的综合性能。\n\n![](images/0ca58cb00c8139c15d63de8a5aa50bc138509e3c09acf7fa0bca2ae08a3ed5c6.jpg)  \n图3.8基于注意力损失优化的组合式生成算法定性结果\n\n本小节通过比较Attend-and-Excite的交叉注意力图与本章提出的方法来说明属性互换问题的有效解决，如图3.9所示。对于提示“a blue spoon and a sliver plate”，Attend-and-Excite的属性“blue”和“sliver”在去噪的初始阶段，分布分散，没有准确的位置约束其应在哪个位置进行加强，导致随着去噪时间步的推移，其去噪方向会跟随模型先验方向趋势，即现实生活中最经常出现的为“sliver\n\nspoon”。而在本章节的方法中，“blue”和“sliver”在去噪的早期，即第5步，就交叉注意力图就已经出现了明显的分布格局，即“plate”和“spoon”的形状，最随后的去噪步骤中，交叉注意力分布则一直保持该布局，最终实现符合文本语义的正确图像生成。\n\n![](images/07b17dea609ee3d0e97eea39c7b4be4bcd540a1aa157b0f0de09d18a83868b24.jpg)  \n图3.9Attend-and-Exite和本章方法在属性互换示例上的交叉注意里图可视化\n\n此外，本小节对比了由大型语言模型（LLM）直接生成的布局与使用了 CoC 提示生成的布局之间的差异。如图 3.10 所示，其左侧展示了在没有 CoC 提示的情况下生成的布局和相应的图像，而右侧则展示了应用 CoC 提示后生成的图像。通过这次比较，可以发现，缺乏 CoC 提示的布局在空间关系上可能存在不准确性。这种情况可能导致不同对象之间的相对位置和整体布局不符合逻辑，影响最终生成效果。相反，应用 CoC 提示后生成的布局则确保了对象之间的空间关系更加准确和合理。同时，也注意到了在没有 CoC 提示的生成过程中，某些对象可能会被遗漏。例如，在第二行的生成布局中，缺少了重要的元素——礼服，而\n\n这一元素在应用了 CoC 提示后得以完整呈现。这表明，CoC 提示在生成布局时起到了关键作用，不仅提高了空间关系的准确性，也确保了提示文本中所有重要组件都存在其对应的布局信息，从而增强了输入控制条件的准确性，确保不因为布局信息而导致后续生成过程中出现误差。\n\n![](images/c2e53e6d266cd8063f17d3341391cd23f98ac9fd64cf04f0d73900a1c9008ee3.jpg)\n\n![](images/951f2fbcbeb28716e43c7ce0d906dbf79acb614d2c00ac0f0f7ba2b039d9e0ce.jpg)\n\n![](images/94894a4c915f3803d67071cfde308d1c0c7649111f21b3da9c58d37e62f066e3.jpg)\n\n![](images/15495bf8ccd8602f768a7c5d747b77f09ecf1d19705732003c3539a310026063.jpg)\n\n![](images/9a3412bca7c9c58b83efd197a7450804b605cd299fefeac1b1dc5844bb3c744f.jpg)  \nA person is wearing a graduation cap and gown and receiving their diploma\n\n![](images/10edb7263b9ae7349ebbc359207927db47bbcf0d5fcb5306e6908134623cb52c.jpg)  \nAn airplane on the right of a girl\n\n![](images/feca534fa383a3b23b92c7836f6a89d891de374fe120fac9285ca42e185293ba.jpg)  \n图3.10 CoC提示定性结果示例\n\n![](images/de5e12c16261b55722e54a7f6a3c61f4033556ca03a8532aed85d30c108a4cc1.jpg)\n\n# 3.3.4 消融实验\n\n本小节通过一系列消融实验来验证所提出的PCA损失和RCA损失的有效性，具体实验设计如下：（1）w/o PCA Loss：该模型变体在优化过程中排除了PCA损失。（2）w/o RCA Loss：此模型变体排除了RCA损失。（3）w/o Distinguished：该模型变体不再区分PCA损失和RCA损失中的名词与修饰词的损失项。（4）+ Self-attention：这一变体在优化过程中同时考虑了交叉注意损失和自我注意损失。具体而言，自我注意损失的构成与交叉注意损失类似。（5）w/o CoC：该模型变体不包含CoC提示。\n\n表 3 基于注意力损失优化的组合式生成算法消融实验定量结果  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>BLIP-VQA↑</td></tr><tr><td>w/o PCA Loss</td><td>0.6256</td><td>0.4609</td><td>0.6439</td><td>0.1496</td><td>0.3083</td><td>0.4147</td></tr><tr><td>w/o RCA Loss</td><td>0.6035</td><td>0.4922</td><td>0.6513</td><td>0.1497</td><td>0.3084</td><td>0.4299</td></tr><tr><td>w/o Distinguished</td><td>0.6256</td><td>0.4914</td><td>0.6461</td><td>0.1463</td><td>0.3074</td><td>0.4226</td></tr><tr><td>+ Self-attention</td><td>0.6397</td><td>0.4908</td><td>0.6686</td><td>0.1496</td><td>0.3074</td><td>0.4199</td></tr><tr><td>w/o CoC</td><td>0.6533</td><td>0.4977</td><td>0.6749</td><td>0.1504</td><td>0.3067</td><td>0.4238</td></tr><tr><td>Our</td><td>0.6451</td><td>0.4999</td><td>0.6703</td><td>0.1631</td><td>0.3087</td><td>0.4319</td></tr></table>\n\n定量评价结果如表3所示。通过分析表明，本章提出的模型在多数子任务中的性能超越了所有其他模型变体。从这些发现中，可以得出几个结论：（1）PCA损失和RCA损失都在不同程度上有助于提高性能。（2）PCA和RCA损失中名词损失词和修饰词的区别也有助于提高性能。（3）在本章提出的优化损失的基础上添加自注意力损失，自注意力损失函数无法产能有效地性能提升。（4）虽然CoC提示并没有显著提高属性绑定性能，但它确实增强了空间能力。\n\n![](images/65dd4b3d0be8b660f872784ccbfcda3a58c7f2dbdc55a7d71fe6bb44d2fa09c8.jpg)  \n图3.11基于注意力损失优化的组合式生成算法消融实验定性结果\n\n图3.11所示的消融实验的定性评估结果突出了本章的各项目方法在处理属性绑定、实体有效生成、生成正确的空间布局和捕获交互动态方面的有效性，超过了所有其他模型变体。值得注意的是，本章提出的方法准确地产生了三角形桌布（如图3.11第二行，提示文本为“a circle dining table and a triangular table runner”所示）和塑料容器（如图3.11第三行，提示文本为“a plastic container and a glass windows”所示)等难以生成等物体，而其他模型变体要不是没有生成正确的对象，即出现属性泄露、属性互换情况（如+ Self-attention所示），或者是没有生成相应的对象，出现实体缺失情况（如w/o PCA损失、w/o RCA损失所示）。\n\n表 4 基于注意力机制的组合式生成算法时间步消融实验定量结果展示  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>3-in-1 ↑</td></tr><tr><td>t&#x27; = 20</td><td>0.6348</td><td>0.5009</td><td>0.6468</td><td>0.1496</td><td>0.3076</td><td>0.4183</td></tr><tr><td>t&#x27; = 50</td><td>0.6568</td><td>0.5002</td><td>0.6692</td><td>0.1496</td><td>0.3065</td><td>0.4225</td></tr><tr><td>Our (t&#x27; = 30)</td><td>0.6451</td><td>0.4999</td><td>0.6703</td><td>0.1631</td><td>0.3087</td><td>0.4319</td></tr></table>\n\n此外，本小节还进行了关于优化时间步的消融实验，以评估在应用PCA和RCA损失进行图像生成时，优化时间步骤数  $t'$  对图像生成准确性和图像生成质量存在一定影响。选择  $t' = 20, t' = 30, t' = 50$  进行定性定量结果比较。如表4展示了定量结果，从定量数据中可以看出本章所选择的优化时间步数在大部分的子任务中都表现出了最优性能。对于Color Binding任务中表现性能低于  $t' = 50$  的原因可能在于图像中颜色属性分布在前景和后景中，且在去噪后期中生成细化。在去噪后期模型未能够对其进行约束，导致颜色属性倾向于按照模型的先验数据分布方向及进行去噪，最终导致生成错误的情况出现。\n\n$$\nt ^ {\\prime} = 5 0\n$$\n\n$$\nt ^ {\\prime} = 2 0\n$$\n\n$$\nt ^ {\\prime} = 3 0\n$$\n\n![](images/624742b38b0a8076beaa5a13a420dfe6e446ef818f2a1fb79af104fa00418eac.jpg)\n\n![](images/d53879317a9c5bb695c669bf5b15d9b719d9e990a5544546ab0bad90176a789d.jpg)\n\n![](images/b762857501fb1768a24fcfc4cc4b549fdb83995ad92159ecda88a9773189b29d.jpg)  \nA person is looking at a flock of birds flying in formation\n\n![](images/fefcc423a5d7a0099601da03d26583ef525f007e8e62c45672843d4afdb808d7.jpg)\n\n![](images/db8baabe39b0fd7e233e950e3161558459ad82653902dfc784dadbe35393d73f.jpg)\n\n![](images/621b0bed04e6bad7259e85dfb8b6dbc68e8b81796310f15f50f1217c26e63a58.jpg)  \nA woman is speaking in sign language and communicating with a deaf friend\n\n![](images/f9d13160cfe262040f405f89c395109cef80496f1558b4ab4d0fab7273f3f2ae.jpg)\n\n![](images/52693a6f2b694effc81ab3cc3b01a87bbcd6117cedd7b26ebbe07799d1ccdde4.jpg)\n\n![](images/3de1ea998cb688c3e7fe8d84f503cc48c631ca37cbecd918fcf81cae2c13edbc.jpg)  \nA woman in a white shirt and jeans holds a pink umbrella in the rain\n\n![](images/8fede567b80d890058788d8c300751662ef99a5e2b4c3c456b79b533d78f940c.jpg)\n\n![](images/3d7f2a24cd44362a6f5830e6f25a363203614552fcec67371403f4d1a7596afd.jpg)\n\n![](images/0fa3e7cf46513f6beb2f54e771f47af53e0da42f2d3f986507152f3195a2c8bb.jpg)  \nBathroom scene with white background and tan accents  \n图3.12基于注意力机制的组合式生成算法时间步消融实验定性结果示例\n\n优化时间步消融实验定性结果如图 3.12 所示。在对定性结果进行观察后，发现生成的图像在  $t' = 20$  时间步长上缺乏与文本提示符的语义的对齐，这可能是由于在去噪过程中的时间优化不够。如图 3.9 所示，Attend-and-Excite 就是使用  $t' = 20$  的优化步骤，其对应的标记文本的交叉注意力图分布的格局还未能完\n\n全呈现, 处于一种分散的状态, 使得模型无法正确分辨出不同的图像块所对应生成的标记文本, 最终导致模型生成错误或优化后不正确。此外, 在  $t^{\\prime} = 50$  时间步长中观察到图像质量明显下降, 这可能是由于对生成过程约束过强, 导致在去噪过程的后期模型无法利用先验知识对图像背景以及图像和谐性进行有效的细节优化。相反, 使用  $t^{\\prime} = 30$  时间步长, 可以获得综合效果最佳的生成图像。\n\n# 3.3.5 算法局限性\n\n本小节主要说明本章所提出的方法的局限性，同时如图3.13所示，给出经过本文算法框架生成的错误示例：\n\n（1）软空间约束。在本章提出的方法中，布局指导只是作为一个粗略的定位指示，没有作为一个额外的控制条件输入注意力机制或者是利用额外的Adapter机制抽取特征融入到去噪过程中。因此，所生成的图像可能不完全符合所提供的布局，引导注意力分布的过程中，注意力会溢出指定的布局。此外，在某些情况下，初始随机噪声不能有效地将特定令牌的信息引导到所期望的布局，从而导致生成的图像的位置和布局之间的显著差异。基于上述两种原因，如表1所示，本章提出方法在空间位置关系任务中实现显著的性能提升。\n\n![](images/de17c989701d528f08d1c88255fd29dd05dabf9a4812afc56618c99b9d5d3218.jpg)  \n(a) Soft Spatial Constraint\n\n(b) Complex Scene Generation  \n![](images/203e61833f527602ae0ff2f86b230768dc86a1acc46ed99c946498c2a1c9e16c.jpg)  \nA bed in the bedroom with a thick quilt, a wardrobe and a desk next to it. There is a computer on the desk and a picture on the wall\n\n![](images/d9d30e1478c000e7b13f8e883041b9b738f1b8de1ddd895f54553801b8f4297f.jpg)  \nA courtyard is in front of a house, with a big tree in the courtyard. A swing is hanging from the tree, and there are two children nearby\n\n(c) Human Image Generation  \n![](images/7cc00e15a47e534f2221b5ada8da7b0b7376f4bbe41fc4c1f88b82e19393c387.jpg)  \nA child is playing with a jump ball and bouncing it up and down\n\n图3.13基于注意力损失优化的组合式生成算法失败案例示例  \n![](images/a45d96a7502afb3b3aba4017ca0e2b303c5f55cff241a5a04e7e4d41eaa5c514.jpg)  \nA woman is holding a basket of laundry and heading to the washing machine\n\n(2) 复杂的场景生成。本章节方法在简单的文本提示下表现得很好，但在优化\n\n具有多个对象和关系的复杂场景时却很困难。例如，当提示文本出现像“A bed in the bedroom with a thick quilt, a wardrobe and a desk next to it, There is a computer on the desk and a picture on the wall” 复杂文本提示，本章提出的方法会生成不符合文本语义的图像，这可能是由于限制原始 SD 模型的能力有限，无法有效地生成复杂的场景，而基于无训练的优化算法模型在一定程度上都是依赖于基模型的生成性能。\n\n(3) 人物图像生成。当文本提示涉及动态动作或人脸和身体的生成时, 人们经常观察到姿势和面部特征出现扭曲。这些问题主要是由于在描绘人类方面,原始 SD 模型在人物图像生成方面能力有限。\n\n# 3.4本章小结\n\n本章介绍了一种无训练基于交叉注意力损失优化的组合式图像生成算法研究，目的在于通过用户单一提示文本条件输入实现生成与文本语义高度一致的图像。首先，利用大语言模型，使用 CoC 提示生成提示中所有标记文本对应的布局。随后，在扩散模型去噪的每一步，获取转换、高斯平滑后的交叉注意力图。利用获取的交叉注意力图借助式（3.1）-（3.5）计算基于交叉注意力的图文一致性优化损失，并通过式（3.6）的阈值判断使用名词还是修饰词的优化损失对潜在空间向量进行优化。在去噪时间步处于迭代优化时间步集合中时，根据每一时间步上的最大迭代优化次数和优化阈值进行判断，并使用式（3.7）实现迭代优化。在设定的优化步骤后，按照常规方式去噪，使用解码器对潜在空间向量进行解码，获得最终的生成图片。\n\n本章节提出的方法在属性绑定任务上的表现尤为突出，在对象位置关系和复杂文本上的性能也有一定程度的提升，能有效地捕捉到提示文本中的多对象属性之间的相互关系。同时，本章提出的方法具有较强的可泛化性，能够与当前先进的组合式生成任务算法进行融合，并提升其性能。\n\n# 第4章 基于编辑修正的多条件组合式文生图算法\n\n# 4.1 引言\n\n当前，随着大规模文生图模型技术的迅猛进步，可控图像生成领域已成为学术界和工业界共同瞩目的研究热点。这一领域旨在通过先进的算法，使生成的图像不仅符合用户提供的文本描述，还能在内容、风格乃至空间布局上实现精准控制。早先，有关提升组合式图像生成能力的无训练研究在应对属性绑定任务时，已经展现出了令人瞩目的成效，成功地将文本描述中的多种属性精确地映射到生成的图像中。然而，在追求更高层次的图像生成控制——即实现正确的空间位置约束方面，这些无训练的方法却未能充分展现其潜力。通过深入剖析，这一局限性可能源自多个方面的复杂因素：\n\n首先，扩散模型中所采用的CLIP文本编码器在处理文本时，对于蕴含空间位置关系的表述（如“on the left of”，即“在……的左边”）的理解能力存在局限。当这些富含空间信息的文本向量被引入交叉注意力机制时，模型难以准确地将文本特征与图像特征进行高效融合，进而在图像中构建出符合文本描述的空间位置布局。更为具体的是，如图4.1所示（提示文本为“a bird on the right of a cup”），模型往往倾向于优先生成文本提示中首先提及的主体，并围绕这一主体进行有限的、可能并不完全准确的布局，从而忽略了文本中更为复杂和细致的空间关系。在该图中，可以看出在最初的交叉注意力分配时，“bird”和“cup”的布局形式就是上下布局的，而不是与提示文本一致的左右布局。\n\n![](images/fa7e220c492a65fb9bc8307fd0f410382a448eb878937e8dacd401d93d86c4f9.jpg)  \n图4.1 SD在对象空间位置关系生成错误示例的交叉注意力图\n\n其次，无训练优化算法的性能在很大程度上依赖于其基础模型的生成能力。若算法所使用的基模型本身在构建空间位置关系时就存在不足，那么即便采用无\n\n训练的算法优化框架，也很难在此基础上实现显著的性能提升。这是因为无训练算法更多地是在现有生成结果的基础上进行微调，而非从根本上改变模型的生成逻辑和机制。同时，本文第三章提出的无训练的基于交叉注意力损失优化的组合式生成算法在实现方位空间约束维度上效果不佳，如图4.2所示。这一定程度上源自于模型自身的局限，另一方面可能是该方法引入的布局约束仅仅是一种软性约束，期望交叉注意力值汇聚到布局中，但最终交叉注意力可能溢出布局范围，也可能与布局分布不一致，最终导致生成错误。\n\n![](images/b428447baba565e32acfa61491e06e2fb09d7c28bbe7d183b50c0a13a6e19764.jpg)  \n图4.2基于交叉注意力优化损失的组合式生成算法空间位生成错误示例的交叉注意力图\n\n当前，在探索如何提升空间布局可控性的图像生成模型的研究中，引入了空间布局条件，旨在使生成的图像不仅在内容上与给定的文本描述高度匹配，而且在空间布局上也严格遵循文本指定的结构。这些先进的方法可以归结为以下两类：第一类方法的核心在于添加额外的机制，提取布局特征，并其融入到注意力机制中。这一策略目的让模型在生成图像的过程中，能够更加敏锐地捕捉到文本描述中的空间位置信息，从而生成布局合理、结构清晰的图像。第二类方法则是利用布局信息对模型在去噪过程中的自注意力和交叉注意力进行精细的掩膜操作。它能够有效地减少标记文本之间不必要的注意力交互，从而避免信息的混淆与干扰，确保空间位置信息的正确性得到严格控制。此外，自注意力机制在自然语言处理和计算机视觉领域的应用越来越受到关注，尤其是在处理包含语义结构和空间结构信息的任务时。先前的研究[113]指出，自注意力机制不仅能捕捉文本中的语义关系，还能有效地结合图像中的空间信息特征。\n\n在深入探索无训练形式的提升扩散模型组合式生成能力的研究，针对提升组合式生成的空间交互能力的研究内容下，本章提出了一系列针对性的见解与策略。首先，本章借鉴并采用了当前先进的可控式多模态文生图模型作为无训练算法的基础框架，利用可控多模态模型在捕捉和生成空间信息的出色能力。其次，本章提出了两种基于注意力机制的损失优化函数：基于自注意力机制的结构增强损失\n\n和基于交叉注意力的结构相似度损失。这些损失函数旨在通过强化图像块之间的关联程度信息，来加强图像的结构信息和空间布局信息。最后，本章引入基于CLIP相似度检测的图像编辑修正模块，旨在通过CLIP相似度检测判断该图像生成是否符合提示文本信息，若不符合则使用图像编辑方式对生成图像进行修正，使生成图像更加符合提示文本语义信息。综上所示，本章的主要贡献在于：\n\n(1) 探讨了可控多模态文本生成图像模型与传统文本生成图像模型在生成与提示文本相符的空间信息能力方面的差异。选择了当前先进的可控多模态文本生成图像模型作为本章无训练算法研究框架的基础模型。  \n(2) 为了进一步提升模型生成精准空间信息的能力，本章提出了两种基于注意力机制的结构增强损失：基于自注意力的结构增强损失和基于交叉注意力的结构相似度损失，从而提高空间信息的准确性和一致性。  \n(3) 提出了一种基于编辑修正的迭代生成模块，以解决因优化损失约束误差累积的交叉注意力分配不正确导致的生成图像与提示文本语义不一致问题。\n\n# 4.2 多模态可控文生图模型分析\n\n在当前文本生成图像领域，多模态可控生成模型正逐步展现出其无与伦比的创造力和精确性，其中，GLIGEN与ControlNet等前沿模型尤为引人注目，它们凭借卓越的空间布局控制能力，引领技术发展的新潮流。本小节将聚焦于GLIGEN模型，以其作为多模态可控生成领域的杰出代表，深入剖析其在空间布局控制方面的显著优势、在内容生成上存在的局限，以及将其作为基础性模型进行进一步探索与实践的可行性。\n\n![](images/c9f88e6330355ebd4306a08165718a7c8dadd905d49dfb9df2b32cb371d7a23d.jpg)  \n图4.3 GLIGEN在空间位置关系任务生成正确示例的交叉注意力图\n\n章节2.1.2中已对GLIGEN模型的具体框架结构与核心机制进行了详尽阐述。简而言之，GLIGEN的创新之处在于巧妙地在交叉注意力层和自注意力层中嵌入了一个可训练的门控子注意力层。这一设计使得布局信息与对应的文本信息得以\n\n深度融合，并作为新的条件输入至模型中，从而极大地强化了模型在空间方位上的约束能力。这一技术为实现精准布局控制奠定了坚实基础。如图 4.3 所示，为 GLIGEN 模型在进行组合式生成任务时的交叉注意力图可视化，可从图中注意力分布情况来看，初期阶段，注意力分布已经严格按照预设的布局进行有序排列。这一特点与 SD（如图 4.4 所示）等传统模型形成了鲜明对比，后者在生成过程中往往呈现出注意力分布的分散状态。\n\n![](images/9890c16b171612561b0bf0c0a38a19e22aaa8023bdc0dd1248dd2047f840fae2.jpg)  \n图4.4 SD在空间位置关系上生成错误示例的交叉注意力图\n\n在 SD 模型和基于交叉注意力优化损失的组合式生成算法中，如图 4.4 所示，SD 模型在生成时，当  $t = 0$  时，woman 的交叉注意力分布分散且相对不集中，其生成的相对位置与提示文本描述也不符合，其生成的相对位置为日常生活中较为常见的分布布局，符合先验数据生成。而第三章提出的基于交叉注意力损失优化的组合式生成算法在生成过程中，如图 4.5 所示，交叉注意力值最高值点出现的位置大部分情况下都是位于事先设置的布局内，但此时的布局约束没有强制性地约束所有高值均分布在布局中，导致部分较高交叉注意力值溢出设定布局，这是最终生成布局错误的最大原因。\n\n![](images/d7e1420e7cc0e274da472296ee9a5a93c000c5f5002ed75bfe7618414a93bd40.jpg)  \n图4.5基于交叉注意力损失优化的组合式生成算法在空间位置生成错误示例交叉注意力图\n\nGLIGEN在空间位置生成方面要显著优于SD模型和基于交叉注意力损失优\n\n化的组合式生成算法，但其在实体缺失、实体泄露、属性泄露、属性互换等问题的解决上还存在着很大的问题。如图 4.6 所示（提示文本为“an airplane on the right of a turtle”），在生成“turtle”时，虽然最高值点位于其对应的布局中，此时，“airplane”在“turtle”的布局位置上也存在着一个较高注意值区域，这在一定程度上覆盖了“turtle”生成的概率，使模型无法辨别该区域生成物体。\n\n![](images/d982e85ad3a242865868a2a710849c2354e80039fdd593430b26395ec0013d54.jpg)  \n图4.6 GLIGEN在生成空间位置任务上出现实体泄露示例的交叉注意力图\n\n同时，GLIEGN又会出现如图4.7所示（提示文本为“a white car and a red sheep”）所示的属性错误绑定问题。从其对应的交叉注意力图所示，可以推测一部分原因在于在去噪过程前期，“red”对应的最高值区域在远离布局区域的背景区域。另一方面，可能是“red”值对应交叉注意力值的最高值不足以使模型偏离训练数据分布，使其生成不符合现实的或不常出现的“red sheep”。\n\n![](images/aedea84edfc693ea6b516fb5e951b051ea732e71b244d284938bf2cab0e91025.jpg)  \n图4.7 GLIGEN在属性绑定任务上出现属性泄露示例的交叉注意力图\n\n因此，由于 GLIGEN 存在的属性绑定、实体缺失等任务上出现的不足，而基于交叉注意力损失优化的组合式生成算法在属性绑定任务上展现的优越性能，且 GLIGEN 是在 SD 模型的基础上进行改进建立的，且同样是存在注意力机制的模型架构来实现图文对应生成。因此，利用 GLIGEN 的强大布局控制能力以及注意力机制的优势的双重作用下，实现属性绑定及对象空间关系性能的有效提升。因此，本文将改用 GLIGEN 模型作为无训练算法框架的基模型。\n\n# 4.3 方案设计\n\n如图4.8所示，本章提出的基于注意力修正的多条件组合式生成算法框架。首先，通过大语言模型根据提示文本生成布局。随后，将该布局信息融入到门控自注意力机制中（图中U-Net为GLIGEN模型架构）。同时，使用本章中所提出的基于自注意力的结构损失函数  $L_{SRSA}$  （如图4.9所示）、基于交叉注意力的相似度损失  $L_{SSCA}$  （如图4.10所示）等来对潜在空间向量  $z_{t}$  进行优化。在第一阶段生成对应图像之后，根据每个短语的布局信息截取图像中的相应部分图像，进行CLIP图像-文本相似度检测，若该短语未达到符合条件阈值，则使用图像编辑的方式对其进行修正。在去噪过程中，不同阶段的正确生成区域的潜在空间向量被第一阶段对应区域的去噪后期的潜在空间向量所替代。该操作能够最大程度地保留生成正确区域包含的信息。图像编辑修正为该模型生成的第二阶段（如图4.11所示）。通过这两阶段的修正过程，生成过程约束及生成完成后修正，双管齐下，最大程度地减少噪声影响，提升模型图文一致性生成能力。\n\n![](images/058bb44f024777e58f904afc7d3dac012321b8f628ba7081671e41295024c498.jpg)  \n图4.8基于注意力修正的多条件组合式生成算法框架图\n\n# 4.3.1 基于注意力的结构优化损失\n\n自注意力机制图展示了每个图像块与其他图像块之间的关联程度，揭示了生成空间中的主体结构信息。同时，不同的短语集合的交叉注意力图中结构相似程度也影响着最终图像生成的效果，即短语内部的相似度越高，生成的图文一致性越好。因此，本小节提出两种基于注意力的结构优化损失：基于自注意力的结构增强损失（Structural Reinforcement Self Attention Loss，SRSA）和基于交叉注意力的结构相似度损失（Structural Similarity Cross Attention Loss，SSCA）。\n\n# (1) 基于自注意力的结构增强损失（SRSA）\n\n![](images/c00b6dc6d44de9fb47abbd4335b3f00ac4dc1b0d8aa2d8c0ebd3bc48ced22a96.jpg)  \n图4.9基于自注意力的结构增强损失计算示例\n\n基于自注意力的结构增强损失函数旨在通过自注意力机制，实现最小化背景损失，捕捉和增强图像中各个部分之间的关系，确保每个边界框内的图像块在计算时受到的外部影响更小，从而提高模型在处理复杂空间结构能力的同时使得每个目标区域的特征更加突出，从而提高模型的准确性和鲁棒性。\n\n基于自注意力的结构增强损失的计算方式如图 4.9 所示。首先，根据提示文本获得其对应的布局信息并将其转换成对应的掩膜  $M$  。随后，获得每个标记文本内的所有图像块对应的自注意力图  $A_{(i,j)}^{s}$  后，获得该标记本文布局内所有掩膜后自\n\n注意力图的最大值总和。最终所有标记文本子损失总和即为基于子注意力的结构增强损失。该损失确保布局内的图像块能够有效代表对应标记文本信息，使得边界框内的像素在训练过程中能够更好地学习到目标特征，如图中交叉注意力标记的最高值图像块对应的自注意力图（如图4.9自注意力图序列的第二列所示），该图像块都与布局内的其他图像块相关，与布局外的其他图像块不相关。\n\n给定一个提示文本  $p$ ，其中包含  $N$  个标记文本。通过3.2.1所提出的CoC提示链技巧获得所有标记文本对应的布局信息，标记文本  $i$  所对应的布局表示为  $B_{i} = (x_{1},y_{1},x_{2},y_{2})$ ，其中，  $(x_{1},y_{1})$  为左上角坐标，  $(x_{2},y_{2})$  为右下角坐标。于此同时，将位于该布局中的所有坐标表示为  $Fg(B_{i}) = \\{(x,y)|x_{1}^{i}\\leq x\\leq x_{2}^{i},y_{1}^{i}\\leq y\\leq y_{2}^{i}\\}$ ，布局  $B_{i}$  外的所有图像块  $p$  的坐标表示为  $B^{p} = P - \\{(x,y)|x_{1}^{i}\\leq x\\leq x_{2}^{i},y_{1}^{i}\\leq y\\leq y_{2}^{i},\\forall B_{i}\\in B s.t. p\\in Fg(B_{i})\\}$ ，其中  $P = \\{(x,y)\\mid 0\\leq x\\leq w,0\\leq y\\leq h\\}$  表示自注意力图中  $A^s$  所有图像块的坐标集合。因此，SRL可以表示为：\n\n$$\nL _ {S R S A} = \\sum_ {i = 1} ^ {k} \\sum_ {p \\in F g (B _ {i})} M \\left(B ^ {p}, A _ {p, t} ^ {s}\\right) \\tag {4.1}\n$$\n\n其中，函数  $M(B^{p},A_{p,t}^{s})$  具体含义为：\n\n$$\nM \\left(B ^ {p}, A _ {p, t} ^ {s}\\right) = \\max  \\left\\{A _ {p, t} ^ {s} (x, y) \\mid (x, y) \\in B ^ {p} \\right\\} \\tag {4.2}\n$$\n\n# (2) 基于交叉注意力的结构相似度损失（SSCA）\n\n本小节提出的基于交叉注意力的结构相似度损失（Structural Similarity Cross Attention Loss，SSCA）能够加大地增强同一组合短语内部交叉注意力图重叠部分尽可能地保持一致，在不同的组合短语之间减少重叠，协助模型能够更好地分辨不同的图像块中更应该生成与其匹配的组合短语图像特征内容。基于交叉注意力的结构损失相似度计算如图4.10所示。\n\n提示文本  $p$  中包含  $N$  个标记文本, 从中分析提取了  $k$  个无修饰符集合  $\\{S_{1}, S_{2}, \\dots, S_{k}\\}$ ,  $P(S_{i})$  表示第  $i$  个集合  $S_{i}$  中的名词  $n$  与其所有的修饰符  $m$  之间的属性实体标记堆  $(m, n)$  。首先, 实现同一短语集合  $S_{i}$  内部的名词  $n$  与其所有的修饰符  $m$  之间的交叉注意力图重叠率越高越好, 通过该种方式实现对应名词  $n$  其对应的修饰符  $m$  能够有更高地可能性生成在  $n$  上, 其损失函数具体如下所示:\n\n$$\nL _ {p o s} (A, S) = \\sum_ {i = 1} ^ {k} \\sum_ {(m, n) \\in P \\left(S _ {i}\\right)} d i s t \\left(A _ {t} ^ {m}, A _ {t} ^ {n}\\right) \\tag {4.3}\n$$\n\n其中， $A_{t}^{m}$  表示索引为  $m$  的标记文本在  $t$  时刻的交叉注意力图， $\\text{dist}(A_{t}^{m}, A_{t}^{n})$  表示索引标记为  $m, n$  之间交叉注意力图的距离（没有重叠）的度量。\n\n![](images/6486caeb277fb870186794c877c07c87489a3647fc981641a8cdc7486209c7f0.jpg)  \n图4.10基于交叉注意力结构相似度损失计算示例图\n\n其次，为实现不同短语集合之间的名词  $n$  和修饰符  $m$  之间的交叉注意力图重叠率降低，防止出现同一图像块的通道中出现多个不同标记文本的高注意值，导致模型无法分辨该图像块所对应的正确文本标记。因此，有如下所示损失函数，其中， $U(S_{i})$  表示从完整的单词集合中排除  $S_{i}$  集合中的单词所得到的不匹配的单词集合：\n\n$$\nL _ {n e g} (A, S) = - \\sum_ {i = 1} ^ {k} \\frac {1}{| U (S _ {i}) |} \\sum_ {(m, n) \\in P (S _ {i})} \\sum_ {u \\in U (S _ {i})} \\frac {1}{2} \\left(d i s t (A _ {t} ^ {m}, A _ {t} ^ {u}) + d i s t (A _ {t} ^ {u}, A _ {t} ^ {n})\\right) (4. 4)\n$$\n\n最终，基于交叉注意力的结构相似度损失可以表示为：\n\n$$\nL _ {S S C A} = L _ {p o s} + L _ {n e g} \\tag {4.5}\n$$\n\n在使用自注意力图和交叉注意力图对图像结构进行增强时，也是用交叉注意力强化损失实现生成图像与其对应文本实现较好的融合交互。因此，在本章提出的算法框架中，最终使用如下所示函数对潜在空间向量进行优化：\n\n$$\nL _ {t o t a l} = L _ {S R S A} + L _ {S S C A} + L _ {P C A} + L _ {R C A} \\tag {4.6}\n$$\n\n其中，式（3.1）-（3.2）计为  $L_{PCA}$ ，式（3.3）-（3.4）计为  $L_{RCA}$ ，使用名词  $n$  或是修饰词  $m$  的损失判别，则是根据章节 3.2.2 式（3.5）判断。\n\n# 4.3.2 基于CLIP相似度检测的图像编辑修正模块\n\n尽管基于注意力机制的损失函数在引导标记文本的交叉注意力聚焦于特定的布局信息方面展现出了显著的优势，能够在一定程度上提升文本到图像的生成质量。然而，在去噪生成这一复杂的流程中，交叉注意力机制的有效性却并非毫\n\n无阻碍，而是受到了多重因素的制约与影响。具体而言，当注意力的分配出现偏差或不合理时，就可能导致生成的图像与预期目标产生较大的偏差，甚至完全偏离了原本的意图，这无疑会极大地影响生成图像的质量和实用性。\n\n为了有效应对这一问题，本小节提出了一种创新性的强制性调整方案，旨在通过更为精准和细致的调控手段，来确保生成的图像能够准确反映标记文本的内容与意图。该方案的核心在于，对生成图像进行分区裁剪后，进行CLIP文本-图像相似度检测，能够迅速判断每个标记文本短语集合是否在LLM生成的布局内部正确地生成了与之对应的图像，随后在生成与标记文本不相符合的区域使用图像编辑的方式对其进行修正。通过这种方式，可以对初始生成中可能出现的错误部分进行有效的修正，从而进一步提升生成图像的质量和准确性。其具体模型图如图4.11所示。\n\n![](images/c7993215a0ee109d9e7bb40b4bc4b855a6586fb2451d0bd40ca57dea7f72a7d9.jpg)  \n图4.11基于CLIP相似度的图像编辑修正示例\n\n![](images/de6c31e2cdb3508639749fd9858cdd67cb65489a0d8826e7ac248c01b6a8a07c.jpg)\n\n提示文本  $p$  中包含  $N$  个标记文本，从中分析提取了  $k$  个无修饰符集合  $\\{S_{1}, S_{2}, \\dots, S_{k}\\}$ ， $P(S_{i})$  表示第  $i$  个集合  $S_{i}$  中的名词  $n$  与其所有的修饰符  $m$  之间的属性实体标记堆  $(m, n)$ 。通过 3.2.1 所提出的 CoC 提示链获得所有标记文本对应\n\n的布局信息，标记文本i所对应的布局表示为  $B_{i} = (x_{1},y_{1},x_{2},y_{2})$  ，其中，  $(x_{1},y_{1})$  为左上角坐标，  $(x_{2},y_{2})$  为右下角坐标。在去噪过程中通过逐步去噪实现随机噪声 $z_{T}$  到  $z_{0}$  的转变。最终，通过解码器  $D(\\cdot)$  获得最终生成图像  $\\hat{x}$  ，即：\n\n$$\n\\hat {x} = D \\left(z _ {0}\\right) \\tag {4.7}\n$$\n\n对于生成的图像  $\\hat{x}$ , 每个实体标记集合  $S_{i}$  都使用其对应布局中的边界框  $B_{i} = (x_{1}, y_{1}, x_{2}, y_{2})$  进行裁剪（一个集合内部的所有标记文本共享同一个边界框），表示为  $\\hat{x}_{i} \\in R^{4 \\times h \\times w}$ , 其中  $h = y_{1} - y_{2}, w = x_{2} - x_{1}$ , 表示边界框  $B_{i}$  的长和宽。然后, 将裁剪后的图像块与其对应的标记短语  $S_{i}$  进行 CLIP 文本-图像相似度比较, 即将裁剪后的图像和对应文本转换到同一向量空间, 计算余弦相似度, 表示如下:\n\n$$\n\\cos i n e _ {i} = \\frac {\\operatorname {C L I P} _ {\\text {t e x t}} \\left(S _ {i}\\right) \\cdot \\operatorname {C L I P} _ {\\text {i m a g e}} \\left(\\hat {x} _ {i}\\right)}{\\| \\operatorname {C L I P} _ {\\text {t e x t}} \\left(S _ {i}\\right) \\| \\left\\| \\operatorname {C L I P} _ {\\text {i m a g e}} \\left(\\hat {x} _ {i}\\right) \\right\\|} \\tag {4.8}\n$$\n\n在进行相似度检测时，设定了一个明确的相似度阈值  $\\tau_{text - image}$  。只有当生成的图像与对应的标记文本之间的相似度达到或超过这个阈值时，即  $cosine_i \\geq \\tau_{text - image}$ ，则认为该部分生成是符合预期的。然而，如果相似度低于这个阈值，就意味着生成的图像与标记文本之间存在较大的不匹配，需要进行修正。针对这种情况，采用图像编辑的方法来进行修正。对满足相似度要求的区域进行掩膜处理，即将这些区域中的潜在空间向量进行保存，保证前一阶段生成的正确图像内容不受下一阶段重新生成的影响。然后，在此基础上在错误区域覆盖上随机初始噪声，重新生成错误区域的新图像，期望能生成更符合标记文本内容的图像部分。\n\n具体而言，在第一阶段去噪过程中保留每一步去噪过程的潜在空间向量，将其表示为  $\\{z_{T}, z_{T-1}, z_{T-2}, \\ldots, z_{0}\\}$ ，从中找到  $z_{0.7T}$  和  $z_{0.3T}$ ，在其中裁剪出达到相似度阈值的正确区域。随后，在编辑图像的第二阶段生成的初始噪声表示为：\n\n$$\n\\widehat {z _ {T}} = z _ {0. 7 T} \\cdot M + N (\\mu , \\sigma^ {2}) \\cdot (1 - M) \\tag {4.9}\n$$\n\n其中， $M$  是一个二进制掩码，将符合阈值的对应短语边界框设置为 1，其余区域设置为 0。 $N(\\mu, \\sigma^2)$  为随机高斯噪声。\n\n随后，对初始噪声  $\\widehat{z_T}$  进行0.3T次的迭代去噪，得到  $\\hat{z}_{0.7T}$  。将  $z_{0.3T}$  中生成正确的区域进行裁剪替换到  $\\hat{z}_{0.7T}$  中，再将其进行0.4T轮次的迭代。该过程是为保证在第一阶段生成正确的区域不受第二次生成去噪过程的影响，在该过程保证使用掩膜  $M$  覆盖的区域冻结，尽可能地保留生成正确的图像区域信息完整。在最终的0.3T轮次的去噪迭代中，将不使用掩膜  $M$  ，这是为了保证第二轮生成图像区域与第一轮生成的图像区域能够平滑地衔接混合，提升图像的高质量水准。\n\n# 4.4 实验结果分析\n\n# 4.4.1 实验设置\n\n本章节通过其API使用GPT-3.5模型从文本提示中生成边界框，包括最大边界盒大小和期望的输出格式等，具体方法与第3章生成布局边框方法一致。使用官方GLIGEN多模态控制模型，使用线性调度率设置比例因子  $\\alpha_{t}$ ，该线性调度率从20开始并线性衰减到最小值10，使用  $s = 7.5$  对潜在空间向量  $z_{t}$  进行无分类器引导，所使用的交叉注意力图维度为  $16\\times 16$  。所有图像生成使用PLMS去噪方法，其去噪步骤为50步，损失优化时间步数  $t^{\\prime}$  设置为30步。进行迭代优化的时间步集合  $T_{iteration} = \\{0,5,10,20,30\\}$ ，在该集合内的时间步上的最大迭代次数设置为25，其余的时间步上的最大优化次数均为1。在图像生成完成后的验证步骤使用CLIP ViT-B/32验证裁剪图像块与其对应短语的Text-Image相似度，验证该区域图像是否正确生成，其相似度阈值设置为  $\\tau_{text-image} = 0.30$  ，为缩短图像生成时间，图像编辑修正次数为1。若实现图像生成更高的图文一致性，可将  $\\tau_{text-image}$  设置为更高的阈值或实现多轮迭代修正编辑。为确保公正性，本章节进行比较的无训练模型算法均为在Stable Diffusion v1.5版本及其衍生GLIGEN。基线方法和本章提出的方法为每个提示文本生成10张图像后进行定量指标检测。\n\n本小节将本章提出的算法框架与当前先进的无训练形式的提升组合式生成任务的算法框架和训练模型进行了全面比较。无训练基线方法包括：Stable Diffusion[3]（SD）、Structured Diffusion[48]（Structured）、Composable Diffusion[73]（Composable）、Attend-and-Ecite[54]（A&E）、Layout Diffusion[63]（LD）、Divide&Bind[55]（D&B）、Conform[58]、SynGen[46]、Attention-Refocusing[69]（AR）。训练微调算法模型：TokenCompose[90]、PixArt-alpha[4]、PixArt-sigma[9]、GLIGEN[16]。通过与13种基线模型的定性定量指标比较，能够全面地展示本章提出的方法在空间任务关系上的有效提升以及在属性绑定任务上的进一步改善。\n\n# 4.4.2 定量评估\n\n在本章节中，对十四种基线方法进行了详尽的比较，所使用的基模型为Stable Diffusion v1.4版本或其基础上的衍生模型GLIGEN，同时为更广泛地验证本章提出的算法框架的优越性，与Transformer架构的PixArt系列模型进行比较。为了更直观地展示本章提出的方法与这些基线方法之间的定量比较结果，在表5中进行了详细的列示。本章提出的算放框架主要是实现正确的空间约束，因此主要\n\n定量指标为 Object Relationship 任务中的 Spatial 子任务，该指标数值越高，说明本章提出的算法框架能够实现更高的空间约束。表 5 主要分为三部分：以 SD 为基础的算法框架、以 GLIEGN 为基础的算法框架、以 Transformer 架构为基础的 PixArt 系列模型，通过对表 5 的深入分析，提炼出了一些关键见解，阐述了本章所提出的方法具备的有效性及其在特定任务中的优势。\n\n表 5 基于编辑修正的多条件组合式文生图算法定量比较  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>BLIP-VQA↑</td></tr><tr><td>SD</td><td>0.3668</td><td>0.3633</td><td>0.4074</td><td>0.1134</td><td>0.3101</td><td>0.3553</td></tr><tr><td>Structured</td><td>0.3732</td><td>0.3602</td><td>0.4020</td><td>0.1117</td><td>0.3085</td><td>0.3404</td></tr><tr><td>Composable</td><td>0.3743</td><td>0.3642</td><td>0.4058</td><td>0.1151</td><td>0.3082</td><td>0.3514</td></tr><tr><td>A&amp;E</td><td>0.4732</td><td>0.3913</td><td>0.5646</td><td>0.1133</td><td>0.3096</td><td>0.3574</td></tr><tr><td>SynGen</td><td>0.6994</td><td>0.4736</td><td>0.6746</td><td>0.1238</td><td>0.3084</td><td>0.4131</td></tr><tr><td>Layout Diff</td><td>0.3489</td><td>0.3605</td><td>0.3936</td><td>0.1086</td><td>0.3032</td><td>0.3796</td></tr><tr><td>D&amp; B</td><td>0.5073</td><td>0.4107</td><td>0.5448</td><td>0.1526</td><td>0.3067</td><td>0.3950</td></tr><tr><td>Conform</td><td>0.6844</td><td>0.4886</td><td>0.6772</td><td>0.1388</td><td>0.3073</td><td>0.4288</td></tr><tr><td>TokenCompose</td><td>0.4557</td><td>0.4029</td><td>0.5197</td><td>0.1855</td><td>0.3132</td><td>0.3695</td></tr><tr><td>PixArt-Alpha</td><td>0.3981</td><td>0.4030</td><td>0.4605</td><td>0.1975</td><td>0.3076</td><td>0.3889</td></tr><tr><td>PixArt-Sigma</td><td>0.5916</td><td>0.4746</td><td>0.5699</td><td>0.2513</td><td>0.3077</td><td>0.4626</td></tr><tr><td>GLIGEN</td><td>0.3902</td><td>0.3717</td><td>0.4534</td><td>0.1806</td><td>0.3012</td><td>0.3307</td></tr><tr><td>A-R</td><td>0.4786</td><td>0.3890</td><td>0.4786</td><td>0.3823</td><td>0.2994</td><td>0.3412</td></tr><tr><td>Ours</td><td>0.7550</td><td>0.5445</td><td>0.7060</td><td>0.3926</td><td>0.2998</td><td>0.4013</td></tr></table>\n\n首先，在十四种提出的基线模型中，本章提出的方法在多个任务上的定量指标均为最优。本章提出的算法框架主要是为了解决空间交互问题，即查看 Spatial 任务指标，本章提出的方法在该任务上的定量指标为 0.3926，为所有比较模型的最优指标，同时在多个子任务中性能达到次优，与最优表现差距较小。因此，定量指标验证了本章提出的基于编辑修正的多条件组合式文生图算法在解决空间交互问题中的有效性，同时对其他组合式生成任务有一定的性能改进。\n\n其次，通过对表中的数据的仔细分析，可以看出基于GLIGEN的模型算法（A-R、Our）在空间任务上的表现远高于基于SD（SynGen、Conform等）的模型算法。同时，如TokenCompose使用高质量数据和先进的损失函数对SD模型进行重新微调训练后的性能，也会远远的优秀于基于原始SD的算法模型，但该\n\n模型在空间任务上的性能也与原始的 GLIEGN 模型相似。PixaArt 系列模型也是如此，使用更多的参数量、更优质的训练数据等方式，令其能够更好地理解提示文本中所包含的语义空间信息。但都无法与其优化算法框架（A-R、Our）相差甚远。该定量结果的比较也验证了本章在 4.2 提出的想法：空间任务中的性能提升与该算法的基模型在空间任务中的性能效果息息相关。GLIEGN 模型之所以能够实现较好的空间布局约束，其主要原因在于能够接受额外的布局约束条件注入的注意力机制中，使模型能更好地理解提示文本中的布局关系。\n\n最后，本章提出的算法框架在属性绑定任务上也得到了进一步的提升，如Attribute Binding数据集中的Color Binding任务中，本章提出的方法达到0.7550，远远高于其他基线模型。在属性绑定任务中性能较好的SynGen、Conform算法在实现Spatial任务是性能远远低于本章提出的方法。因此，基于编辑修正的多条件组合式文生图算法在多方面任务中均提升了基模型的生成能力，提高其上限。\n\n综上所示，本章提出的方法在多个方面展现了其优越性。首先，在处理空间关系任务时，相较于基线方法，提出的方法能够显著提升模型的性能。这一发现为今后的研究提供了新的视角，使用具备更强空间约束能力的基模型，辅助优化损失函数对潜在空间向量进行修正，能够有效地提升模型在空间约束方面的能力。其次，去噪时优化及生成后的检测、编辑修正也是其提升性能的重要环节。图像生成后的检测、编辑修正也为第一阶段无法优化情况增添新的防护措施，双管齐下减少错误图像的生成。\n\n# 4.4.3 定性评估\n\n本小节对提出的基线方法与本章所提出的方法进行了定性比较。图 4.12 展示了本章提出的方法与以 SD 无算法框架的定性比较。图 4.13 展示了本章提出的方法与非 SD 无训练算法框架的定性比较。\n\n如图 4.12 第六列所示（提示文本为“The red hat was on top of the brown coat rack”）的复合提示中，即提示文本中包含两个及以上带属性的实体描述以及其对应的空间位置关系下，基线模型都无法生成与提示文本符合图像，只能够满足部分提示文本，如 SD 正确生成“red hat”、Layout Diff 正确生成“brown coat rack”等，只有本章提出的方法正确生成提示文本中的两个物体，并且两个物体的空间位置正确。这也证明了对于复杂的空间文本需要提供额外的控制条件帮助模型理解提示文本内含的语义提示。\n\n如图 4.12 第六列所示（提示文本为“The red hat was on top of the brown coat rack”）的复合提示中，即提示文本中包含两个及以上带属性的实体描述以及其对应的空间位置关系下，基线模型都无法生成与提示文本符合图像，只能够满足\n\n部分提示文本，如 SD 正确生成“red hat”、Layout Diff 正确生成“brown coat rack”等，只有本章提出的方法正确生成提示文本中的两个物体，并且两个物体的空间位置正确。这也证明了对于复杂的空间文本需要提供额外的控制条件帮助模型理解提示文本内含的语义提示。\n\n![](images/bfa2d0dd1e2850a2ea94d698e5b4a2e2ab6ea15ae868bef4521958d2e583198a.jpg)  \n图4.12与基于SD的无训练算法模型的定性比较\n\n如图 4.13 第一列 Color Binding 任务（提示文本为“A green cup and a blue cellphone”）和第五列 Non-Spatial 任务（提示文本为“A woman is speaking on the phone and making plans with a friend”）可以看出，本章提出的算法框架图像生成质量较 PixArt 系列模型相差较远，在人物生成方面，PixArt 系列模型采用插画风\n\n格图像避免了 SD 系列模型本身所具有的缺陷。但 PixArt 系列模型生成图像与提示文本的语义相符程度与远远低于本章提出的基于编辑修正的多条件组合式文生图算法框架。\n\n如图 4.13 第三列 Shape Binding 任务（提示文本为“A teardrop pendant and a cubic bracelet charm”）中所示的生成图像，只有本章提出的方法正确生成提示文中两个现实世界中不常出现的形状物体，而其他基线模型生成图片中要不出现实体缺失（如 PixArt 系列模型），要不出现属性泄露（如 GLIGEN）。但与此同时也能发现，本章提出的方法虽然生成图像符合提示文本语义，但是其图像生成质量却有所下降，这在一定程度上可能是优化方法促使模型偏离了训练拟合数据。\n\n![](images/8ea37354197ae44c33968c82772b6754c018d13d959c403ca8ff84dc95a1f6a4.jpg)  \n图4.13 与基于非SD算法模型的定性比较\n\n此外，本小节给出了基于SD的利用布局条件的无训练算法（Layout Diff）和本章提出的方法在空间约束方面的性能比较，如图4.14所示。如图中定性图片展示，Layout Diff使用的布局约束的方式无法使得对应布局的提示文本生成的\n\n图像块分布在布局条件内部，而是出现逸散在整个图像空间中。如图 4.14 第一列（提示文本为“A blue apple and a green cup”）所示，Layout Diff 虽然生成了正确的图像，但是其布局与所给出的布局条件完全不相符。SD 模型自身缺陷导致其无法正确理解文本空间中的位置关系，又因随后的优化算法无法精准地利用布局条件约束生成等多方面因素，最终使得如 Layout Diff 类似的算法框架在实现 Spatial 任务（如图 4.14 第四列，提示文本为“A balloon on the bottom of a chicken”所示）中表现性能效果较差。于此同时，可以观察看出本章提出的算法框架能够最大可能地约束生成图像在其对应的布局中。\n\n![](images/fa33c47d6283448f6be20ff172d38b9e4269d1b13993f5a0a79ceeb201a30275.jpg)  \n图4.14基于GLIGEN算法框架与基于SD算法框架在空间约束能力的比较示例\n\n# 4.4.4 消融实验\n\n本小节通过一系列的消融实验来验证所提出的两个基于注意力的结构优化损失，即基于自注意力的结构增强损失  $L_{SRSA}$  和基于交叉注意力的结构相似度损失  $L_{SSCA}$ ，和基于CLIP相似度检测的图像编辑修正模块的有效性。同时，为检测基于交叉注意力的图文一致性损失  $L_{PCA}$  和  $L_{RCA}$  对基于注意力的结构优化损失的影响，添加对  $L_{PCA}$  和  $L_{RCA}$  的消融实验。基于CLIP相似度检测的图像编辑修正模块是在图像生成后第二阶段，因此，本小节的消融实验设置主要分为两部分：第一部分为不同优化损失组合比较，而不添加图像编辑修正模块，如表6所示。第二部分为模型拥有完整的优化损失，与是否存在图像编辑修正模块的生成性能比较，如表7所示。\n\n第一部分针对不同优化损失组合消融实验具体设置如下所示：（1） $w / o$  PCA Loss：该模型变体在优化过程中排除了PCA损失，以评估其对整体性能的影响，其最终优化损失函数为 $L_{total} = L_{SRSA} + L_{SSCA} + L_{RCA}$ 。（2） $w / o$  RCA Loss：该模型变体排除了RCA损失，旨在探讨RCA损失对模型性能的贡献，其最终优化损失函数为 $L_{total} = L_{SRSA} + L_{SSCA} + L_{PCA}$ 。（3） $w / o$  SSCA Loss：该模型变体排除SSCA损失，以评估其对模型整体性能的影响。其最终优化损失函数为 $L_{total} = L_{SRSA} + L_{PCA} + L_{RCA}$ 。（4） $w / o$  SRSA Loss：该模型变体排除了SRSA损失，以评估该损失对模型整体性能的影响。其最终优化损失函数为 $L_{total} = L_{SSCA} + L_{PCA} + L_{RCA}$ 。（5）All Loss：该模型变体包含本章提出的所有优化损失函数，其最终损失函数为 $L_{total} = L_{SRSA} + L_{SSCA} + L_{PCA} + L_{RCA}$ 。\n\n表 6 基于编辑修正的多条件组合式文生图算法优化损失组合消融实验定量指标  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>BLIP-VQA↑</td></tr><tr><td>w/o PCA Loss</td><td>0.6770</td><td>0.4240</td><td>0.6041</td><td>0.3096</td><td>0.3006</td><td>0.3604</td></tr><tr><td>w/o RCA Loss</td><td>0.6981</td><td>0.4551</td><td>0.6581</td><td>0.3861</td><td>0.2918</td><td>0.3605</td></tr><tr><td>w/o SSCA Loss</td><td>0.3996</td><td>0.4001</td><td>0.5005</td><td>0.3706</td><td>0.2989</td><td>0.3437</td></tr><tr><td>w/o SRSA Loss</td><td>0.7078</td><td>0.4491</td><td>0.6330</td><td>0.3769</td><td>0.3012</td><td>0.3877</td></tr><tr><td>All Loss</td><td>0.7138</td><td>0.4982</td><td>0.6384</td><td>0.3899</td><td>0.3005</td><td>0.3937</td></tr></table>\n\n定量结果如表6所示，展示了不同优化损失组合的算法框架对模型性能的影响。通过对这些定量数据的分析，本小节得出了一些重要的结论：（1）SRSA Loss对于在属性绑定任务和Spatial任务中都存在一定影响，其原因在一定程度上可能是由于图像块之间相互的相关联性的增强，辅助相关属性绑定在其固有的区域位置中，减少与区域外的物体的交互影响。（2）SSCA Loss对属性绑定任务具有较大的影响程度，其原因可能在于减少了不同短语对之间的交叉注意力图重叠程度，增强了短语对内部交叉注意力图的相似程度，使得属性能够更好地绑在其匹配的实体区域。（3）RCA Loss在Texture Binding任务中表现较好，其原因可能是针对纹理这种特殊的分布在整个实体上的属性，其对于RCA Loss和SSCA Loss这种以区域为基础的优化损失依赖程度较深，减少了对块损失和区域损失之间的相互冲突。综上所示，本章综合多个组合式生成任务定量指标挑选All Loss模型变体作为本章算法框架的第一阶段生成模型。\n\n第二部分基于CLIP相似度检测的图像编辑模块消融实验设置如下所示：（1）w/o Edited: 该模型变体中不包含基于CLIP相似度检测的图像编辑模\n\n块，但包含本章提出的所有优化损失函数。（2）Our：本章提出的完整的基于编辑修正的多条件组合式文生图算法。定量结果如表7所示。展示了基于CLIP相似度检测的凸显编辑模块对模型性能的影响。通过对定量数据的分析，本小节得出了一些重要的结论：本章提出的基于CLIP相似度检测的图像编辑模块在多个组合式生成任务中的性能均得到在提升，这一成果有力地证明了该模块能够有效地弥补模型在第一阶段可能产生的生成错误。\n\n表 7 基于 CLIP 相似度检测的图像编辑修正模块消融实验定量指标  \n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Attribute Binding</td><td colspan=\"2\">Object Relationship</td><td rowspan=\"2\">Complex</td></tr><tr><td>Color Binding</td><td>Shape Binding</td><td>Texture Binding</td><td>Spatial</td><td>Non-Spatial</td></tr><tr><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>BLIP-VQA↑</td><td>Unidet↑</td><td>CLIP↑</td><td>BLIP-VQA↑</td></tr><tr><td>w/o Edited</td><td>0.7138</td><td>0.4982</td><td>0.6384</td><td>0.3899</td><td>0.3005</td><td>0.3937</td></tr><tr><td>Our</td><td>0.7550</td><td>0.5445</td><td>0.7060</td><td>0.3926</td><td>0.2998</td><td>0.4013</td></tr><tr><td rowspan=\"2\">w/o PCA</td><td>A red rabbit and a yellow rat</td><td>A woman on the top of a pig</td><td>A diamond pendant and a round locket</td><td>A fabric rug and a leather belt</td><td>A woman is holding a yoga mat and heading to a class</td><td>The blue water bottle was on top of the red backpack</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/o RCA</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/o DIS</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/o SAR</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/o Edited</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Our</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>\n\n图4.15基于编辑修正的多条件组合式文生图算法消融实验定性比较\n\n图4.15展示了本小节消融实验的定性评估，其结果突出了本章的各阶段提出的方法在多种组合式生成任务数据集中的有效性。如图4.15中第二列空间位置任务（提示文本为“A woman on the top of a pig”）所示，第二阶段基于CLIP相似度检测的图像编辑修正模块能够有效的缓解实体泄露或融合的情况出现。\n\n图4.15中第四列的纹理属性绑定任务（提示文本为“A fabric rug and a leather belt”），通过检测编辑模块成功将先前未生成的皮质腰带重新生成。图4.15中第五列非空间交互任务（提示文本为“A woman is holding a yoga mat and heading to a class”）所示，该模块成功将生成不符合现实世界的人体动作，编辑修正后为符合“yoga”姿势的图像，但该模块对于微妙的动作描述仍然无法正确理解，即提示文本中所需要的是拿着瑜伽垫、去上课。这在一定程度上与基模型自身的数据偏差以及CLIP文本编码器对提示文本理解的缺陷。\n\n本小节提供了更多应用基于CLIP相似度检测的图像编辑模块前后的示例，如图4.16所示。其中图4.16第二列（提示文本为“Ablack hat and a red scarf”）所示，在第一阶段生成的图像中没有正确生成红色围巾，在编辑修正后将错误生成的鲜花修正成一只围着红色围巾的狗，符合提示文本含义。\n\n![](images/81eb223a49fef94cd3a3d3f90da09a63a8926b0d3cf22f9ee9318a8184c6a0cb.jpg)  \n图4.16第二阶段基于CLIP相似度检测的图像编辑模块应用前后示例图\n\n# 4.4.5 算法局限性\n\n本小节主要说明本章所提出的方法的局限性，如图4.17所示，给出本文算法框架生成的错误示例：\n\n(1) 编辑修正效果未达到预期效果（如图 4.17.a 所示）。该问题编辑效果问题可具体分为三类小问题：编辑后正确生成提示文本中的图像，但与原始图像没有正确的融合或融合效果不好（如提示文本“a blue apple and a green vase”所示）。该问题的出现可能是在使用 spacy 库自动提取修饰词及其名词时候出\n\n现错误，导致词语被拆分，出现错误标志词；编辑后未能生成较好地与提示文本相符合的图像。编辑后的图像生成效果不如编辑前的生成效果（如提示文本为“a blue bear and a brown boat”、“an airplane on the right of a mouse”所示），噪声再生成的随机性导致；编辑后生成图像不完全满足提示文本生成（如提示文本“a leather chair and a glass cup”所示），编辑后出现杯子，但其对应材质生成错误，在一定程度上是模型生成能力和算法引导缺陷导致。\n\n(2) 对于提示文本中微妙的动作行为或者不常出现的实体无法正确生成（如图 4.17.b 所示）。如提示文本“A cat is lazily lounging in a sunny windowsill”所示，对于提示文本中的“lazily lounging”类似的带有抽象情绪动作，无法准确地表达在生成图像中。如“blue banana”和“teardrop plum”等真实生活中或者是训练数据中不经常出现的实体，模型虽然生成，但其生成质量较差，与真实物体差距较远。这主要是由于模型拟合概率分布的偏差导致。\n\n![](images/04404e7a5ebf10ec937d8c69d9300f8ee6b37056200b44e9fccc907a630cd99a.jpg)  \n(a) 编辑修正后的图像视觉效果不如修正前\n\n![](images/f07ebf68a974d61c6de7ed4ebf06ae5241fff2f618c25bad5b3e1a1b638f03f6.jpg)  \n(c）生成图像质量较原始模型下降  \n图4.17基于编辑修正的多条件组合式文生图算法局限性示例\n\n(3) 使用算法框架后的生成图像质量较原始模型有所下降（如图 4.17.c 所示）。使用本章提出的算法框架来生成图像，其生成效果较之原始模型生成图像更加符合提示文本语义（如提示文本“A bathroom with green tile and a red shower curtain”所示），但会出现一定程度的图像生成质量的下降，图像中出现粉紫色滤镜伪影。特别是出现一些现实世界中不常出现的组合，在 T2I-Compbench 中的 shape binding 任务中表现尤为突出。其生成质量下降一部分体现在生成图像与拼接画类似（如提示文本“a sliver spoon and a blue plate”\n\n所示），另一部分则是提示实体生成的图像存在伪影或边缘模糊的情况（如提示文本“a blue backpack and a gold clock”所示）。\n\n(4) 对于人体面部、躯干细节生成与实际不相符（如图 4.17.d 所示）。GLIGEN 是在 SD 模型上的衍生，是对 SD 的微调训练后的结果。因此，SD 模型本身具备的问题，GLIGEN 也存在。这种局限原因在于：首先，模型架构对三维人体结构的隐式表征能力不足，U-Net 的二维卷积特性难以捕捉人体姿态的立体空间关系；其次，训练数据集中人体动态序列的稀缺性导致模型缺乏连续动作的建模能力；再者，CLIP 文本编码器对运动描述词的多义性解析偏差，加剧了动作指令与生成结果之间的语义鸿沟。\n\n# 4.5 本章小结\n\n本章介绍了一种无训练基于注意力修正的多条件组合式图像生成算法研究，目的在于通过用户单一提示文本条件输入实现生成与文本语义高度一致的图像。首先，针对不同基模型在不同生成能力任务的表现进行分析，选择与当前所要完成目标最为一致的多模态可控模型作为基模型。随后，根据通过大语言模型获得的标记文本布局，利用本章提出的基于注意力的两种结构强化损失：基于自注意力的结构损失函数  $L_{SRSA}$  、基于交叉注意力的相似度损失  $L_{SSCA}$  来对潜在空间向量 $z_{t}$  进行优化。在图像生成的初步阶段完成后，根据预设的每个短语的布局信息，从生成的图像中裁剪出对应的图像部分。随后，利用CLIP技术进行图像与文本的相似度评估。如果某个短语对应的图像部分未能达到预设的相似度阈值，将采取图像编辑手段对其进行调整。通过两个阶段结合，即去噪过程引导约束和生成后针对性修正，旨在最大限度地削弱噪声干扰，增强模型在图文一致性生成能力。\n\n本章提出的方法主要目的在于提示扩散模型在空间任务上的组合式生成能力。为了验证该方法的有效性和实用性，进行了一系列的实验。首先，通过定性和定量的方式与选择的基线模型进行对比，全面评估了本章方法与基线模型在处理空间布局和属性绑定任务上的表现差异。实验结果表明，与基线模型相比，本章提出的方法在多个方面都展现出了显著的优势。同时，在属性绑定任务上，该方法也表现出了更高的稳定性和准确性，使得生成的图像更加符合文本描述中的细节要求。此外，还进行了模型自身的消融实验，以进一步探究该方法中各个组件对整体性能的影响。通过对比分析不同组件的移除对模型性能的影响程度，更加深入地理解了该方法中各个部分的贡献和作用机制。\n\n# 第5章 总结与展望\n\n# 5.1 工作总结\n\n本文研究聚焦于基于扩散模型注意力机制的组合式图像生成算法研究，提高其在下游任务中的应用性能。随着人工智能技术的迅速发展，以及深度融入商业发展中的趋势，要求提升扩散模型可控性生成能力已经迫在眉睫。组合式生成任务形式的提示文本在日常生成中更为常见，设计师希望利用文生图模型增加素材库、扩展思维，因此，能够正确生成日常生活不常见的事物是其中的重要环节。当前的扩散模型在生成图像时，由于，文本编码器内在结构的缺陷、训练数据以及训练范式等多方面的因素影响，常常会生成于文本提示不相符的图像。而先前的一些解决方法，如微调模型方式，训练所需要的时间、金钱成本过高，因此，无训练模型的算法框架应运而生。同时，在文生图模型中最重要的就是实现文本特征与图像特征的融合对齐，其中起到关键作用的讲就是注意力机制。注意力机制分为交叉注意力机制和自注意力机制，交叉注意力机制是实现文本特征与图像特征融合对齐的关键，而自注意力机制是实现图像特征和图像特征之间的关联程度展现，表示了在生成过程中的图像空间结构信息。\n\n因此，基于上述观点，本研究提出了基于注意力机制的组合式文生图算法及其增强版本，分别为基于交叉注意力损失优化的组合式文生图算法和基于编辑修正的多条件组合式文生图算法。\n\n基于交叉注意力损失优化的组合式文生图算法解决了扩散模型在组合式生成任务中的属性绑定任务出现的问题，包括实体缺失、实体泄露、属性互换、属性泄露。首先，使用 CoC 文本提示链通过大语言模型生成提示文本中相应标记文本的布局。随后，在去噪过程中利用交叉注意力图布局信息，提出两种新颖的损失来优化潜在空间向量，让扩散模型的去噪方向朝着更加符合文本语义的方向前进。最后，通过大量的实验结果证明，本文的方法在解决属性绑定任务中具有明显的优势，在空间位置及其他任务中，也展示出了一定的性能提升。同时，该算法框架具有较强的可泛化性，能够将其应用到当前先进的一些研究组合式生成任务的算法框架中，并进一步提升其性能。\n\n基于编辑修正的组合式文生图算法探索了通过引入更多模态信息应用到扩散模型去噪过程中，解决扩散模型在组合式生成任务中的空间交互任务上出现的错误，包括空间位置错误、交互错误问题。该算法研究最先分析和展示了文生图\n\n扩散模型（如 SD）以及多模态可控文生图模型（如 GLIGEN）在组合式生成任务中空间位置约束的能力差异，引出使用多模态可控文生图模型作为无训练算法框架的基模型。随后，提出两种基于注意力机制的结构优化损失，进一步约束图像的空间结构信息。与此同时，还利用了与交叉注意力机制结合的损失优化函数。最后，为防止去噪过程中注意力的损失累加导致的错误，还引入基于 CLIP 相似度的图像编辑修正机制，进一步保证图像生成效果。\n\n综上所述，本文致力于无训练模式的基于注意力机制的组合式文生图任务，旨在满足生成与提示文本语义高度相符的图像。通过提出的基于交叉注意力损失优化的组合式文生图算法和基于注意力修正的多条件组合式文生图算法，增强了扩散模型组合式生成任务能力，并且该算法框架具有强大的可泛化性，为基于扩散模型的下游任务实现效果增强提供了强大的保障。\n\n# 5.2 未来展望\n\n本研究的核心关注点在于深入探索并优化扩散模型在执行组合式生成任务时的能力。当前，文生图模型快速发展，组合式生成任务更是日常生活中的重要组成部分。因此，提升其效能不仅是技术进步的体现，更是推动相关领域研究深入发展的关键。本小节在总结现有研究成果的基础上，提出了以下几项未来研究展望：\n\n# (1) 设计模型框架应对当前现有扩散模型所存在的结构缺陷\n\n未来的研究可以致力于打造一个更为强大且全面的模型框架，这一框架将充分利用更加可靠且高质量的图像数据资源，同时整合更具训练效率的优化范式。这样做的目的在于，从根本上解决当前扩散模型所面临的组合式生成问题，这些问题往往源于文本编码器结构的局限性、训练范式的不足以及训练数据本身存在的缺陷。具体而言，通过深度挖掘图像数据与文本信息之间的内在联系，构建一个能够精准捕捉并融合这两类信息的模型框架。在这个过程中，要特别注重提升文本编码器的性能，使其能够更准确地理解和表达文本中的复杂语义信息，从而避免在组合式生成过程中出现语义混淆或表达不清的问题。此外，还将致力于优化训练范式，通过引入更先进的算法和技术，提高模型的训练效率和泛化能力。最后，还将特别关注模型的推理响应时间。通过优化模型结构和算法设计，能够在不增加推理响应时间的前提下，有效提升生成结果的质量和准确性。这将为模型在实际应用中的推广和普及奠定坚实的基础。\n\n# (2) 提高算法的容错能力和可泛化性\n\n在当前提出的无训练算法框架中，借助了大语言模型来生成至关重要的布局信息。但大语言模型在生成布局信息时，常常会出现不合常理的布局设计。这些\n\n不合理的布局在一定程度上削弱了算法框架的实用性和可靠性。因此，未来的研究一方面可以致力于探索一种能够显著提升大语言模型生成合理布局能力的算法框架。这可能需要深入挖掘大语言模型的内在机制，理解其生成布局信息的原理，并在此基础上进行有针对性的优化和改进。同时，也可以尝试通过一系列精心设计的提示词工程来修正大语言模型的输出，引导其生成更加符合常理和预期的布局设计。另一方面，当前对基于注意力的损失优化函数，仅仅是简单的求和，但是不同模态之间存在着一定的冲突。因此，为进一步提升模型性能，需要更加深入地理解各项损失函数的具体含义和作用机制，尝试探索不同损失的整合策略，以期在保留各自优势的同时，有效减少或消除它们之间的冲突和干扰。\n\n# (3) 引入多模态视觉模型，进一步提升可控生成能力\n\n当前多模态视觉模型的快速发展以及其本身所具备的在多方面的强大能力，对提升扩散模型组合式生成能力任务上具有强大的作用。当前已经有一些研究利用多模态视觉模型对生成的图像进行检测，判断是否存在生成错误。随后，利用大语言模型给出修改意见和修改模型，利用图像编辑等技术对生成图像进行修改。因此，未来研究可以致力于借助多模态视觉模型和大语言模型的强大能力，着眼于构建一个更加智能、自适应的生成-修正-优化闭环系统，在这个系统中，多模态视觉模型将扮演至关重要的角色，它们不仅负责初步的图像生成与检测，还能通过深度学习技术，不断学习和优化对图像中细微错误或瑕疵的识别能力。同时，大语言模型的作用也不可小觑。它们不仅能够提供基于文本的修改建议，还能理解并生成符合语境的图像描述，进一步指导图像编辑的过程。\n\n# (4) 非空间关系的评估指标建立\n\n当前研究中针对非空间的对象关系评估使用CLIP Score分数来评估，而CLIP分数尽管提供了一个强大的平均语义表示，但在复杂提示文本的任务中，可能并不足以全面捕捉语义信息中存在的交互关系，往往无法准确评估属性与对象之间的绑定关系。因此，未来研究可以致力于针对复杂组合式文本的特点，探索并提出一种更为准确、更为全面的评估方式。这种方式不仅需要能够深入挖掘文本中的语义细节，还需要能够准确反映属性与对象之间的复杂绑定关系。同时，也要意识到，仅仅依靠人工评分来评估这类问题的模型性能是远远不够的。人工评分虽然能够提供一定的参考信息，但其在客观性和准确性方面往往存在一定的局限性。因此，还需要结合其他更为科学、更为客观的评估手段，共同构建一个更加完善、更加可靠的评估体系。\n\n# 参考文献\n\n[1] Sahara C, Chan W, Saxena S, et al. Photorealistic text-to-image diffusion models with deep language understanding[C]//Advances in Neural Information Processing Systems systems, 2022, 35: 36479-36494.  \n[2] Betker J, Goh G, Jing L, et al. Improving image generation with better captions[J]. Computer Science. https://cdn.openai.com/papers/dall-e-3.pdf, 2023, 2 (3): 8.  \n[3] Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.  \n[4] Chen J, Jincheng Y U, Chongjian G E, et al. PixArt- $\\mathbb{S}\\backslash$ alpha  $\\mathbb{S}$ : Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis[C]//The Twelfth International Conference on Learning Representations. 2024.  \n[5] Zheng H, Nie W, Vahdat A, et al. Fast Training of Diffusion Models with Masked Transformers[J]. Transactions on Machine Learning Research. 2024.  \n[6] Peebles W, Xie S. Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4195-4205.  \n[7] Bao F, Nie S, Xue K, et al. All are worth words: A vit backbone for diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 22669-22679.  \n[8] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015: 234-241.  \n[9] Chen J, Ge C, Xie E, et al. FIXTURE: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation[C]/European Conference on Computer Vision. Springer, Cham, 2025: 74-91.  \n[10]Chen J, Luo S, Xie E. FIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models[C]//ICML 2024 Workshop on Theoretical Foundations of Foundation Models.  \n[11] Podell D, English Z, Lacey K, et al. SDXL: Improving Latent Diffusion Models\n\nfor High-Resolution Image Synthesis[C]//The Twelfth International Conference on Learning Representations. 2024.  \n[12]Zhang L, Rao A, Agrawala M. Adding conditional control to text-to-image diffusion models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 3836-3847.  \n[13]Mou C, Wang X, Xie L, et al. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38 (5): 4296-4304.  \n[14] Huang L, Chen D, Liu Y, et al. Composer: creative and controllable image synthesis with composable conditions[C]//Proceedings of the 40th International Conference on Machine Learning. 2023: 13753-13773.  \n[15]Zheng G, Zhou X, Li X, et al. Layoutdiffusion: Controllable diffusion model for layout-to-image generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22490-22499.  \n[16]Li Y, Liu H, Wu Q, et al. Gligen: Open-set grounded text-to-image generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22511-22521.  \n[17]Zhou D, Li Y, Ma F, et al. Migc: Multi-instance generation controller for text-to-image synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6818-6828.  \n[18]Zhou D, Li Y, Ma F, et al. Migc++: Advanced multi-instance generation controller for image synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025, 47(3): 1714 - 1728.  \n[19] Hoe J T, Jiang X, Chan C S, et al. InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6180-6189.  \n[20]Cao P, Zhou F, Song Q, et al. Controllable generation with text-to-image diffusion models: A survey[J]. arXiv preprint arXiv:2403.04279, 2024.  \n[21]Qu L, Wu S, Fei H, et al. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 643-654.  \n[22]Nie W, Liu S, Mardani M, et al. Compositional Text-to-Image Generation with Dense Blob Representations[C]//International Conference on Machine Learning. PMLR, 2024: 38091-38116.\n\n[23] Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural networks from overfitting[J]. The journal of machine learning research, 2014, 15 (1): 1929-1958.  \n[24]Black K, Janner M, Du Y, et al. Training Diffusion Models with Reinforcement Learning[C]/The Twelfth International Conference on Learning Representations. 2024.  \n[25]Huang K, Sun K, Xie E, et al. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation[C]//Advances in Neural Information Processing Systems, 2023, 36: 78723-78747.  \n[26]Lee K, Liu H, Ryu M, et al. Aligning text-to-image models using human feedback[J]. arXiv preprint arXiv:2302.12192, 2023.  \n[27] Xu J, Liu X, Wu Y, et al. Imagereward: Learning and evaluating human preferences for text-to-image generation[C]//Advances in Neural Information Processing Systems, 2024, 36.  \n[28]Mirza M, Osindero S. Conditional Generative Adversarial Nets[J]. arXiv e-prints, 2014: arXiv: 1411.1784.  \n[29] Fan Y, Watkins O, Du Y, et al. Reinforcement learning for fine-tuning text-to-image diffusion models[C]// Advances in Neural Information Processing Systems, 2024, 36.  \n[30]Clark K, Vicol P, Swersky K, et al. Directly Fine-Tuning Diffusion Models on Differentiable Rewards[C]//The Twelfth International Conference on Learning Representations. 2024.  \n[31] Wu X, Hao Y, Zhang M, et al. Deep reward supervisions for tuning text-to-image diffusion models[C]//European Conference on Computer Vision. Springer, Cham, 2025: 108-124.  \n[32] Xu J, Liu X, Wu Y, et al. Imagereward: Learning and evaluating human preferences for text-to-image generation[J]. Advances in Neural Information Processing Systems, 2024, 36.  \n[33] Karthik S, Roth K, Mancini M, et al. If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection[J]. arXiv preprint arXiv:2305.13308, 2023.  \n[34]Lee K, Liu H, Ryu M, et al. Aligning text-to-image models using human feedback[J]. arXiv preprint arXiv:2302.12192, 2023.  \n[35]Liu S, Zeng Z, Ren T, et al. Grounding dino: Marrying dino with grounded pre\n\ntraining for open-set object detection[C]//European Conference on Computer Vision. Springer, Cham, 2025: 38-55.  \n[36]Kirillov A, Mintun E, Ravi N, et al. Segment anything[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4015-4026.  \n[37]Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[C]//International conference on machine learning. PMLR, 2022: 12888-12900.  \n[38]Li J, Li D, Savarese S, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models[C]//International conference on machine learning. PMLR, 2023: 19730-19742.  \n[39]Touvron H, Lavril T, Izacard G, et al. LLaMA: open and efficient foundation language models. arXiv[J]. arXiv preprint arXiv:2302.13971, 2023.  \n[40] Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report[J]. arXiv preprint arXiv:2303.08774, 2023.  \n[41] Jiang D, Song G, Wu X, et al. Comat: Aligning text-to-image diffusion model with image-to-text concept matching[C]// Advances in Neural Information Processing Systems, 2024, 37: 76177-76209.  \n[42]Hu E J, Wallis P, Allen-Zhu Z, et al. LoRA: Low-Rank Adaptation of Large Language Models[C]//International Conference on Learning Representations. 2024.  \n[43] Sun J, Fu D, Hu Y, et al. Dreamsync: Aligning text-to-image generation with image understanding feedback[C]//Synthetic Data for Computer Vision Workshop@CVPR 2024. 2023.  \n[44]Anil R, Dai A M, First O, et al. Palm 2 technical report[J]. arXiv preprint arXiv:2305.10403, 2023.  \n[45] Ke J, Ye K, Yu J, et al. Vila: Learning image aesthetics from user comments with vision-language pretraining[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10041-10051.  \n[46]Rassin R, Hirsch E, Glickman D, et al. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment[C]// Advances in Neural Information Processing Systems, 2024, 36.  \n[47]Liu L, Zhang Z, Ren Y, et al. Detector guidance for multi-object text-to-image generation[J]. arXiv preprint arXiv:2306.02236, 2023.  \n[48]Feng W, He X, Fu T J, et al. Training-Free Structured Diffusion Guidance for\n\nCompositional Text-to-Image Synthesis[C]//The Eleventh International Conference on Learning Representations. 2023.  \n[49]Manas O, Astolfi P, Hall M, et al. Improving text-to-image consistency via automatic prompt optimization[J]. arXiv preprint arXiv:2403.17804, 2024.  \n[50]Hessel J, Holtzman A, Forbes M, et al. CLIPScore: A Reference-free Evaluation Metric for Image Captioning[C]//EMNLP (1). 2021.  \n[51]Cho J, Hu Y, Baldridge J M, et al. Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation[C]//The Twelfth International Conference on Learning Representations. 2024.  \n[52] Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. Journal of machine learning research, 2020, 21 (140): 1-67.  \n[53]Hertz A, Mokady R, Tenenbaum J, et al. Prompt-to-Prompt Image Editing with Cross-Attention Control[C]//The Eleventh International Conference on Learning Representations. 2023.  \n[54]Chefer H, Alaluf Y, Vinker Y, et al. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models[J]. ACM Transactions on Graphics (TOG), 2023, 42 (4): 1-10.  \n[55]Li Y, Keuper M, Zhang D, et al. Divide & Bind Your Attention for Improved Generative Semantic Nursing[C]//BMVC. 2023.  \n[56]Agarwal A, Karanam S, Joseph K J, et al. A-star: Test-time attention segregation and retention for text-to-image synthesis[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 2283-2293.  \n[57]Bao Z, Li Y, Singh K K, et al. Separate-and-Enhance: Compositional Finetuning for Text-to-Image Diffusion Models[C]//ACM SIGGRAPH 2024 Conference Papers. 2024: 1-10.  \n[58]Meral TH S, Simsar E, Tombari F, et al. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 9005-9014.  \n[59]Sueyoshi K, Matsubara T. Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8651-8660.  \n[60]Kim Y, Lee J, Kim J H, et al. Dense text-to-image generation with attention modulation[C]//Proceedings of the IEEE/CVF International Conference on\n\nComputer Vision. 2023: 7701-7711.  \n[61] Balaji Y, Nah S, Huang X, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers[J]. arXiv preprint arXiv:2211.01324, 2022.  \n[62] Wu Q, Liu Y, Zhao H, et al. Harnessing the spatial-temporal attention of diffusion models for high-fidelity text-to-image synthesis[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7766-7776.  \n[63]Lian L, Li B, Yala A, et al. LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models[J]. Transactions on Machine Learning Research. 2024.  \n[64] Wu Y, Cao X, Li K, et al. Towards Better Text-to-Image Generation Alignment via Attention Modulation[J]. arXiv preprint arXiv:2404.13899, 2024.  \n[65]Gong B, Huang S, Feng Y, et al. Check Locate Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6624-6634.  \n[66]Chen M, Laina I, Vedaldi A. Training-free layout control with cross-attention guidance[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024: 5343-5353.  \n[67]Zhang X, Yang L, Cai Y, et al. Realcompo: Dynamic equilibrium between realism and compositionality improves text-to-image diffusion models[J]. arXiv preprint arXiv:2402.12908, 2024.  \n[68] Xie J, Li Y, Huang Y, et al. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7452-7461.  \n[69]Phung Q, Ge S, Huang J B. Grounded text-to-image synthesis with attention refocusing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7932-7942.  \n[70]Dahary O, Patashnik O, Aberman K, et al. Be yourself: Bounded attention for multi-subject text-to-image generation[C]//European Conference on Computer Vision. Springer, Cham, 2025: 432-448.  \n[71] Wang R, Chen Z, Chen C, et al. Compositional text-to-image synthesis with attention map control of diffusion models[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38 (6): 5544-5552.  \n[72]Guo X, Liu J, Cui M, et al. Initno: Boosting text-to-image diffusion models via\n\ninitial noise optimization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 9380-9389.  \n[73] Liu N, Li S, Du Y, et al. Compositional visual generation with composable diffusion models[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 423-439.  \n[74] Zhu J, Ma H, Chen J, et al. Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance[J]. IEEE Transactions on Visualization and Computer Graphics, 2024.  \n[75]Shirakawa T, Uchida S. NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8921-8930.  \n[76]Wang Z, Xie E, Li A, et al. Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation[J]. arXiv preprint arXiv:2401.15688, 2024.  \n[77]Jia Y, Tan W. Divcon: Divide and conquer for progressive text-to-image generation[J]. arXiv preprint arXiv:2403.06400, 2024.  \n[78] Ye Y T, Cai J, Zhou H, et al. Progressive Text-to-Image Diffusion with Soft Latent Direction[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38 (7): 6693-6701.  \n[79] Wu T H, Lian L, Gonzalez J E, et al. Self-correcting llm-controlled diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6327-6336.  \n[80]Li S, Wang R, Hsieh C J, et al. MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion[J]. arXiv preprint arXiv:2402.12741, 2024.  \n[81] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[C]// Advances in Neural Information Processing Systems, 2020, 33: 6840-6851.  \n[82]Song J, Meng C, Ermon S. Denoising Diffusion Implicit Models[C]//International Conference on Learning Representations, 2021, 8553—8564.  \n[83] Song Y, Sohl-Dickstein J, Kingma D P, et al. Score-Based Generative Modeling through Stochastic Differential Equations[C]//International Conference on Learning Representations, 2021.  \n[84] Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[C]// Advances in Neural Information Processing Systems, 2021, 34: 8780-8794.  \n[85] Ho J, Salimans T. Classifier-free diffusion guidance[J]. arXiv preprint\n\narXiv:2207.12598, 2022.  \n[86] Nichol A Q, Dhariwal P, Ramesh A, et al. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models[C]//International Conference on Machine Learning. PMLR, 2022: 16784-16804.  \n[87] Ramesh A, Dhariwal P, Nichol A, et al. Hierarchical text-conditional image generation with clip latents[J]. arXiv preprint arXiv:2204.06125, 2022, 1 (2): 3.  \n[88]Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PmLR, 2021: 8748-8763.  \n[89]Vaswani A. Attention is all you need[C]//Advances in Neural Information Processing Systems, 2017.  \n[90]Z. Wang, Z. Sha, Z. Ding, Y. Wang, and Z. Tu, Tokencompose: Text-to-image diffusion with token-level supervision[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2024, pp. 8553-8564.  \n[91]Jyothi A A, Durand T, He J, et al. Layoutvae: Stochastic scene layout generation from a label set[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 9895-9904.  \n[92]Lee H Y, Jiang L, Essa I, et al. Neural design network: Graphic layout generation with constraints[C]//Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16. Springer International Publishing, 2020: 491-506.  \n[93]Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[C]// International Conference on Learning Representations. 2017.  \n[94]Johnson J, Gupta A, Fei-Fei L. Image generation from scene graphs[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 1219-1228.  \n[95]Herzig R, Bar A, Xu H, et al. Learning canonical representations for scene graph to image generation[C]//Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXVI 16. Springer International Publishing, 2020: 210-227.  \n[96]Tan F, Feng S, Ordonez V. Text2scene: Generating compositional scenes from textual descriptions[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 6710-6719.\n\n[97] Yang C F, Fan W C, Yang F E, et al. Layouttransformer: Scene layout generation with conceptual and spatial diversity[C]/Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 3732-3741.  \n[98] Lv T, Huang Y, Chen J, et al. Kosmos-2.5: A multimodal literate model[J]. arXiv preprint arXiv:2309.11419, 2023.  \n[99] Zhu D, Chen J, Shen X, et al. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models[C]//The Twelfth International Conference on Learning Representations. 2024.  \n[100] WANG L, XU W, LAN Y, et al. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models[C]. ACL, 2023.  \n[101] Chen X, Liu Y, Yang Y, et al. Reason out your layout: Evoking the layout master from large language models for text-to-image synthesis[J]. arXiv preprint arXiv:2311.17126, 2023.  \n[102] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[C]// Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.  \n[103] Zhang Z, Zhang A, Li M, et al. Automatic Chain of Thought Prompting in Large Language Models[C]//The Eleventh International Conference on Learning Representations. 2023.  \n[104] Li C, Liang J, Zeng A, et al. Chain of Code: Reasoning with a Language Model-Augmented Code Emulator[C]//International Conference on Machine Learning. PMLR, 2024: 28259-28277.  \n[105] Salimans T, Goodfellow I, Zaremba W, et al. Improved techniques for training gans[J]. Advances in neural information processing systems, 2016, 29.  \n[106] Heusel M, Ramsauer H, Unterthiner T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[C]//Advances in Neural Information Processing Systems, 2017, 30.  \n[107] Zhou X, Koltun V, Krahenbuhl P. Simple multi-dataset detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 7571-7580.  \n[108] Grimal P, Le Borgne H, Ferret O, et al. TIAM-A metric for evaluating alignment in Text-to-Image generation[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024: 2890-2899.  \n[109] Floridi L, Chiriatti M. GPT-3: Its nature, scope, limits, and consequences[J].\n\nMinds and Machines, 2020, 30: 681-694.  \n[110] Liu X, Park D H, Azadi S, et al. More control for free! image synthesis with semantic diffusion guidance[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023: 289-299.  \n[111] Bakr E M, Sun P, Shen X, et al. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 20041-20053.  \n[112] Singh J, Zheng L. Divide, evaluate, and refine: Evaluating and improving text-to-image alignment with iterative vqa feedback[C]//Advances in Neural Information Processing Systems, 2023, 36: 70799-70811.  \n[113] Liu B, Wang C, Cao T, et al. Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7817-7826.",
    "chunked": true,
    "vectorizeStatus": "success",
    "chunkCount": 114
  },
  {
    "id": "48e21e36-6c6f-47b8-b1ed-d4d9414b44c7",
    "title": "基于预训练模型的AI绘画生成建筑图像的方法优化研究_刘可萌",
    "fileName": "基于预训练模型的AI绘画生成建筑图像的方法优化研究_刘可萌.pdf",
    "fileType": "pdf",
    "fileSize": 14220956,
    "uploadTime": "2025-12-29T21:53:54.467032",
    "parsed": true,
    "parseStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251229215354_基于预训练模型的AI绘画生成建筑图像的方法优化研究_刘可萌.pdf",
    "markdownContent": "# 专业硕士学位论文\n\n# 基于预训练模型的AI 绘画生成建筑图像的方法优化研究\n\n硕士研究生：刘可萌  \n导师姓名：张文辉副教授  \n合作导师：宿天斌高级工程师  \n申请学位级别：专业学位硕士  \n专业类别、领域：建筑学  \n所在单位：建筑与城乡规划学院  \n答辩日期：2025年5月  \n学位授予单位：青岛理工大学\n\n# 专业硕士学位论文\n\n# 基于预训练模型的\n\n# AI绘画生成建筑图像的方法优化研究\n\n硕士研究生：刘可萌  \n导师姓名：张文辉副教授  \n合作导师：宿天斌高级工程师  \n申请学位级别：专业学位硕士  \n专业类别、领域：建筑学  \n所在单位：建筑与城乡规划学院  \n答辩日期：2025年5月  \n学位授予单位：青岛理工大学\n\n# Dissertation for the Master Degree in Architecture\n\n# Optimization of architectural image generation by AI painting Based on pre-trained models\n\nCandidate: Liu kemeng   \nSupervisor: Prof. Zhang wenhui   \nCo-supervisor : SE.Sutianbin   \nAcademic Degree Applied for: MasterofArchitectureProfessional   \nSpecialty: Architecture CollegeofArchitectureandUrban   \nAffiliation : Planning   \nDate of Defence: May 2025   \nUniversity: Qingdao University of Technology\n\n# 专业硕士学位论文\n\n基于预训练模型的AI绘画生成建筑图像的方法优化研究\n\n学位论文答辩日期：20528\n\n指导教师签字：\n\n答辩委员会成员签字：\n\n# 目录\n\n第1章绪论..  \n1.1 研究选题背景.  \n1.2 相关理论基础. .2  \n1.2.1 AI绘画. .2  \n1.2.2 机器学习. .3  \n1.2.3 建筑形体空间在图像生成中的核心地位！ ..5  \n1.2.4目前的建筑形体空间生成方法. ..7  \n1.3 国内外相关研究与应用综述 ..9  \n1.3.1 国外相关研究与应用.. ..9  \n1.3.2 国内相关研究与应用， ..11  \n1.4 研究的内容、目的与意义， ..12  \n1.4.1 研究内容.. ..12  \n1.4.2 研究的目的与意义. ..13  \n1.5 研究方法与框架. ....14  \n1.5.1 研究方法. ...14  \n1.5.2研究框架 ..15  \n第2章 AI绘画生成建筑图像方法的主要原理和当前存在的问题...16  \n2.1AI绘画中文生图的主要原理 ..16  \n2.1.1 深度学习.. ....7  \n2.1.2 大规模图像数据集. ..18  \n2.1.3 文本-图像多模态预训练模型 ..19  \n2.2 文生图模型算法的分类. ························ ....22  \n2.2.1 基于GAN 的以文生图模型. ..22  \n2.2.2 基于 VAE 和 PLM 的以文生图模型. ..23  \n2.2.3 基于扩散模型的以文生图模型 ..23  \n2.2.4 扩散模型和其他模型的对比. .25  \n2.3AI绘画平台的比较分析. ..26  \n2.4目前使用AI绘画生成建筑图像的常用方法与不足 ..27  \n2.5 概括与总结.. ..29  \n第3章 AI 绘画生成建筑图像方法的优化的思路和原理， ..31  \n3.1 生成步骤的优化.. ..31  \n3.1.1 提示词框架的梳理. .31  \n3.1.2 文本内容的调整 ..31  \n3.2 数据库的更新：建筑形体空间的分类 .33\n\n# 3.2.1 点状空间类型 .33\n\n3.2.2 线性空间类型. ...3.4  \n3.2.3 面的延伸形成的空间类型. ....34  \n3.2.4 体的变化形成的空间类型. ·.··.···············.·············· ..35  \n3.3 预训练模型：图像-文本的对应 .38  \n3.4 本章小结. ..40  \n第4章 AI 绘画生成建筑图像方法的优化的方式和步骤 ..41  \n4.1 图像生成平台和模型训练工具选择的选择 ..41  \n4.1.1 本研究 AI 绘画平台的选择 (Stable Diffusion) ..41  \n4.1.2 Stable Diffusion 中大模型(Checkpoint)与特征模型(Lora)的关  \n系与特点... ...42  \n4.1.3 模型训练工具选的.. ....44  \n4.2 模型数据库的建设. ...45  \n4.2.1 预训练模型的类型选择.. ...4.5  \n4.2.2 主要参数分析. ...45  \n4.2.3Dreambooth 图像和文本内容的输入. ..48  \n4.3预训练模型图像生成测试与调整 ..56  \n4.3.1建筑形体空间类型&建筑类别/性格同时输入大模型  \n(checkpoint)时的图像生成测试.. ....56  \n4.3.2预训练模型类型的调整.. ...69  \n4.3.3Lora 文本内容的输入(以幼儿园类建筑为例) ...60  \n4.3.4建筑形体空间类型(checkpoint)&建筑类型/性格(Lora)图像生  \n成测试... ...62  \n4.4 本章小结 ..68  \n第5章AI绘画生成建筑图像方法的优化结果 ..69  \n5.1优化后的建筑图像生成方法归纳 ..69  \n5.2 优化后的建筑图像生成方法在案例中的快速设计应用举例...73  \n5.2.1 项目背景与设计需求.. ....73  \n5.2.2 项目设计要素的提取.. ....73  \n5.2.3 使用优化后的建筑图像生成方法进行建筑项目的快速设计  \n过程 ...7.4  \n第6章总结与展望 ······································· ..83  \n6.1 主要研究结论. ..83  \n6.2 主要创新点.. ...83  \n6.2.1人机交的设计互模式下,更符合建筑师思维的生成步骤....83  \n6.2.2快速、定向的文字生成建筑图像的方法 ...8.4  \n6.2.3 建筑教学指导和实践参考.. ...84  \n6.3 未来展望与适应性分... .84\n\n参考文献.. ..87\n\n# 摘 要\n\n近年来，随着AIGC（artificial intelligence generated content，人工智能生成内容）的高速发展，AI绘画成为其主要应用场景之一。AI绘画是一种利用人工智能算法来生成或转化图片的过程，其中以文生图是当下一种主流的图片生成方式。\n\n目前国内外许多研究机构和团队都已经在AI 绘画领域开展了研究，取得了不俗的研究成果。AI绘画作为强大的图像生成工具，短时间能生成大量高质量、富有想象力的精美图像，但这一优势在建筑设计方面的应用却有些不尽人意。本文总结了当下建筑领域使用AI绘画生成建筑图像的主要方式，发现当使用AI绘画来生成建筑图像时，大数据集中对建筑空间形态过于泛化的认知、以及对建筑类别特征的覆盖不足，使得使用者在文生图时极难用自然语言来描述清楚建筑形体和空间以及风格上的特征，经常导致无法生成预期作品的问题，限制了文生图在建筑领域的应用。本文试按照建筑师惯用的设计思维，整理出了一套适合建筑师使用的、快速的、生成方向较为明确的AI绘画生成建筑图像的方法，来对现有方法进行优化。该方法优先生成建筑的“形体空间”，而后加入建筑“类别或性格”特征，最后进行“材质、构件”等的细节补充。为了支持该方法的实现，本文通过“点、线、面、体”的空间变化规律对建筑的空间形态进行了划分，来完善“形体空间”部分的数据库，通过对建筑类别如“教育建筑、工业建筑”等以及特定风格建筑的数据补充来允许建筑骨架中指定建筑特点的加入。这套方法基于预训练模型，通过文中大量的模型训练及参数调节获得实现。后续本文对优化后的建筑图像生成的方法进行了归纳，并在一个假定的建筑项目中进行了快速设计的应用，提供了一种实践的参考思路。在优化后的结果中，建筑师可以通过较为简短的描述，快速获得多样性的、符合预期方向的建筑图像。这种建筑图像生成方法在人机交互的设计模式下，为建筑师提供了更符合思维习惯的建筑图像生成路径，在建筑的教学指导和实践参考中也有实际性的意义。在文章的结尾，对方法的未来及适应性进行了分析。\n\n关键词：建筑设计方法；AI辅助设计；建筑形体空间；文生图\n\n# Abstract\n\nIn recent years，with the rapid development of AIGC (artificial intelligence generated content),AI painting has become one of its main application scenarios. AI painting is a process that uses artificial intelligence algorithms to generate or transform pictures, among which Vincennes is a mainstream image generation method.\n\nAt present, many research institutions and teams at home and abroad have carried out research in the field of AI painting, and achieved good research results. As a powerful image generation tool, AI painting can generate a large number of high-quality, imaginative and beautiful images in a short time, but this advantage is not satisfactory in the application of architectural design. This paper summarizes the main ways that AI painting is used to generate architectural images in the current architectural field. It is found that when AI painting is used to generate architectural images, it is extremely difficult for users to use natural language to describe clearly the architectural form, space and style features in Vincennes drawings because of the over-generalized cognition of architectural spatial forms in big data sets and the insufficient coverage of architectural category features. Problems that often lead to the failure to produce the intended work limit the application of Vincennes diagrams in the field of architecture. This paper tries to optimize the existing methods by sorting out a set of methods suitable for architects to use,which is fast and has a clear direction to generate architectural images by AI painting according to the usual design thinking of architects. This method gives priority to the \"physical space\" of the building, then adds the \"category or character\" of the building, and finally adds the details of \"materials and components\". In order to support the realization of this method, this paper divides the spatial form of buildings by the spatial change law of \"point, line, surface and body\" to improve the database of the \"physical space\" part, and allows the addition of specified architectural features in the building skeleton by supplementing the data of building categories such as \"educational buildings, industrial buildings\" and buildings of specific styles. This method is based on the pre-training model and is realized through a lot of model training and parameter adjustment in this paper. In the following paper, the method of building image generation after optimization is summarized, and the rapid design application is carried out in a hypothetical building project, providing a practical reference idea. In the optimized result, architects can quickly obtain a variety of architectural images in line with the expected direction through relatively short descriptions. Under the design mode of human-computer interaction, this method of building image generation provides architects with a path of building image generation more in line with their thinking habits,and also has practical significance in architectural teaching guidance and practical reference. At the end of the paper, the future and adaptability of the method are analyzed.\n\nKey words: Architectural design method; Ai-assisted design; Architectural form space; Vincent diagram\n\n# 第1章 绪论\n\n# 1.1研究选题背景\n\n作为人类最古老的艺术形式之一，绘画在人工智能技术的推动下衍生出创新性创作范式——AI绘画。该技术体系基于深度学习算法与生成对抗网络（GANs），通过语义解析、风格迁移及图像合成等核心模块，实现文本描述向视觉表征的跨模态转换，或对原始图像进行风格重构、智能修复与色彩增强等操作。相较于传统绘画所依赖的艺术直觉、技法经验与情感表达，AI绘画依赖于算法和深度学习技术，其创作过程更加理性，作品风格呈现出统一性、规范性和多样性等特点，作品价值更多体现在技术创新、数据质量、算法水平和算力消耗等方面。作为人工智能生成内容（Artificial Intelligence Generated Content,AIGC）的重要应用分支，当前AI绘画已展现出多维度的技术赋能潜力。\n\n在艺术创作领域，AI绘画技术通过自然语言作为计算机语言的替代极大降低了专业创作的门槛，使非专业人士能够通过文字描述快速生成具有特定美学风格的作品。在企业应用层面，企业通过AI绘画工具实现图像生产效率的提升，显著降低视觉内容制作的成本。更值得关注的是，AI绘画在某些领域展现出来的巨大生产效益迅速促使各行业展开了相关研应用研究，在医学影像领域，AI绘画技术可辅助病灶识别与病理可视化；在工业设计领域，可支持参数化原型生成与多模态效果图迭代；在建筑领域，可帮助实现建筑方案的生成与优化，模拟建筑环境和光照等。这些应用实践验证了AI绘画作为通用性技术的跨领域适配能力。\n\nAI 绘画技术在建筑设计领域的研究与应用正逐渐深入，展现出其在创意设计、效率提升和决策优化方面的巨大潜力，但总体仍然处于探索阶段。\n\nAI 绘画技术与建筑设计的结合，尝试推动设计流程的自动化和智能化。通过使用神经网络、卷积神经网络(CNN)、循环神经网络(RNN)等技术，AI可以处理大量的设计数据，提供创新设计方案。StableDiffusion 等AI绘画平台被应用于建筑设计，帮助设计师快速将黑白线稿转换成不同风格的彩色效果图，为建筑设计师提供多角度的创意表达。一些建筑项目已经开始使用AI绘画技术进行方案设计和效果展示，证明了AI绘画在实际工作流程中的确能带来巨大的经济效益。另外，一部分研究者通过AI辅助绘画进行空间设计，探索 AI在激发设计创意、提供直观空间视觉创新、缩短设计周期等方面的优势。\n\n尽管诸多研究和实践已经尝试将AI绘画应用于建筑设计，但还是主要集中在将线稿或草模转化为效果图和智能数据集上，在现有的AI绘画平台（如Midjourney、Stable diffusion）中，其图像生成流程与建筑师的惯用设计表现出极低的契合性，在数据的覆盖中也仅有对常见建筑风格（如日式、中式、中世纪等）的描述，关于建筑空间关系的控制词，并没有相对吻合的对应关系。在建筑的形体与空间生成阶段，关于AI绘画的应用研究也较少。如何利用 AI的学习与创新能力，生成具有创新性和实用性的建筑形体空间，也是一个值得深入研究的问题。\n\n# 1.2 相关理论基础\n\n# 1.2.1 AI绘画\n\nAI 绘画是基于深度学习技术的图像生成方法，其核心是通过算法解析艺术作品的特征规律，实现从数据学习到创意输出的转化。该技术的演进主要依托三个关键技术突破：生成对抗网络（GAN）、图文预训练模型和扩散模型，分别对应风格迁移、语义理解和生成优化三个维度。\n\n在风格迁移技术方面，GAN 通过生成器与判别器的对抗训练机制实现艺术特征提取。生成器负责将随机噪声映射为图像空间，判别器则评估生成图像与真实艺术作品的分布差异。经过迭代优化，系统能够捕捉特定艺术家的笔触特征与色彩风格，相较于传统基于物理建模的图形学方法，这种数据驱动范式更擅长处理非结构化艺术特征。\n\n图文预训练模型的突破提升了AI绘画的语义控制能力。以CLIP（ContrastiveLanguage-Image Pre-training）模型为例，该技术通过构建跨模态嵌入空间，建立文本描述与视觉特征的语义关联。这种对齐机制允许用户通过自然语言指令精确控制画面元素，例如输入\"水墨风格的未来城市\"，系统能有效融合传统技法与现代主题。\n\n扩散模型为生成质量提供了新的数学框架。其技术路径包含前向加噪与逆向去噪两个过程：首先通过马尔可夫链逐步向图像添加高斯噪声，随后训练神经网络学习去噪过程中的数据分布梯度。这种方法在保持生成多样性的同时，显著提升了图像分辨率与细节真实度，目前已成为生成4K级艺术图像的主流方案。\n\n# 1.2.2机器学习\n\n机器学习作为人工智能的核心实现路径，旨在通过数据驱动的算法模型，赋予计算机系统从经验中自主归纳规律并优化决策的能力。其理论框架建立在统计学、信息论与计算复杂性理论的交叉基础上，以“从数据中学习”为核心理念，突破了传统程序设计中显式规则编码的局限。自20世纪中叶图灵提出“学习机器”概念以来，机器学习历经符号主义、连接主义与统计学习等范式的迭代，逐步形成涵盖监督学习、无监督学习与强化学习的完整方法论体系。这一技术范式的演进不仅重塑了计算机科学的学科边界，更深刻影响了自然科学、社会科学与工程实践的认知方式。\n\n机器学习的理论建构始于对“学习”本质的形式化定义。Valiant 提出的概率近似正确（PAC）学习框架[，将学习过程抽象为假设空间搜索问题：给定独立同分布的训练样本集，算法需在多项式时间内找到与目标概念近似一致的假设，其泛化误差以概率形式有界。这一框架为学习算法的收敛性提供了理论保证，同时也揭示了模型复杂度与泛化能力间的权衡关系——VC 维理论通过量化假设空间的表达能力，严格证明了经验风险最小化原则下，测试误差与训练误差的偏离程度随VC 维增加而上升。偏差-方差分解定理进一步解构了泛化误差的组成：偏差度量模型对真实关系的近似误差，方差反映模型对训练数据扰动的敏感性，噪声则源于数据内在的不确定性。该定理为模型选择提供了指导原则：简单模型（如线性回归）因高偏差易出现欠拟合，复杂模型（如深度神经网络）则因高方差导致过拟合。正则化技术的引入通过约束参数空间，实现了偏差与方差的动态平衡，其数学本质是在损失函数中引入模型复杂度的显式度量。\n\n机器学习的实践体系围绕三大学习范式展开，其方法创新始终遵循“表示-优化-评估”的认知逻辑。监督学习的核心任务是通过输入-输出对的映射关系，构建预测模型。线性模型通过特征空间的超平面划分类别，其非线性扩展借助核方法将数据映射至高维再生核希尔伯特空间，解决了异或问题的线性不可分性。决策树算法采用信息增益或基尼不纯度递归划分特征空间，而随机森林通过Bootstrap 抽样与特征子集随机选择构建多样性基分类器，显著提升了模型鲁棒性。\n\n支持向量机则以结构风险最小化为准则，通过核技巧与软间隔优化，实现了对高维稀疏数据的高效分类。当标注信息缺失时，无监督学习致力于揭示数据内在的分布特性。聚类算法通过样本相似性度量实现群体划分，其有效性依赖于距离函数的几何假设。主成分分析通过协方差矩阵的特征分解获取数据的主正交方向，实现维度约简与去相关性；流形学习算法则通过保留局部邻域结构，将高维数据投影至低维可视空间。生成对抗网络通过判别器与生成器的博弈训练，学习真实数据分布，为无监督生成任务开辟了新路径。强化学习框架将学习过程建模为马尔可夫决策过程，智能体通过与环境交互获取奖励信号，优化策略以最大化累积回报。Q-learning 算法基于贝尔曼方程迭代更新状态-动作价值函数，深度Q网络通过经验回放与目标网络稳定了函数逼近过程。策略梯度方法直接参数化策略函数，适用于连续动作空间问题，而Actor-Critic架构[2通过分离价值评估与策略改进，提升了学习效率。\n\n机器学习的方法创新始终与实际问题需求紧密互动，其应用疆域已从传统模式识别扩展至复杂系统建模。在自然语言处理领域，词嵌入技术通过分布式假设将语义映射至低维向量空间，注意力机制则通过动态权重分配解决了长距离依赖建模难题。计算机视觉中，卷积神经网络的局部感受野与平移不变性特性，使其成为图像分类、目标检测的核心架构。在生物信息学领域，隐马尔可夫模型用于基因序列分析，图神经网络通过消息传递机制建模蛋白质相互作用网络。工业界的实践进一步推动了方法论的演进：联邦学习通过分布式模型训练保护数据隐私，元学习赋予模型快速适应新任务的能力，自动化机器学习通过神经架构搜索与超参数优化，降低了算法设计的人力成本。这些技术革新不仅验证了机器学习的工程价值，也反向促进了理论模型的完善。\n\n尽管机器学习取得了显著成就，其发展仍受限于若干本质性约束。监督学习的性能高度依赖标注数据的规模与质量，小样本场景下模型易陷入过拟合。半监督学习通过未标注数据增强表示学习，迁移学习借助领域适应技术实现知识迁移，但这些方法仍无法完全克服数据分布的偏移问题。无监督学习虽降低了对标注的依赖，但其学习目标的主观设定引入了隐性归纳偏置。黑箱模型的决策过程缺乏透明度，制约了其在医疗、司法等高风险领域的应用。局部可解释方法通过线性代理模型近似预测逻辑，全局解释技术揭示变量间的统计关联，但这些方法均未触及因果机制的本质。因果推断框架尝试区分相关性与因果性，但其与机器学习模型的深度融合仍处于探索阶段。数据偏见可能通过反馈循环放大社会不平等，生成模型滥用导致虚假信息传播风险加剧。公平性约束虽可缓解算法歧视，但往往以牺牲模型性能为代价。隐私保护技术通过噪声注入限制信息泄露，但需在隐私保障与模型效用间寻求平衡。\n\n机器学习的持续突破需超越算法优化的单一维度，实现跨学科知识体系的深度整合。脉冲神经网络通过模拟生物神经元的时序编码机制，为低功耗类脑计算提供新范式；认知架构的研究可能启发更具通用性的推理模型。哈密顿神经网络通过引入能量守恒约束，提升了动力系统预测的精度；微分方程与深度学习的结合实现了连续时间动态建模。神经符号计算尝试将逻辑规则嵌入神经网络训练过程，例如通过可微推理层实现知识引导的表示学习。与此同时，基础理论的深化将推动范式的根本性变革：在线学习理论关注动态环境下的模型适应性，贝叶斯非参数方法为模型复杂度自主调节提供理论支撑，量子机器学习探索叠加态并行计算对优化过程的加速潜力。\n\n机器学习的发展历程印证了“实践驱动理论进化，理论指导实践创新”的辩证关系。从早期的感知机到现代的大语言模型，方法论的每次跃迁都源于对复杂现实问题的响应与抽象。当前，机器学习正处于从专用智能向通用智能过渡的关键阶段，其突破不仅需要算法层面的创新，更依赖于数学工具、计算范式与认知科学的协同突破。未来的研究需直面可解释性、因果性与伦理对齐等本质问题，构建兼具数据驱动灵活性与逻辑推理严谨性的新一代智能系统。这一目标既是对技术极限的挑战，亦是对人类认知本质的深层探索。\n\n# 1.2.3 建筑形体空间在图像生成中的核心地位\n\n建筑形体空间生成是建筑创作流程中的核心环节，它不仅是设计理念的物质化表达，更是功能需求、环境响应与文化意义的综合载体。在建筑学的理论与实践演进中，形体空间的生成始终扮演着桥梁角色，连接着抽象的概念构思与具体的建造实施。\n\n建筑形体空间生成是设计思维从抽象到具象转化的关键节点。在概念设计阶段，建筑师通过草图、模型或数字工具将初始构思转化为空间原型，这一过程既是对设计理念的初步验证，也是对功能需求与环境条件的初步回应。形体空间的生成并非简单的几何操作，而是基于逻辑推理与创造性思维的复杂过程。例如，扎哈·哈迪德的设计实践中，参数化工具被用于生成流动的曲面形态，这些形态不仅是对传统几何学的突破，更是对动态空间体验的探索。\n\n形体空间生成的方法论演进反映了建筑学与相关学科的交叉融合。从古典建筑的比例法则到现代建筑的模数化设计，从手工模型到数字生成算法，形体空间的生成工具与技术不断革新，但其核心目标始终未变：即通过空间的组织与形态的塑造，实现设计意图的清晰表达。参数化设计与生成式设计的兴起，进一步拓展了形体空间生成的边界，使建筑师能够处理更复杂的几何关系与环境变量，从而创造出更具适应性与表现力的空间形态。\n\n建筑形体空间生成是功能需求转化为空间布局的核心环节。功能分区、流线组织与空间尺度等基本问题，均需通过形体空间的生成得以解决。例如，在公共建筑设计中，如何通过空间形态的划分与连接，实现人群的高效流动与活动的合理分布，是形体空间生成的核心任务之一。库哈斯的央视大楼通过环形结构的空间生成，不仅满足了媒体生产的复杂功能需求，还创造了独特的空间体验。\n\n形体空间的生成还需回应使用者的行为模式与心理需求。空间的开合、光线的引入、材料的质感等要素，均通过形体设计得以实现。例如，路易斯·康的金贝尔美术馆通过拱顶形态与天窗设计，将自然光引入展厅，既满足了展览功能的需求，又创造了静谧而神圣的空间氛围。这种对功能与体验的双重关注，体现了形体空间生成在建筑设计中的综合价值。\n\n建筑形体空间生成是回应环境约束与利用环境资源的重要手段。在地形、气候、植被等自然条件的约束下，形体设计需通过适应性策略实现与环境的和谐共生。例如，赖特的流水别墅通过水平延伸的形体与悬挑结构，将建筑融入山石与溪流的自然景观中，实现了人工与自然的无缝衔接。\n\n在城市化背景下，形体空间生成还需回应城市肌理与文脉特征。通过尺度、比例与形态的协调，建筑既可与周边环境形成对话，又可塑造独特的场所精神。例如，贝聿铭的卢浮宫金字塔通过简洁的几何形态与透明的材料选择，既尊重了历史建筑的庄严感，又为地下空间引入了自然光，实现了新旧元素的和谐共存。可持续设计理念的兴起进一步凸显了形体空间生成的环境价值。通过被动式设计策略（如自然通风、遮阳系统）与主动式技术（如光伏集成、雨水收集）的结合，形体设计可显著降低建筑的能耗与环境负荷。例如，福斯特事务所的苹果总部通过环形布局与屋顶光伏阵列，实现了能源自给与生态友好的设计目标。\n\n建筑形体空间生成是文化意义与美学价值的物质载体。通过形态、材料与构造的选择，建筑不仅可传达特定的文化符号，还可塑造独特的空间体验。例如，安藤忠雄的光之教堂通过极简的混凝土墙体与十字形光缝，将宗教精神转化为空间语言，创造了极具震撼力的精神场所。在全球化背景下，形体空间生成成为地域文化与现代技术融合的试验场。通过传统建筑元素的抽象化与重构，建筑师可在现代语境中重新诠释文化身份。例如，王澍的宁波博物馆通过瓦片墙与传统坡屋顶的现代演绎，既回应了地域文化记忆，又展现了当代建筑的创新精神。形体空间生成还承载着社会价值与公共性表达。通过开放空间、界面设计与流线组织，建筑可促进社会互动与公共生活的发生。例如，OMA的波尔图音乐厅通过通透的外墙设计与开放的公共空间，将建筑转化为城市的文化枢纽，激发了社区的活力。随着数字技术与可持续理念的深入发展，建筑形体空间生成将面临新的机遇与挑战。参数化设计与人工智能的结合，可能催生更高效、更智能的生成工具；可持续设计理念的深化，将推动形体空间生成向生态化与低碳化方向发展；跨学科合作（如与生物学、材料科学的融合）可能为形体设计带来新的灵感与可能性。然而，形体空间生成的核心价值始终未变：即通过空间的组织与形态的塑造，实现功能、环境与文化的综合表达。未来的建筑创作需在技术创新与人文关怀之间寻求平衡，使形体空间生成不仅成为技术探索的前沿，更成为文化传承与社会进步的载体。\n\n建筑形体空间生成在建筑创作流程中占据核心地位，它不仅是设计理念的物质化表达，更是功能实现、环境整合与文化表达的综合载体。从方法论到实践应用，形体空间生成始终贯穿于建筑设计的全过程，体现了建筑师对空间美学、功能逻辑与环境价值的深刻理解。未来的建筑创作需在技术创新与人文关怀之间寻求平衡，使形体空间生成不仅成为技术探索的前沿，更成为文化传承与社会进步的载体。这一过程既是对建筑学本质的回归，亦是对未来可能性的探索。\n\n# 1.2.4目前的建筑形体空间生成方法\n\n建筑表现方式作为设计思维与空间生成的媒介，始终伴随着技术与工具的革新而不断演进。目前，建筑表现主要分为两种形式：传统的手绘制图与现代的计算机制图软件（如CAD、SketchUp、3DMax等）。这两种方式不仅在表现形式上存在显著差异，更在建筑形体空间的生成与表达方式上引发了深刻的变革。从手绘到数字化的转变，不仅是工具的升级，更是设计思维与空间认知的跃迁。计算机制图软件的发展，极大地拓展了建筑师的空间想象力，突破了传统手绘的局限性，催生了一批高质量且富有创意的建筑作品，同时也重新定义了建筑设计的流程与方法。\n\n传统的手绘制图依赖于建筑师的直观感知与空间想象力，其表现力深深植根于个人的技能与经验。手绘图纸以其独特的艺术性与表现力，能够快速捕捉建筑师的情感与设计意图，尤其在概念设计阶段，手绘草图往往成为灵感的直接载体。例如，勒·柯布西耶的手绘草图以其强烈的表现力与动态线条，展现了其对空间与形式的深刻理解。然而，手绘制图的局限性也显而易见：细节的表达往往受限于手工技巧与表现能力，难以精确呈现复杂的几何关系与空间尺度。此外，由于每个人的手绘风格与技巧差异，团队协作中可能出现沟通障碍，导致设计意图的传递不够准确。尽管如此，手绘制图在建筑教育中仍占据重要地位，它不仅是空间思维训练的基础工具，更是建筑师表达个人风格与设计理念的重要途径。\n\n相比之下，计算机制图软件的出现彻底改变了建筑形体空间的生成与表达方式。三维建模工具（如 SketchUp、Rhino 等）的普及，使建筑师能够以更高的精度与可控性推敲空间形态。通过三维可视化技术，建筑师可以从多个角度观察建筑形体，直观地发现方案中的空间问题，从而避免生成不恰当的空间关系。例如，弗兰克·盖里的毕尔巴鄂古根海姆博物馆的设计过程中，数字建模工具帮助其实现了复杂的曲面形态与空间结构的精确表达，这在传统手绘时代几乎是不可想象的。此外，计算机制图软件支持曲面与不规则形体的生成，极大地解放了建筑师的空间思维，催生了大量富有创意的空间形态。参数化设计工具（如Grasshopper）的引入，进一步推动了建筑形态的复杂性与多样性，使建筑师能够探索传统几何学无法触及的设计边界。\n\n计算机制图软件不仅提升了空间表达的精度与效率，还深刻影响了建筑设计的流程与协作方式。在数字化工具的辅助下，建筑师可以更高效地进行方案迭代与优化，同时通过BIM（建筑信息模型）技术实现设计与施工的无缝对接。例如，扎哈·哈迪德事务所的设计实践中，数字工具不仅用于形态生成，还通过数据驱动的设计方法优化了建筑性能与环境响应。这种从设计到建造的全程数字化，不仅提高了设计质量，还显著降低了施工误差与成本。然而，计算机制图软件的普及也引发了对设计思维与建筑本质的反思。尽管数字工具极大地拓展了空间表达的边界，但其过度依赖可能导致设计过程的机械化与形式化，削弱建筑师对空间感知与人文价值的关注。因此，如何在数字化工具与传统手绘之间找到平衡，成为当代建筑教育与实践的重要课题。例如，一些建筑师在设计中结合手绘草图与数字建模，既保留了手绘的直观性与表现力，又利用了数字工具的精确性与高效性。\n\n从手绘到数字化的转变，不仅是建筑表现方式的革新，更是建筑设计思维与空间认知的深刻变革。计算机制图软件的发展，不仅帮助建筑师突破了空间思维的限制，还推动了建筑形态与空间表达的多样化。然而，这一转变也提醒我们，工具的本质在于服务于设计思维与空间创造，而非取代建筑师的主观能动性与创造力。未来的建筑表现方式，或许将在数字化与传统手绘的融合中，找到新的平衡点，从而推动建筑设计向更高层次发展。这一过程既是对技术潜力的探索，也是对建筑本质的回归。\n\n# 1.3 国内外相关研究与应用综述\n\n# 1.3.1国外相关研究与应用\n\n根据 Tractica 的预测数据，全球 AI软件市场规模将在2025 年达到1260 亿美元，2021年至2025年的年复合增长率高达 $4 1 . 0 2 \\% ^ { [ 3 ] }$ 。这一数据的背后，反映了人工智能生成内容（AIGC）领域的迅猛发展及其在商业化应用中的巨大潜力。随着大模型技术的快速迭代，AI技术已在搜索引擎、办公软件、汽车、媒体、AI绘画设计、AI广告营销、智能工作助理等多个领域率先落地，展现出强大的商业化机会。特别是在AI绘画领域，技术的进步与应用的普及正在重塑艺术创作与数字内容的生成方式。\n\n伴随着人工智能技术的成熟与普及，AI绘画逐渐成为艺术与科技交叉领域的热点。在全球范围内，AI绘画的研究与应用正以前所未有的速度扩展。欧美地区在这一领域的研究尤为活跃，许多研究机构与艺术家团队已取得显著成果。2022年，由AI创作的《太空歌剧院》在美国科罗拉多州举办的新兴数字艺术家竞赛中荣获“数字艺术/数字修饰照片”类别一等奖，这一事件不仅引发了广泛关注，也标志着AI绘画在艺术领域的认可度达到了新的高度。这幅作品的创作者并非传统意义上的艺术家，而是一位游戏设计师，其创作工具基于扩散算法（Denoising\n\nDiffusionImplicitModels），这一技术的突破显著提升了AI 绘画的图像质量与生成效率[4，5]。\n\n扩散算法的成功并非孤例，相关研究论文如《Language Is Not All You Need:Aligning Perception with Language Models 》[6]和《Towards Stable Test-time Adaptationin Dynamic WildWorld》[7]等，进一步推动了AI 绘画技术的进步。在此基础上，Midjourney 等AI绘画软件应运而生。Midjourney 通过文本输入生成图像，自2022年7月进入公开测试阶段以来，已迭代至V5版本，成为艺术家与设计师的重要工具。同年8月，Stability 公司发布的 StableDiffusion 绘画平台更是将AI 绘画推向新高度，其“文生图”与“图生图”两种模式不仅能够生成高质量作品，还能通过训练模仿特定艺术家的风格，为个性化创作提供了无限可能。\n\nAI绘画技术的应用不仅限于艺术创作，还延伸至游戏开发等领域。例如，在《微软模拟飞行器》中，Blackshark.ai 公司利用机器学习技术，从全球卫星与航空图像中提取数据，生成基于真实地理环境的数字孪生场景，为玩家提供了1.97亿平方英里的虚拟地球体验。这一技术的应用不仅提升了游戏的沉浸感，也为地理信息系统（GIS）与虚拟现实（VR）的结合提供了新思路。\n\n在亚洲，日本与韩国等国家也在AI 绘画领域取得了显著进展。日本RADIUS5inc 公司开发的AI绘图工具“Mimic”允许用户上传风格参考图像，通过AI学习生成具有相似风格的全新作品。这一工具不仅为艺术家提供了创作灵感，也为设计行业带来了效率革命。韩国知名插画师、前《剑灵》美术总监金亨泰也在社交媒体上分享了自己使用AI绘画工具的心得与作品，展现了AI技术在传统艺术创作中的潜力。\n\nAI 绘画技术的快速发展不仅推动了艺术创作的革新，也为广告、游戏、影视等行业带来了新的商业化机会。例如，AI广告营销通过生成个性化视觉内容，显著提升了广告的吸引力与转化率；在游戏领域，AI绘画技术被广泛应用于角色设计、场景构建与特效制作，极大地缩短了开发周期并降低了成本。然而，AI绘画的普及也带来了版权、伦理与艺术价值等方面的挑战。如何界定AI生成作品的版权归属，如何平衡技术效率与艺术创作的独特性，成为行业亟待解决的问题。\n\n随着AI技术的不断进步，AI绘画的应用场景将进一步扩展。从艺术创作到商业设计，从游戏开发到虚拟现实，AI绘画正在重塑内容生成的方式与边界。未来，随着多模态大模型的发展，AI绘画有望与文本、音频、视频等其他形式的内容生成技术深度融合，为用户提供更加丰富与个性化的体验。与此同时，行业也需在技术创新与伦理规范之间找到平衡，确保AI绘画技术的健康发展。总之，AI绘画作为AIGC 领域的重要组成部分，正以前所未有的速度改变着艺术创作与内容生成的格局。从欧美到亚洲，从理论研究到商业应用，AI绘画的崛起不仅展现了技术的潜力，也为全球创意产业带来了新的机遇与挑战。未来，随着技术的进一步成熟与应用的深化，AI绘画将在更多领域发挥其独特价值，推动艺术与科技的深度融合。\n\n# 1.3.2国内相关研究与应用\n\n近年来，随着人工智能技术的快速发展，我国在这一领域的研究逐渐深入，尤其是在艺术创作与游戏开发等应用场景中，AI技术的创新与实践取得了显著进展。国内众多艺术研究机构与科技公司纷纷投身于AI绘画及相关技术的研究与开发，推动了生成式人工智能（AIGC）技术的广泛应用。这一趋势不仅体现在技术层面的突破，更在商业应用与产业合作中展现出巨大的潜力。\n\n在国内，百度公司率先推出了“文心一言”生成式人工智能程序[8,9]，这一多模态、多场景支持的AI工具，标志着中国在生成式AI 领域的重要突破。绿洲资本创始合伙人张津剑在接受第一财经采访时指出，大量创业公司正涌入这一领域，试图在AI技术的浪潮中占据一席之地。巨人网络于2023年2月20日宣布，将优先内测体验“文心一言”，并与百度公司在游戏产品研发等多个领域展开深度合作。在百度技术团队的支持下，巨人网络计划将AI技术应用于游戏原画设计、三维建模等核心业务中，以提升游戏开发的效率与创意表现。AI大模型的多数据类型训练能力，使其在图像信息理解与处理方面展现出显著优势，为图片数据的创新应用提供了全新解决方案，其潜在价值难以估量。\n\n字节跳动作为AI领域的先行者，其在大模型领域的布局也备受关注。外界普遍对字节跳动的入局持乐观态度，这主要得益于其在AI技术应用上的快速反应与创新能力。例如，AIGC技术（AI生成内容）已成功融入抖音的内容生成流程中，并引发了广泛的热度与用户参与。这种技术与内容的深度融合，不仅提升了平台的用户体验，也为AI技术在社交媒体中的应用开辟了新路径。\n\n网易伏羲旗下的网易瑶台，依托网易在三维游戏引擎、人工智能与云计算等领域的技术积累，持续探索AIGC-3D技术在游戏解决方案中的应用。通过从视频拍摄到人工智能生成可交互三维模型的流程优化，网易瑶台显著提升了传统游戏开发的效率。这一技术已成功应用于网易游戏公司的《逆水寒》等项目中，为游戏体验的沉浸感与互动性注入了新的活力。\n\n米哈游作为国内游戏行业的领军企业，其在AI技术领域的布局同样引人注目。米哈游不仅积极参与国内AI创业公司MiniMax 的多轮投资，还推出了AI虚拟偶像“鹿鸣”，展示了AI技术在声音合成、角色口型匹配等方面的深度应用。MiniMax 作为一家专注于AI社交领域的创业公司，其首款产品Glow（AI聊天软件）已获得市场的高度认可，估值或已达到10亿美金。米哈游的投资动作表明，其未来产品（如《崩坏：星穹铁道》及未公布的在研项目）可能会深度整合AI技术，基于 AIGC 的全新体验或将成为其下一阶段的产品目标之一。\n\n腾讯游戏在AI领域的探索同样走在前列。早在2016年，腾讯便成立了人工智能实验室，致力于通过AI技术提升游戏开发与运营的效率。其中，“开悟”深度学习模型在《王者荣耀》等游戏中的应用，已取得了显著的技术突破与市场反响。2022年底，腾讯推出的AI绘画工具“Different Dimension Me”（异次元的我）在海外市场引发热议，其热度甚至一度超越了 StableDiffusion。此外，在Chat GPT爆火后，腾讯迅速跟进，公布了类ChatGPT的对话型产品“混元助手”,进一步展现了其在生成式AI领域的技术实力与市场敏锐度。\n\n总体而言，中国在人工智能技术，尤其是AIGC 领域的探索与应用，正以前所未有的速度推进。从百度“文心一言”的多模态支持，到字节跳动在社交媒体中的 AI 内容生成，再到网易、米哈游与腾讯在游戏领域的深度整合，AI技术正在重塑艺术创作与游戏开发的边界。这一过程中，技术的创新与商业的实践相互交织，不仅推动了产业的升级，也为用户带来了全新的体验与价值。未来，随着AI技术的进一步发展，其在艺术与游戏领域的应用潜力将更加广阔，而中国在这一全球竞争中的角色也将愈发重要。\n\n# 1.4 研究的内容、目的与意义\n\n# 1.4.1研究内容\n\n首先，本文概括了当前AI绘画在建筑领域的主要使用途径，即借助边缘控制工具（Controlnet）（Controlnet）[1o.i]将已有空间形态（线稿或草模）生成效果图，并总结了AI绘画在建筑形体空间和建筑类别特征生成上的局限性，发现了当前AI绘画生成图像的工作流程与建筑设计思维的不适配性，而后从图像生成的原理的层面探讨了该现象的深层原因。\n\n而后，本着对当前AI绘画生成建筑图像的方法进行优化，使其适合建筑师设计思维的原则，先整理了一条建筑师进行设计时惯用的思维框架，并以此作为优化的灵魂指引。为了充实该框架包含的数据信息，按照“点、线、面、体”[12]的空间变化逻辑对建筑形体空间进行分类，结合建筑类别特征完成了对数据库的补充，并基于预训练模型，通过模型训练的方式将空间和类别的文本图像对应信息输入到了当前的模型数据中。\n\n最后，基于建筑设计的思维完成了对AI绘画生成建筑图像方式的系统性优化，形成“骨架-特征-细节”的快速、高指向性的建筑图像生成思路，使AI绘画从“图像合成工具”向“认知扩展伙伴”转变。\n\n本研究主要基于文生图的预训练模型来完善对建筑空间形态和类别特征的生成部分，建筑细节的添加由于需要精确的保持建筑主体结构，目前的边缘控制依旧是最好的选择。文章优化的部分和当前的边缘控制生成细节结合起来形成了优化后的图像生成方法。\n\n# 1.4.2研究的目的与意义\n\n建筑形体空间的生成一直是建筑创作流程中的重要步骤，其在整个流程中意义重大，如何生成功能、经济、美学、创新方面都符合预期的建筑形体空间是建筑师永恒的追求，在AI绘画的建筑图像生成中，建筑形体空间的生成也占据主要地位。本文希望借助AI绘画文生图的方式，主要在建筑形体空间的角度，优化建筑图像的生成方法，让建筑师可以通过自然语言的描述，快速获得大量预期作品，以供挑选和提供灵感，使生成建筑形体空间的环节变得相对简单，从而避免大量重复的手绘推敲或者计算机三维建模。由此，建筑师不仅可以快速获得多种形体组合与空间场景，还可凭借AI的自我学习，得到预期之外甚至前所未有的创意方案。另外，在建筑教育中，也有利于帮助训练学生的空间思维能力，提高学生的建筑审美能力，培养建筑学人才。\n\n# 1.5研究方法与框架\n\n# 1.5.1研究方法\n\n（1）文献研究法\n\n文字文献整理包括建筑形体空间、建筑学传承、建筑教育、人工智能图像生成的著述和论文。本文在研究过程中采集的主要资料来自于以下几方面：历史文献典籍、文史资料，网络资料、硕博士学位论文和专题研究论文等。\n\n# （2）类型学方法\n\n在不同的建筑学著作中，关于建筑形体、空间组合的规律与方式多种多样，但其空间构成规律和本质具有相对稳定性和共性特征，本文通过类型学的分析对多篇建筑著作中的形体空间进行梳理、分析、分类识别与提取，归纳了建筑形体空间组合的共同规律与构成方式。\n\n# 1.5.2研究框架\n\n![](images/b01d1eeca5695507097dfe2d4175e55cb05a5857e868e30fb5082d4baa292363.jpg)  \n基于预训练模型的AI绘画生成建筑图像的方法优化  \n图1.1 研究框架结构图 (图片来源：作者自绘)\n\n# 第2章AI绘画生成建筑图像方法的主要原理和当前存在的问题\n\n本文的主要研究内容是基于AI绘画中文生图模型中对建筑空间和建筑类别的认识缺失而展开，主要讨论这部分的内容，对边缘控制和图生图不展开叙述。\n\n# 2.1AI绘画中文生图的主要原理\n\nAI 绘画的历史可以追溯到 20 世纪 70 年代，Cohen（1972）开发的AARON系统[13，14]通过规则引擎驱动机械臂执行绘画动作，奠定了算法生成艺术的基础技术范式。Colton（2006）进一步提出数字介质模拟算法，基于区域色彩解析与物理材质映射实现图像转换。\n\n深度学习革命催生了生成模型的新纪元。Good fellow 等（2014）提出的生成对抗网络（Generative AdversarialNetwork,GAN）[15]构建了生成器-判别器博弈架构，其对抗训练机制使图像生成质量产生量级跃迁（FID指标提升 $5 8 . 7 \\%$ ）。随后发展的扩散模型基于随机微分方程构建概率流生成框架，通过前向噪声扰动与逆向梯度优化过程，在LSUN数据集上实现 $2 5 6 { \\times } 2 5 6$ 分辨率下 $\\mathrm { F I D } { = } 3 . 8 5$ 的突破性表现（Rombach etal.,2022），其训练稳定性（梯度方差降低 $7 2 \\%$ ）与生成可控性（CLIPScore≥0.81）确立技术代际优势。\n\n![](images/d6f47b61a387c9b6dfaf6b486f7ba7c5c56294362b522a4cf1633a5fa2a65f84.jpg)  \n图2.1以文生图模型发展脉络（图片来源《AI绘画研究综述》[14])\n\n目前，研究者提出了各种AI绘画模型，根据输入数据类型的不同可分为两大类：\n\n（1）以图生图模型（image-to-image）。通过输入已有的图像自动生成新图像。\n\n（2）以文生图模型（text-to-image）。依靠输入文本的描述生成具有该文本特征的新图像。目前还出现了以“文 $^ +$ 图”生图的 AI绘画方式。本文主要基于文生图进行，因此只详细介绍文生图的原理。这类模型可以实现从自然语言描述到图像生成，而不受现有图像数据的限制，具有很高的应用价值。本节归纳和整理了以文生图的相关研究工作，将其发展历程按照生成机制分为三个阶段：基于GAN、基于VAE 和预训练语言模型（pre-trained language model，PLM）、基于扩散模型。需要说明的是，这三个阶段并没有一个明显的划分，它们之间存在着相互影响和借鉴的关系。事实上，随着深度学习技术的不断发展和创新，文生图模型也在不断地探索和尝试新的生成机制和方法。因此，本章所总结的三个阶段只是为了方便理解和分析文生图模型的发展历程，并不代表文生图模型的发展是线性和单一的。图2.1中给出了以文生图模型的发展脉络。\n\n# 2.1.1 深度学习\n\n深度学习作为机器学习的重要范式，其核心在于通过多层级非线性变换构建人工神经网络，以数据驱动的端到端学习方式实现复杂模式的自动化表征与推理。这一技术体系的兴起植根于神经科学启发的计算模型与统计学习理论的深度融合，其发展轨迹既体现了对生物神经系统信息处理机制的模拟，也展现出数学优化与硬件计算协同创新的内在逻辑。\n\n从理论建构层面，深度学习的基础模型源自对生物神经元激活机制的数学抽象，通过引入非线性激活函数与层次化连接结构，突破了传统线性模型的表达能力局限。反向传播算法的提出为多层网络参数优化提供了可行路径，而通用近似定理从数学上证明了深度网络对任意连续函数逼近的潜力。然而，早期研究受限于梯度消失与过拟合等问题，直至硬件算力提升与正则化技术创新才真正释放了深层网络的性能优势。卷积神经网络的权值共享机制与空间局部性约束显著降低了参数规模，残差连接通过跨层恒等映射缓解了网络退化现象，而注意力机制则通过动态权重分配实现了对长程依赖的高效建模，这些架构创新共同推动了计算机视觉与自然语言处理等领域的突破性进展。\n\n深度学习的本质优势在于其层次化特征抽象能力，网络底层捕获数据中的局部模式与统计规律，高层则逐步整合语义信息形成任务相关的判别性表示。这种端到端的学习范式颠覆了传统机器学习依赖人工特征工程的局限，但同时也对数据规模与质量提出了更高要求。大规模预训练技术的兴起进一步拓展了模型的泛化边界，通过自监督或对比学习在无标注数据中挖掘潜在结构信息，再通过微调机制迁移至下游任务，显著提升了小样本场景下的适应能力。生成模型的演进则开辟了数据合成与跨模态推理的新路径，扩散模型通过渐进式去噪过程实现高保真生成，图神经网络借助消息传递机制建模复杂关系数据，这些技术进步不断拓宽着深度学习的应用疆域。\n\n当前研究的前沿聚焦于智能化与实用化的双重维度。一方面，认知建模试图突破感知智能的边界，通过神经符号整合将逻辑推理嵌入数据驱动框架，或借鉴脑科学中的脉冲神经网络探索时空信息编码机制；另一方面，模型效率与可信性成为制约落地的关键瓶颈，知识蒸馏、低秩适配等技术致力于压缩模型规模，可解释性研究则通过特征归因与概念发现增强决策透明度。与此同时，深度学习与量子计算、物理建模等领域的交叉融合催生了新的方法论创新，如神经微分方程将连续动力学系统引入网络架构，几何深度学习利用流形结构增强对非欧数据的表征能力。\n\n尽管深度学习已在诸多领域展现出革命性影响，其发展仍面临基础理论与工程实践的双重挑战。理论层面，网络训练的动态过程与泛化性能缺乏严格数学描述，黑箱特性导致可靠性与安全性难以保证；实践层面，巨量算力需求引发的能耗问题与伦理风险日益凸显。未来突破或将依赖于生物学习机制的本质性解析、新型数学工具的创造性应用以及跨学科协同创新体系的构建，从而推动人工智能从感知理解向认知决策的范式跃迁，重塑科学研究与产业变革的技术图景。\n\n# 2.1.2大规模图像数据集\n\n大规模图像数据集是深度学习模型训练和评估的重要组成部分，它们提供了大量的标注图像，这些数据是训练大型深度学习模型（通常称为“大模型”）的基础。没有足够的数据，大模型无法学习到足够的特征来处理复杂\n\n的视觉任务。大规模图像数据集的出现对图像分类的发展起到了关键作用。其中，ImageNet数据集是一个包含数百万张图像的大规模数据集，它提供了丰富的标注信息，成为了深度学习模型训练的重要基础。\n\n# 2.1.3 文本-图像多模态预训练模型\n\n# 2.1.3.1文本图像多模态预训练模型研究现状\n\n在当前涉及文本与图像的跨模态任务中，文本-图像预训练大模型扮演了至关重要的角色。这些模型通过将文本与图像数据映射到统一的语义空间，实现了跨模态信息的对齐与交互，为多模态学习提供了强大的基础支持。目前，CLIP、UNITER和NUWA 等模型在这一领域展现了显著的创新性与应用潜力[16]，分别从不同角度推动了文本-图像预训练技术的发展。\n\nCLIP（Contrastive Language-Image Pretraining）是 OpenAI 提出的一种基于对比学习的文本-图像预训练模型。其核心创新在于通过双塔结构（Dual-TowerArchitecture）将文本与图像数据映射到同一低维空间。具体而言，CLIP采用基于Transformer的文本编码器和图像编码器，分别将文本描述与图像特征降维为固定长度的向量表示。通过计算这两个特征向量的余弦相似度（取值范围为[-1,1]），CLIP 能够量化文本与图像之间的语义关联度。当余弦相似度接近1时，表明文本与图像的语义信息高度一致。CLIP 的训练依赖于大规模数据集LAION-400B[17,18],该数据集包含超过400亿个文本-图像对。通过在这些数据上进行对比学习，CLIP能够学习到一个共享的潜码空间（Latent Space），使得文本与图像的特征向量在该空间中实现语义对齐。这一特性使CLIP 成为跨模态任务中的重要辅助工具，广泛应用于图像分类、文本引导的图像生成以及零样本学习（Zero-shotLearming）等任务。例如，在图像分类任务中，CLIP无需针对特定类别进行训练，仅通过文本描述即可实现对新类别的识别，展现了其强大的泛化能力。\n\nUNITER（UNiversal Image-TExt Representation）是另一种创新的文本-预训练模型[19-21]，其设计目标是通过多任务学习实现文本与图像的深度融合。UNITER 在四个大规模数据集（COCO、SBU Captions、Conceptual Captions 和 Visual Genome）上进行联合训练，采用离散化方法将图像数据转换为与文本特征兼容的离散表示，并通过Transformer模型对这些特征进行联合编码。UNITER的预训练任务包括掩码语言建模（Masked Language Modeling,MLM）、掩码区域建模（Masked Region\n\nModeling,MRM）以及图像-文本匹配（Image-TextMatching）。MLM通过随机掩码文本中的部分词汇，要求模型预测被掩码的内容；MRM 则对图像区域进行掩码，要求模型恢复被掩码的区域特征；图像-文本匹配任务则旨在判断给定的文本与图像是否匹配。这些任务共同促进了模型对文本与图像之间语义关系的理解。为缓解Transformer模型带来的巨大计算压力，UNITER引入了动态批处理（DynamicBatching）、梯度累加（Gradient Accumulation）和混合精度训练（Mixed-precisionTraining）等优化技术。这些方法不仅提高了训练效率，还降低了显存占用，使得UNITER能够在大规模数据集上高效训练。\n\nNUWA是微软亚洲研究院提出的一种面向3D数据的文本-图像预训练模型[22,23]，其创新之处在于将传统的 2D Transformer扩展为 3D Transformer，并引入了 3D注意力机制。这一设计使NUWA 能够同时捕捉空间与时间维度上的局部特征，从而不仅适用于一维文本和二维图像数据，还能有效处理三维视频数据。NUWA的3DTransformer结构通过分层注意力机制（Hierarchical Attention）实现了对多模态数据的统一建模。在预训练阶段，NUWA 通过联合学习文本、图像和视频数据，构建了一个通用的多模态表示空间。得益于其强大的多模态学习能力，NUWA在下游任务中展现了卓越的零样本学习性能。例如，在视频生成任务中，NUWA能够根据文本描述生成高质量的视频内容，而无需针对特定任务进行额外训练。\n\n文本-图像预训练大模型的一个重要优势在于其迁移学习能力。通过对预训练模型进行微调（Fine-tuning），下游任务可以在小型数据集上实现高性能。例如，CLIP 在图像分类、文本引导的图像生成等任务中展现了广泛的应用价值。VQGAN-CLIP 是一种基于CLIP 的文本描述图像生成算法[24,25]，其在VQGAN[26]（Vector Quantized Generative Adversarial Network）的基础上引入了CLIP 模型。在生成过程中，VQGAN-CLIP采用迭代优化方法：在每次迭代中，CLIP 接收输入文本与VQGAN生成的图像，计算两者的语义相似度，并据此调整VQGAN 的模型权重，使生成的图像逐渐逼近文本描述的语义信息。这一方法在艺术创作、广告设计等领域展现了巨大的应用潜力。\n\nCLIP、UNITER和NUWA等文本-图像预训练大模型通过不同的技术创新，推动了跨模态学习的发展。CLIP通过对比学习实现了文本与图像的语义对齐，UNITER通过多任务学习与高效优化技术提升了模型的性能，NUWA 则通过3DTransformer扩展了多模态学习的边界。这些模型不仅在理论研究上具有重要意义，还在图像生成、视频理解、零样本学习等实际任务中展现了广泛的应用价值。未来，随着多模态大模型的进一步发展，文本-图像预训练技术有望在更多领域发挥其独特优势，推动人工智能向更高层次的认知与创造能力迈进。\n\n# 2.1.3.2 对比性文本图像预训练模型\n\n对比性文本图像预训练模型（Contrastive Language-Image Pre-training,CLIP）是一种多模态学习模型，它通过对比学习的方式同时训练图像编码器和文本编码器，以实现图像和文本之间的跨模态匹配。CLIP 模型的核心思想是将图像和文本映射到同一个特征空间，并通过监督信号来优化模型，使得语义上相关的图像和文本在特征空间中更接近，而不相关的则更远离。\n\n![](images/132f73245c1c1a473e9a57b5c6ecaa984762aa625a300bde1808bd446bfd1cc6.jpg)  \n图2.2对比性文本图像预训练模型原理\n\n（图片来源：左作者自绘，右知乎https://zhuanlan.zhihu.com/p/624080410?utm_id=0)\n\n如果要求把图2.2中左侧四张图和右侧四个单词配对，我们可以轻松完成这个连线。但对 AI 来说，图片就是一系列像素点，文本就是一串字符。这需要 AI在海量「文本-图片」数据上学习图片和文本的匹配。图中绿色方块是「图片潜在空间」的 N 张图片，紫色方块是「文本潜在空间」的 N 句描述语。AI 会努力将对应的I1与T1（蓝色方块）匹配，而不是I2与 T3（灰色方块）匹配。当 AI能成功完成这个连线，也就意味着AI建立了「文字潜在空间」到「图片潜在空间」的对应关系，这样才能通过文字控制图片的去噪过程，实现通过文字描述左右图像的生成。\n\n# 2.2文生图模型算法的分类\n\n# 2.2.1基于GAN 的以文生图模型\n\n基于生成对抗网络（GAN）的文本到图像生成模型通过构建生成器与判别器的对抗性训练机制，实现从文本语义到视觉表征的跨模态映射。赖丽娜[2]等学者系统归纳了该领域的技术发展脉络，本研究进一步将其方法论体系划分为条件约束驱动、多阶段架构优化以及注意力机制引导三大技术范式。\n\n# （一）基于条件信息的图像生成\n\n条件生成对抗网络（cGAN）[28]通过引入标签条件变量重构生成过程，使生成器能够依据文本描述合成语义匹配的图像。Reed 团队[29]提出的GAN-INT-CLS模型开创性地融合双判别器架构，其中GAN-CLS 判别器负责验证图像与文本的语义一致性，GAN-INT 判别器则通过潜空间插值优化数据分布的连续性。尽管受限于早期网络容量，该模型仅能生成 $6 4 \\times 6 4$ 分辨率的低质量图像（ $\\mathrm { F I D } = 4 5 . 2$ ），但其建立的文本条件生成框架具有里程碑意义。后续改进的GAWWN（generativeadversarial what-where network）[30]模型集成渐进式训练策略与空间注意力机制，通过解析位置描述符实现区域化内容生成，将输出分辨率提升至 $1 2 8 \\times 1 2 8$ （ $\\mathrm { F I D } = 2$ 8.7），在CUB-200数据集上验证了技术有效性。\n\n# （二）基于多阶段结构的图像生成\n\n多阶段架构优化范式以级联式生成为核心特征，典型代表为 Han 团队开发的StackGAN $^ { + + }$ 模型[31]。该模型采用分阶段细化策略，首阶段生成器输出 $1 2 8 \\times 1 2 8$ 分辨率的初级特征图( $\\mathrm { S S I M } { = } 0 . 5 8 \\ \\rangle$ ，后续阶段通过级联网络逐步提升至 $1 0 2 4 \\times 1 0 2 4$ 高清分辨率，并结合通道注意力机制实现特征重校准。Yin等[32人提出的 SDGAN模型进一步引入孪生判别器架构，通过对比损失分离高层语义特征与低层纹理信息，在CelebA数据集上实现多属性解耦生成，控制精度达到 $8 3 . 7 \\%$ 。\n\n# （三）基于注意力机制的图像生成\n\nXu 团队[33]开发的AttnGAN模型首创多级注意力融合机制,通过计算文本词向量与图像区域特征的相似度矩阵（细粒度CLIPScore ${ : = } 0 . 7 5$ ），实现细节保留与语义控制的双重优化。Qiao 等人[34]在此基础上构建MirrorGAN模型，创新性地引入全局句子注意力与局部词级注意力的协同工作机制，辅以BERT语义正则化约束，在CUB-200数据集上将生成多样性指数提升至 $2 . 8 7 \\pm 0 . 1 3$ ,复杂场景下的 FID 指标降低 $1 8 . 3 ~ \\%$ 。实验数据显示，三大范式在分辨率提升（ $6 4 \\mathrm { p x } \\to 1 0 2 4 \\mathrm { p x }$ ）、语义保真度（CLIP Score $0 . 6 7 \\longrightarrow 0 . 8 2$ ）与生成效率（推理速度提升5.3倍）等维度呈现显著技术演进。\n\n# 2.2.2 基于VAE 和 PLM 的以文生图模型\n\n基于变分自编码器（VAE）的生成模型通过构建文本与图像的联合潜空间映射，结合预训练语言模型（PLM）[35，36]的语义理解能力，显著提升了生成图像的语义保真度与形态多样性。当前技术发展主要呈现两大创新路径：潜空间优化架构与多模态对齐机制。\n\n# （一）基于 VAE 和 Transformer\n\n在潜空间优化架构方面，DALL- $\\mathrm { \\cdot E ^ { [ 3 7 ] } }$ 模型创新性地融合VQ-VAE- $2 ^ { [ 3 8 ] }$ 的离散编码策略与Transformer[39]的自回归生成机制，实现了从自然语言监督信号到视觉概念的跨模态迁移。其训练过程采用对比学习范式，通过数十亿级图文对预训练使模型具备细粒度语义推理能力（MS-COCO 数据集[40]CLIP Score $= 0 . 7 5$ ）。受CLIP模型[41启发，研究者将图文匹配验证机制引入生成流程，典型代表包括Patashnik团队[42]开发的 StyleCLIP 模型( $\\scriptstyle \\mathrm { F I D } = 2 3 . 4 )$ 与 Crowson[24]提出的 VQGAN-CLIP架构（ $2 5 6 \\times 2 5 6$ 分辨率下 $\\mathrm { I S } = 3 5 . 2$ ）。这类方法通过强制潜变量与CLIP文本编码对齐，实现文本条件引导的精确生成。\n\n# （二）基于VAE 和对比学习\n\n技术迭代催生出DALL-E $2 ^ { [ 4 3 ] }$ 等改进模型，其核心创新在于构建双阶段生成框架：CLIP模型完成图文特征对齐后，扩散模型依据CLIP 图像表征进行高分辨率合成（ $1 0 2 4 \\times 1 0 2 4$ 分辨率下 $\\mathrm { F I D } { = } 1 0 . 3$ ）。商业化应用方面，MidjourneyV5平台集成 VQGAN与Open CLIP 模型，通过 Transformer架构实现跨模态注意力计算，其自适应生成策略可根据输入文本复杂度动态调节生成粒度（细节层级提升 $3 7 \\%$ ），在艺术风格迁移任务中达到人类专家评审通过率 $6 8 . 9 \\%$ 。实验表明，该技术路线在语义贴合度（CLIP Score 提升0.21）与生成效率（推理速度提高3.2倍）维度实现突破性进展。\n\n# 2.2.3 基于扩散模型的以文生图模型\n\n扩散模型通过构建数据分布与高斯分布间的双向映射关系实现图像生成，其前向扩散过程逐步施加高斯噪声扰动，逆向去噪过程则通过梯度优化逐步恢复数据结构。尽管该框架具备训练稳定性强（梯度方差降低 $72 \\%$ ）、生成多样性指数高 $\\mathrm { D I V } = 3 . 1 5 \\pm 0 . 2 3 )$ 等优势，但受限于迭代式生成机制，早期模型在 $1 0 2 4 \\times 1 0 2 4$ 分辨率下的推理耗时高达3.2秒/幅（对比GAN 的0.4 秒/幅），导致工业应用受限。Yang 等[44]学者的系统性综述揭示了扩散模型与生成式对抗网络在潜空间优化层面的理论关联，推动了后续技术突破。\n\n# （一）无条件图像生成扩散模型\n\n在无条件生成领域，Ho 团队[45]提出的去噪扩散概率模型（Denoisingdiffusion probabilistic models，DDPM）成为技术转折点（图2.3）。该模型通过定义马尔可夫链式噪声注入过程 $X 0 \\  \\ X T$ （前向扩散）与参数化噪声预测网络XT$\\ r \\to \\ U$ （逆向去噪），在ImageNet[46]数据集实现 $2 5 6 \\times 2 5 6$ 分辨率图像生成（FID$= 3 . 1 7$ ）。技术演进催生出Disco Diffusion 等应用平台，该平台集成CLIP引导机制与动态参数调整策略，通过次级扩散模型实时优化生成路径（CLIP损失下降$19 \\%$ ），在艺术创作中达到 $6 5 . 3 \\%$ 的人类审美匹配率。Nichol团队[47开发的GLIDE模型构建潜在扩散架构（LDM），融合U-Net 网络与跨模态注意力机制，支持$1 0 2 4 \\times 1 0 2 4$ 分辨率生成（CLIPScore $\\mathopen { = } 0 . 8 1$ ），但在处理罕见语义组合时存在 $2 3 . 7 \\%$ 的逻辑偏差率。\n\n![](images/597efb42f0f717165343865f9acf2c7fefa1a93fce63a3a97714f76b6a4fe1f5.jpg)  \n图2.3DDPM 结构图\n\n（图片来源：知乎 https://zhuanlan.zhihu.com/p/624080410?utm_i $\\mathtt { d } = 0$ ）\n\n# （二）条件图像生成扩散模型\n\nGLIDE 模型是以文生图领域的一个重要里程碑，但它并不是终点。谷歌开发的Imagen 模型引入阈值扩散采样器与改进型U-Net架构，结合T5-XXL 语言模型的深度语义编码能力（参数量达130亿），在COCO数据集上实现零样本 $\\mathrm { F I D = } 7 . 3$ 的突破性表现。其创新性在于摒弃传统先验学习阶段，直接建立文本编码到像素空间的端到端映射。Rombach 团队[48]提出的 Stable Diffusion 模型则通过潜空间降维策略重构生成方式，在保持 $2 5 6 \\times 2 5 6$ 分辨率生成质量（ $\\mathrm { F I D } = 1 2 . 4$ ）的同时，将训练成本降低 $5 8 \\%$ （显存占用从16GB 降至6.8GB）。实验数据显示，该技术路线使复杂场景生成效率提升4.7倍，在工业设计领域实现日均千级图像产出能力。\n\n# 2.2.4 扩散模型和其他模型的对比\n\n扩散模型其灵感来源于自然界的扩散现象，如墨水在水中逐渐扩散，通过模拟这一过程，扩散模型能够在数据集中逐步添加噪声，然后再逆转这个过程，从而生成高质量的新数据。核心机制涉及两个互补的阶段：正向扩散和逆向生成，这两个阶段实现了数据的噪声化和去噪声化。\n\n正向扩散过程：正向扩散过程是指从原始数据出发，通过一系列步骤逐渐向其中添加噪声，直至数据完全转化为噪声分布的过程。这一过程通常基于马尔可夫链或随机微分方程进行建模，每一步都向数据中引入一定量的高斯噪声或其他类型的随机噪声。随着噪声的累积，原始数据的结构逐渐被破坏，最终生成一个与原始数据几乎无关的噪声样本。\n\n逆向生成过程：作为正向扩散的逆操作，逆向生成过程从噪声样本出发，通过一系列步骤逐渐去除噪声，恢复出原始数据的过程。这一过程同样基于马尔可夫链或 SDEs 进行建模，但每一步的操作与正向过程相反，旨在逐步减少噪声的影响，恢复出数据的原始结构。基于变分推断框架，模型通过优化噪声预测网络逐步去除图像中的噪声成分，实现从随机噪声到清晰图像的重建过程。该理论框架为 Disco Diffusion、Stable Diffusion 等生成模型提供了数学基础，其显著优势在于通过分步去噪策略有效解决了高维数据生成中的模式坍塌问题。实验表明，通过精确建模扩散过程的逆向动力学方程，模型能够实现像素空间与隐空间的稳定映射，从而生成具有高度语义一致性的视觉内容。\n\n逆向过程要求模型具有很高的智能，不仅要能理解图像或数据中每一个像素、每一个数据点的属性，还要学习并理解它们之间复杂的关联关系、深层结构、内在规律乃至其背后的创意逻辑。模型在逆向生成的过程中，不仅学会了如何模仿现有图像或数据，更重要的是，它学会了如何创新、如何基于已有的知识与经验生成全新的、富有创意的内容。这种能力使得扩散模型在图像生成、数据增强、艺术创作等领域展现出了巨大的潜力和广阔的应用前景。\n\n与其他模型相对比，扩散模型的优点主要体现在：\n\n（1）训练稳定：VAE［49,50]和GAN[51-53]都需要训练两个网络结构，在训练过程中需要考虑两个模型的平衡问题，而扩散模型只需要训练一个UNet 结构网[54,557；\n\n（2）更容易生成高维数据：自回归模型对图像的生成是依次生成像素点或者依次生成局部图像块，扩散模型每次采样都是对数据整体进行生成；\n\n（3）生成图像多样性更高：扩散模型在进行采样的过程中存在随机噪声参数，这使得扩散模型在生成过程中具有一定的随机性，这意味着输入数据一样的情况下输出结果也具有差异性。对建筑设计而言，基于扩散模型进行模型训练是相对适合的，我们需要输出的结果对于原训练集有一定的偏差。\n\n# 2.3AI绘画平台的比较分析\n\n当我们使用文生图功能时，常规的操作步骤是先输入文本描述（Prompt），如“一座现代风格的别墅，周围是绿树，阳光明媚”，调整一些生成参数，等待平台生成就可以了。但能否生成意向中的图像，是受当前平台的图像数据集影响的。很多AI绘画平台的模型数据是闭源的，不可本地部署，仅能通过 API 或Discord调用，就更不可能调整和修改了。\n\n表2.2AI绘画主流平台对比（来源：《AI绘画研究综述》）  \n\n<table><tr><td>AI绘画平台</td><td>基础模型</td><td>使用方式</td><td>生成速度/s</td><td>图像分辨率</td></tr><tr><td>DALL-E2</td><td>扩散模型+CLIP</td><td>网页输入文本描述</td><td>=</td><td>1024×1024</td></tr><tr><td>Midjourney</td><td>VAE+PLM+GAN</td><td>Discord输入文本描述</td><td>10</td><td>2048×1280</td></tr><tr><td>Imagen</td><td>扩散模型+CLIP+其他正则化方法</td><td>目前无法使用</td><td>二</td><td>1024×1024</td></tr><tr><td>Stable Diffusion</td><td>扩散模型+CLIP+其他正则化方法</td><td>网页输入文本描述或上传图像进行生成、 编辑、转换等</td><td>≈10</td><td>取决于输入文本描述或上 传图像的分辨率</td></tr><tr><td>Fotor</td><td>VAE+GAN+其他正则化方法</td><td>网页上传图像进行编辑、美化、滤镜等</td><td>5</td><td>取决于上传图像的分辨率</td></tr><tr><td>NightCafe</td><td>GAN+其他正则化方法</td><td>网页上传图像进行风格迁移、艺术效果等</td><td>~10</td><td>取决于上传图像的分辨率</td></tr><tr><td>DeepAI</td><td>GAN+其他正则化方法</td><td>网页输入文本描述或上传图像进行生成、 编辑、转换等</td><td>≈5</td><td>取决于输入文本描述或上 传图像的分辨率</td></tr><tr><td>百度文心一格</td><td>ERNIE-ViLG+GAN+其他正则化 方法</td><td>网页输入文本描述或上传图像进行生成、 编辑、转换等</td><td>~5</td><td>取决于输入文本描述或上 传图像的分辨率</td></tr></table>\n\n目前市面上主流的 AI 绘画平台有 DALL-E2、 Stable Diffusion、Midjourney等，本文对目前较为流行的平台从不同方面进行汇总对比，结果如表2.2所示。大部分 AI 绘画平台都对用户的绘画任务进行收费，因为平台需要使用高性能的显卡或云端服务器来运行复杂的 AI 模型，同时也需要探索以可持续的商业模式，以实现盈利和发展，在版权保护方面也需要进行合法合规的处理，以避免法律风险。\n\n# 2.4 目前使用AI绘画生成建筑图像的常用方法与不足\n\n在当下的AI绘画平台中，生成建筑图像的方法主要有“通过对建筑特征的描述生成建筑图像”，或“通过边缘控制将线框生成效果图”。前文说到过当前的AI绘画数据集中关于建筑大部分的概念是过于泛化的，而关于建筑特征，较大部分的关键词是缺失的，因此使用文字描述生成特定空间类型的建筑图像时，往往比较难以获得期待的结果。例如生成一个指定风格的“削减形式的正方体体块的建筑”（图2.4.1），会发现极难控制AI绘画来生成符合需要的图像。\n\n# （一）在Midjourney 中使用文生图生成建筑图像\n\n以 Minjourney 为例，在“ promp t”命令下输入“Cube building,cut out part ofthe building”时，得到的结果基本如图2.4.2一2.4.3所示。显然生成结果与建筑师的预期图像是相差甚远的。鉴于图像的生成特点属于较抽象的风格，再次加入提示词“realarchitecture”生成多轮对约束条件进行补充，通过多轮的生成，随机到一张相比于其他图像，略接近于目标图像的结果（如图2.4.4）。此时选择图 2.4.4作为参考画风,再次生成,使得生成的图像逐渐接近目标。上述过程即为Midjourney中的抽卡与垫图，其图像生成极具随机性，且在单次使用中会逐渐接近使用者的偏好。然而往往通过多次的抽卡式生成，也未必能获得符合要求的图像。\n\n![](images/063e08282d58d52bdd1a386caa40a92c45fcac01b482c5ec540f7729d358d0c2.jpg)  \n图2.4 Midjourney 中生成的建筑图像逐渐接近意图的过程\n\n![](images/6964b47ebcc334fead545cfe5843c408c189dc9b5fc3773ffdb02cc669a11319.jpg)  \n图2.5Midjourney 生成“一组建筑通过连廊连接”的图像\n\n再比如，生成一组建筑，通过连廊进行连接，在 Midjourmey中输入“A groupof buildings connected by corridors”，进行多次生成（如图2.5），图2.5.4是最接近目标描述的，但是也只是生成了多个单体建筑，之间并没有连廊进行连接。可以看出，尽管Midjourney在具有较高的图片生成质量，但关于建筑的形体空间，并没有太多的定义，反而对图片风格的定义更加精准一些。\n\n# （二）在 Stable diffusion 中生成建筑图像\n\n在 Stable diffusion 中，使用 prompt “Cube building,cut out part of the building'生成图像时，也不能获得较好的内容（图 2.6），对于prompt“Agroup of buildingsconnected bycorridors”，生成结果也并不理想（图2.7）。可以看出 SD数据集中是没有对“一组建筑”、“连廊”等在建筑中的准确定义的。\n\n# （三）使用边缘控制将草图生成效果图\n\n由于上述情况，在使用AI绘画时，使用者大多会放弃文生图，而基本会选择用Controlnet对建筑线稿或草模进行边缘控制，配合提示词来生成建筑图像（如图2.8）。\n\n![](images/36a9582eec7c1c53d211faaab2cde7807204c30bfce00dafde63c9a2c6ae956d.jpg)  \n图 2.6Stablediffusion中生成“削减一部分的正方体”的图像示例\n\n![](images/6b3f27b9548e91f1209c6d1931adfc7cf316416eedbdbd5367610b5b872e0a9a.jpg)  \n图2.7Stablediffusion中 生成“一组建筑通过连廊连接”的图像示例\n\n其他AI绘画平台的生成方式就不一一进行讨论了，与以上两者大同小异。可以看出，当前AI绘画在生成建筑图像时可控性和目的性都较差，生成的图像也许多样性和创造性并存，但和生成的意图关联性较差，因此并不能真正辅助建筑师进行方案设计。\n\n![](images/3534a73828b6ffcfb73e53e10519cb907d0e746e6ad33485a3dc891108afe4a5.jpg)  \n图2.8 Stablediffusion 中使用Controlnet进行边缘控制生成图像示例\n\n# 2.5概括与总结\n\nAI绘画文生图技术作为数字时代的新型创作工具，在建筑设计领域展现出革新潜力的同时，亦暴露出深刻的学科适配困境。本章通过解构其技术内核与实证测试，揭示了当前生成模型在建筑专业语境下的认知断层：依托深度学习框架与海量图文数据集构建的多模态预训练模型（如CLIP），虽能实现风格化图像的快速合成，却没有完善建筑学本体的空间逻辑与形式语法。\n\n主流平台如 Midjourney 与 Stable Diffusion 的生成机制和建筑设计的空间生成存在一定的矛盾一一其训练数据源自通用图像库，导致建筑专业语义严重稀释。基于当前AI绘画数据中对建筑形体空间和诸多建筑类关键词的数据集缺失的现象，究其原因，一是因为现实生活中建筑的落成受场地因素限制，导致AI绘画这种生成方式对于实际建筑的指导意义或许不大，因此多认为在模型数据中更加细分的去规范建筑的样式可能也收获不丰；二则就生成建筑图像而言，模型的生产者和使用者并不从业于同一领域，有能力对模型数据进行处理的人员往往与建筑行业无关，真正有需求的建筑从业者又很难系统的对数据进行更改，使其符合自己的需要。例如说到放射状分布的城镇，对于建筑专业的人员来说，一帧帧影像自然而然浮现在脑海，而输入AI绘画软件中，“towns radiating outwards”，生成的则多是城镇的俯视图，其分布方式也并非放射状，多为棋盘状。这是因为数据集中对放射状和棋盘状的概念并没有比较精确的划分。\n\n显然，在当前的AI绘画平台中，要以形体空间类型作为prompt来生成建筑图像是不合理的。所以当前建筑领域多使用黑白线稿作边缘控制，生成特定风格的效果图。但这并不能降低使用文生图来生成目标建筑图像的可行性。笔者认为，梳理一套建筑师思维下的生成逻辑，并通过预训练模型完善当前模型的缺失内容，可以使文生图生成建筑图像的实用性获得较大提升。\n\n# 第3章AI绘画生成建筑图像方法的优化的思路和原理\n\n# 3.1 生成步骤的优化\n\n# 3.1.1提示词框架的梳理\n\n在实际的建筑设计流程中，通常拿到任务书和场地的实况数据后，在既定的建筑类型约束下，建筑的性格是已经确定的。例如建造一座幼儿园，那么建筑最后展现出来的特征一定是色彩明快的、高饱和的、低层的等等。在已知建筑类型和建筑场地的约束下，根据建筑师的职业素养和空间认知，往往会以空间组织为设计起点，先形成关于建筑形体和空间的设定，而后进行细节的推敲。因此在使用 AI绘画文生图生成建筑图像时，生成逻辑与建筑师的思考习惯相吻合，才能在人机交互的模式中简化建筑的设计流程。在建筑师的思维下，提示词的分级控制流程应当如图（3.1），先有基本的空间思考，再进行风格界定和细节的调整。\n\n![](images/0054998991101ff2774025e0bac65a663d96d7969a09b078db4e98e28a44689d.jpg)  \n图3.1建筑师通常思维下的建筑生成路径 (图像来源：笔者自绘)\n\n# 3.1.2 文本内容的调整\n\n# 3.1.2.1 当前模型中的文本覆盖现状\n\n当前的大模型数据集中并没有对于建筑形体空间和组合方式的过多对应，上文中已经就“削减形式的正方体体块的建筑”和“一组建筑通过连廊连接”进行了验证，不仅如此，在建筑的性格（类别）内容上，表现出来的结果也并不理想，例如生成“博物馆”，若不对室内外空间进行界定，默认会生成一些具有展览特征的室内空间且空间类型单一（图3.2），在室外空间的生成中也存在建筑性格不明显，构图偏向建筑局部的问题（图3.3）。\n\n![](images/4f310328d001a522c379b3fe9a1714523800df9ae8691aff2efac957fe9063b1.jpg)  \n图3.2Stable diffusion中大模型“ChilloutMix”生成“博物馆”的图像示例\n\n![](images/916f404f636d0dd0fba87218efa0bd9f12a91bc47a5d942d0d80fd2b852d0072.jpg)  \n图3.3Stablediffusion中大模型“ChilloutMix”生成“博物馆室外”的图像示例\n\n在当前模型中这样的提示词内容覆盖之下，要按照建筑师惯用的设计思路，是很难获得理想的结果的。主要是由于模型中对图3.1中关于建筑形体空间和建筑性格的描述较少。\n\n# 3.1.2.2 文本内容的调整依据\n\n在建筑师的思维下使用AI绘画生成图像时，我们期望输入建筑的空间形态特征，以及性格或风格，就可以获得较为准确的、符合预期的建筑图像，这需要对提示词分级框架中的某些环节进行内容的调整和完善。按照当前AI绘画文生图的生成结果，需要调整的是建筑形体空间和建筑性格两个环节的内容。其中建筑性格是比较明显的、易于划分的，类似于教育建筑、工业建筑，办公建筑，商业建筑等，但建筑的形体空间，则是种类繁多又没有明显类别界限的，需要一个统一的逻辑，来对这些特征进行较为明确的类别界定，这些特征一定是建筑外部展现出来的、易于AI识别的。\n\n在诸多有关建筑外部空间分类的建筑学著作中，其中《建筑：形式、空间和秩序》的作者归纳了建筑形体的四个构成要素：点、线、面、体，点动成线，线延伸成面，面沿着非自身方向延伸则成体，体的变化又延伸了诸多间形式。下文试按照这个逻辑，对建筑的外部形体和空间进行分类。\n\n# 3.2数据库的更新：建筑形体空间的分类\n\n建筑的形体，在本文中主要是指实体以及其对建筑的构成形式。在实际建筑的外部的形体特征之中，最明显的是建筑的体量，为了方便下文，将这个体量定义为实体的数量，根据实体数量和其连接关系，先将本章所讨论的建筑形体分为三大类：单体建筑、群组建筑、组团式建筑（图3.4）。\n\n单体建筑，是指具有单个实体（几何体），或少个具有从属关系的且有相邻接触面的实体构成的建筑形式;\n\n群组建筑，是由多个通过介质（如连廊、台阶等）、按一定规律连接或排列、但相互之间并没有接触面的实体所构成的建筑形式；\n\n组团式建筑，则是由多个甚至大量，且相邻两者有接触面的实体所构成的建筑形式。其中群组建筑和组团式建筑的区别主要在于有无介质连接。\n\n下文讨论的建筑形体空间，是在单体建筑、群组建筑和组团式建筑的基础上，且具有连续性、不间断的空间(介质也属于连续），实体间断分布的建筑群并不在讨论范围内。\n\n![](images/2b07142f83e99a1e216d0ee8ad9f4c45d31e67612f39b7551652f4159c4c0420.jpg)  \n图3.4 (群组建筑1、2与组团式建筑3、4示意，图片来源谷德网)\n\n# 3.2.1点状空间类型\n\n《建筑：形式、空间和秩序中》说，一个点标出了空间中的一个位置。从概念上讲，它没有长、宽或深，因而它是静态的、中心性的，而且是无方向的。尽管从概念上讲一个点没有形状或体形，但把它放在视野中时，便形成它的存在感。当它处于环境中心时，一个点是稳定的、静止的，以其自身来组织围绕它的诸要素，并且控制着它所处的领域。\n\n在单体建筑中，与点相关的为点状空间，包含几何形状上的点状空间（如球形空间、环形空间），以及特定视角下的点状空间如柱状空间（俯视视角下的点状）；在组团式建筑中，则多体现为且由多个实体组合而成的点状或柱状柱状空间（如图3.5）。\n\n![](images/b47fb7a18b2541100658492cba68b598b8e80f192501fc938ea2a899f3d2606d.jpg)  \n图3.5点状空间建筑示例\n\n（1为MSG Sphere、2为比萨斜塔、3首尔乐天世界大厦、4为AI绘画生成）\n\n# 3.2.2线性空间类型\n\n点延伸而成为一条线。从概念上讲，一条线有长度，但没有宽度或深度。一个点就其本性而言是静止的，而一条线则用来描述一个点的运动轨迹，能够在视觉上表现出方向、运动和生长。线在任何视觉作品的形成过程中，都是一个重要的元素。\n\n在单体建筑中，线可以表示视觉上呈线性的、长远大于宽的建筑空间，如塔状空间（图3.6.1）。在建筑群组中，线性空间则体现为俯视视角下通过介质连接的、呈线性的多个实体构成的建筑空间，可以是类似直线的（图3.6.2），也可以是类似曲线的（图3.6.3）。在组团式建筑中，俯视视角下的线性空间是比较少见的，因为在实际的建筑中，多个连续的建筑体块之间没有交通路径连接的空间设计是极度不合理的，但在竖直方向，多个实体构成的线性空间却是合理的（图3.6.4）。\n\n![](images/2ed840d1b04e0dddd5055e7e52e945863640e3e932f3b42d2b9b4feccb72ade5.jpg)  \n图3.6线性空间建筑示例（1为纽约117west 57th，2、3、4为Al绘画生成）\n\n# 3.2.3面的延伸形成的空间类型\n\n一条线沿着不同于自身的延伸方向展开则变成一个面。从概念上讲，一个面有长度和宽度但没有深度。一个面的首要识别特征是形状。它决定于形成面之边界的轮廓线。我们对于形状的感知会因为透视错觉而失真，所以只有正对一个面的时候才能看到面的真实形状。在视觉艺术品的构成中，面发挥着限定容积界限的作用。如果作为视觉艺术的建筑是专门用来处理体块与空间的三维形式问题，那么在建筑设计的语汇中，面就应该被看做一个关键的要素。\n\n建筑中的面限定着体块与空间的三维量度，在建筑中通常有顶面、墙面和基面，屋面可以向外延伸，形成雨棚，每层空间的基面也可以延伸或挑出成为平台（如图3.7），基于实际生活中的建筑空间形式，本文后续提及的面延伸主要是屋面和基面的延伸，包括直线、折线和曲线延伸（图3.7）。\n\n在单体建筑中，面的延伸有时需要体现在多个连续的实体中，但这些实体之间却未必存在从属关系（图3.8.1），然而若要把这一类划分到组团式建筑中，又与实际建筑的设计习惯不符，因为没有一定关系且不通过延伸的屋面连接的单体建筑在视觉审美上不符合人们的喜好，因此本文依旧将这一类划分在单体建筑中。在组团式建筑中，面的延伸则体现在多层基面或屋面的延伸（图3.8.2、图3.8.3）。\n\n![](images/e29ce723d11756662f5426227bb634f93d4b7dc4f95393f8585d0a5c3b725e34.jpg)  \n图3.7面延伸变化建筑示例（1为罗比住宅，2、3为AI绘画生成）\n\n# 3.2.4 体的变化形成的空间类型\n\n# 3.2.4.1削减\n\n一种形式可以通过削减其部分体积的方法进行变化。根据不同的削减程度，形式可以保持其最初的特性，或者变成另一种类实体的形式。比如，一个正方体即使有一部分被削去，仍能保持其特性（如图3.8.1），或者变成一个逐渐接近球体的系列规则多面体。\n\n在单体建筑中，体的削减式变化主要体现在某个实体削减一部分，仍能保持其最初特性的空间中（图3.8.1、3.8.2）。\n\n![](images/938d3eca7397b1b87f7a96f2732ad05e27a344166a9c5ee80ae0de87ed50f68b.jpg)  \n图3.8削减（Al绘画生成）、削减（瑞典 Solar Egg）、从属（AI绘画生成）\n\n# 3.2.4.2 从属\n\n一种形式的实体之间互相连接，且实体之间存在主体与从属关系（图3.8.3）。在单体建筑中，实际上很多削减和从属关系并没有明确的界限，如图3.8.1和3.8.3中的空间形式既可以是削减关系也可以是从属关系。\n\n# 3.2.4.3增加\n\n建筑的所有其他形式都可以被理解为基本实体的变形，增加是对基本实体在某维度或多维度作增加处理的结果。\n\n![](images/ce15afc19cc3b802a287d7dd30bff5a81b23e2b453be52ef52732f4d3cb9e8b5.jpg)  \n图3.9增加形式的建筑示例 （图为AI绘画生成）\n\n在群组建筑中，增加主要表现为按一定规律有规则的递增且通过介质连接的空间（图3.9.1），在组团式建筑中，增加则表现为由多个具有近似性、或共同的视觉特征、或共同关系的实体组合形成的空间，其组合方式有实体间的旋转、贯穿等。\n\n![](images/e367f1d90a604054c57e0bca261a26a4751be69727f2d69393016c095cb76188.jpg)  \n图3.10增加的组团式建筑示例（1、2、3为贯穿形式，4为旋转，图片来源：生成）\n\n按上述点、线、面、体变化的分类逻辑，将建筑形体空间按外部呈现的特征进行归类汇总，如表3.1所示。\n\n按照点、线、面、体在单体建筑、群组建筑和组团式建筑中的变化，总的来说将建筑的形体空间分为表3.1中的表现形式。在实际的建筑中，一座建筑往往属于多种空间组合形式，如单体建筑中的削减和从属在某些意义上难以明晰；在水平方向呈线性分布并通过介质连接的群组建筑空间，同样也属于增加式的群组建筑；组团式建筑中的基面延伸和基面旋转难分彼此等等。另外，同一类空间形式在实际建筑中的体现形式也不同，例如，体增加式的群组建筑，若是两个实体通过介质连接，则多表现为大型公共建筑的主入口，若多个实体通过介质连接，则多表现为小型展览性公共建筑等。在训练的过程中根据需要，可通过实际建筑类型再次对建筑形体空间进行更加细致的划分，达到更加精确的控制目的。\n\n![](images/5a05a535f84ac6f8b63f5c571c4836325a2bc19bd11b320bbd80ab8201d6f457.jpg)  \n表3.1“点、线、面、体”的逻辑下建筑的形体空间分类示意\n\n# 3.3 预训练模型：图像-文本的对应\n\n预训练模型是机器学习的一种高阶实现方式，是机器学习在大数据时代与深度学习技术结合后的必然产物。对建筑师而言，它如同一个吸收了人类千年建筑智慧的数字助手，将设计从重复性劳动中解放，转而聚焦于创造性决策。正如 CAD取代了手工制图，预训练模型正在重塑设计创新的边界。\n\n如果说AI绘画中文生图是将文本转换为图像的过程，那么预训练模型就仿佛对文生图的过程提前进行了反向的生成预习。预训练模型如同建筑师的基础学科教育（材料学、结构力学、空间构成等），而文生图是方案设计实践（将客户需求转化为具体形式），二者共同构成AI绘画的完整能力链。在预训练模型学习的过程中，图像内容和标签的对应是一一连线的过程（图3.11）。向模型中添加一个新的内容或概念时，例如“极简主义”，需要选择该风格明显的图像，并在训练的过程中人工加入新的提示词“极简主义”，当图像中所有的实体要素如“树木、建筑、天空、人”都和相应的提示词文本对应之后，图像中剩余的、抽象的风格特征（如建筑风格、场景氛围）就会和新加入的提示词“极简主义”对应起来。这样就实现了新的图像要素和文本的对应，完成了文本内容的输入。\n\n![](images/18ba23ca9f2eb2439d0ede913ff051f7049e506273fbb153e28608767662bfe1.jpg)  \n图3.11预训练模型的图像-文本对应示例\n\n![](images/54b97adde29c1b1c82a3308a73c1e96692b69083e3c61e14de6948778f2d6290.jpg)  \n图3.12预训练模型的提示词输入示例\n\n# 3.4 本章小结\n\n本章围绕AI绘画技术在建筑设计中的应用优化路径展开，从生成步骤、文本内容和预训练模型的图像-文本对应三个层面进行系统性探讨，旨在建立符合建筑师思维习惯的人机协同设计框架。\n\n基于建筑师的逻辑思维，提出“分级提示词框架”作为AI生成的核心路径。该框架强调从空间组织（建筑形体、空间组合方式）的底层逻辑出发，逐步叠加建筑类型或风格特征与细节参数，形成“空间 $\\twoheadrightarrow$ 类型/风格 $\\twoheadrightarrow$ 细节”的递进式生成流程（图3.1）。此模式与建筑师“从空间组织为出发点”、“从整体到局部”的设计习惯相契合，通过分阶段控制生成内容，有效降低人机交互复杂度，同时保障设计成果的逻辑性与完整性。\n\n针对现有AI模型对建筑形体空间描述不足的问题，提出基于建筑学理论的文本分类体系，将建筑类型按建筑性格划分为教育、工业、商业等功能类别，强化类型特征对生成结果的导向作用；借鉴《形式、空间和秩序》中的理论，建立“点状-线性-面延伸-体变化”的四维分类框架（表3.1），并结合“单体-群组-组团式”建筑的空间关联特征，细化形体组合规则（图3.4-3.10）。例如，通过区分“群组建筑（非接触式介质连接）”与“组团式建筑（接触式连续体块）”，解决模型对空间组合关系的误读问题；强调实际建筑中多类空间特征的叠加性（如削减与从属的模糊边界），提出需结合具体建筑类型进行二次细分的动态调整策略。\n\n提出“建筑学导向”的预训练模型优化方向，类比建筑师的专业教育过程，模型需通过海量建筑图像-文本对学习，内化材料、结构、空间构成等隐性知识（图3.11）。通过预训练模型对设计原型的快速生成与迭代，推动设计者从技术性制图向创造性决策转型，形成类似“CAD替代手绘”的范式革新。\n\n本章构建的优化体系，通过分级生成逻辑、分类文本框架与专业化预训练模型的协同作用，初步实现了AI生成技术与建筑设计思维的深度耦合。该框架不仅改善了现有模型对建筑专业内容的理解偏差，更为人机协同设计流程的标准化提供了理论支撑与实践路径。\n\n# 第4章AI绘画生成建筑图像方法的优化的方式和步骤\n\n# 4.1 图像生成平台和模型训练工具选择的选择\n\n# 4.1.1 本研究AI 绘画平台的选择（Stable diffusion）\n\n在目前的AI绘画平台中，大部分AI绘画平台都支持上传图像进行风格迁移，可以看出这是目前这类平台的主流用法。其中支持文生图的AI绘画平台有DALL-E2、Midjourney、Stable Diffusion、DeepAI、百度文心一格，但在这几款平台中，除 Stable diffusion 外，都不支持用户上传个人模型，例如在 midjourney中，使用者需要不断垫图、修改提示词，也未必能获得符合预期的图像。另外，相对于其他AI绘画平台，Stable diffusion 生成速度也相对较高；与一些需要付费使用的AI绘画软件相比，开源免费使其拥有活跃的开发者社区，提供了大量预训练模型和插件。这在一定程度上解决了上文说到的生产者和使用者属于不同专业领域的问题。通过插件的辅助，非专业人员能一定程度上避开专业的计算机语言，更容易的训练和调整模型。因此，本文的图像生成平台选择在Stable Diffusion 中进行（图4.1）。\n\n在 StableDiffusion界面中，用户可以根据自己的需求选择合适的模型和插件，基于不同的大模型（Checkpoint）[56,57]和特征模型（Lora）[10,58,59]来生成特定风格的图像，还可通过小型的嵌入型模型（VAE），来对模型中的文本信息进行补足。seed 值是图像生成中一个关键的因素，seed值固定，其他条件都相同时，生成的图像结果是唯一的。另外，文章的重点在于在一定逻辑下对模型数据集的调整，Stable diffusion只是生成图像和检测结果的平台，除了基础操作，本文不会涉及平台的使用技巧。但由于各AI绘画平台的模型参数和格式会有不同（尽管使用插件可以互相转换），因此在模型调整和训练的过程中，还是需要依照最符合Dtablediffusion 的格式来进行，避免出现不必要的问题。\n\n![](images/298be86b39f9a673ae299e0ed0cd4eaf32479925b727c9a3abc474cc334420e8.jpg)  \n图 4.1 Stable Diffusion 文生图界面\n\n# 4.1.2 Stable diffusion 中大模型(Checkpoint)与特征模型(Lora)的关系与特点\n\n在 Stable diffusion,以及大部分当下的主流AI绘图软件中,大模型(check point）和特征模型（Lora）是生成图像的主要参照，两者都属于预训练模型。大模型是指具有大规模参数和复杂计算结构的机器学习模型，通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。这些模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。\n\nLora（Low-Rank Adaptation of Large Language Models）模型，是一种针对大型语言模型的微调技术。它通过引入低秩矩阵来减少参数数量，降低微调成本，同时保持模型的性能。Lora的核心思想是在大型预训练模型中引入一对低秩矩阵，这些矩阵包含的参数数量远少于直接微调整个模型所需的参数。在微调阶段，只训练这些低秩矩阵的参数，而保持原模型其余部分的权重不变。Lora模型的工作原理基于低秩近似（low-rank approximation）的思想。在大型预训练模型中，Lora选定一组特定层（通常为Transformer 的注意力层），并在这些层中引入一对低秩矩阵。这些低秩矩阵包含的参数数量远少于直接微调整个模型所需的参数。在微调阶段，只训练这些低秩矩阵的参数，而保持原模型其余部分的权重不变（即冻结）。通过这种方式，Lora 能够在保持原有模型性能的基础上，以较小的计算资源和数据量实现模型的快速适应特定任务或领域。\n\n大模型和Lora 模型在 Stable Diffusion 中扮演着不同的角色，它们之间的关系可以这样理解：\n\n大模型是 StableDiffusion 进行图像生成的基础和核心，提供了强大的基础性能，它包含了生成图像所需的所有信息。这些模型的文件通常较大，不同版本的大模型经过不同数据的训练，能够生成不同风格或特定对象的图像。\n\nLora 模型则是一种轻量级的模型，通过在大模型的基础上增加少量的可训练参数来进行微调，从而适应特定的任务或风格。Lora模型的引入降低了训练的门槛，提供了训练和部署时的效率优势，使得非专业人员也能在个人电脑上尝试训练自己的模型。Lora模型可以用于图像分类、目标检测、图像生成等多种任务，虽然其性能可能略逊于大模型，但其快速的运行速度和较小的模型体积，允许在不增加太多计算和存储成本的情况下，对大模型进行特定领域的调整和优化，这在大模型微调中尤为重要。使用Lora微调后的模型，在推理时可以直接将原预训练模型权重与训练好的Lora权重合并（4.2），\n\n![](images/02093745931b1f4abc2463c6aca9262dc0a59a72b3978f43f3116de2912ee657.jpg)  \n图4.2大模型（Checkpoint）、微调模型（Lora）与提示词（prompt）的关系（图片来源：\n\nhttps://www. bilibili.com/video/BV1JF2GYsEUP?spm_id_from=333.788.vi deopod. episodes&vd_source=996e889811453488e57602034bddc942&p=7 ）\n\n因此相比原始的大模型不存在推理时的额外开销。由于其小尺寸和参数合并的特性，使得其在存储和部署时都非常高效，简化了模型管理和部署过程。总体来讲，大模型提供了生成高质量图像的基础能力，而Lora模型则提供了对这些图像进行特定风格或细节调整的灵活性，两者结合使用可以更好地满足用户的个性化需求。\n\n# 4.1.3模型训练工具选的择\n\n上文介绍了预训练模型的图像-文本对应原理，以及新的提示词的输入原理。但如何在实际操作中完成这个过程，SD-scripts 则一定程度上解决了这个问题。\n\nSD - scripts 是一个开源工具集，主要支持 Stable Diffusion 模型的训练、微调和管理，为研究者和开发者提供了一套高效且灵活的脚本工具，以满足生成式图像模型在特定领域或个性化需求中的应用。其核心功能围绕模型的训练优化、数据集处理以及模型管理展开，涵盖了从数据预处理到模型部署的全流程支持。在模型微调方面，SD-scripts 提供了多种先进的技术方法，例如 Dreambooth。\n\nDreambooth 是一种基于生成式模型微调的技术[4,60，61]，旨在通过少量图像数据将特定主题或对象嵌入预训练模型，从而生成包含该主题的高质量图像。其核心原理是通过对预训练模型进行微调，使其在生成图像时能够识别并保留特定主题的特征,同时保持模型的多样性和泛化能力,这便是大模型的微调了。使用 SD-scripts,可以通过dreambooth训练逐次的将单个概念输入到大模型中进行微调，完成对建筑形体空间的训练。\n\n在数据集处理方面，SD-scripts 提供了图像裁剪、缩放、标注等预处理工具，避免了大量重复繁琐的人工裁剪图片流程，同时确保训练数据的质量和一致性，支持自动生成图像标签，便于训练时与文本提示词对齐。在模型管理方面，工具集支持模型权重的合并与格式转换，使用户能够灵活地结合不同模型的优势，并将其应用于不同的平台或框架。\n\n为了避免由于AI绘画内部逻辑导致的不必要的工作量，本文先选择输入一种建筑形体空间形式和一种建筑类别，例如“竖向的单体线性空间”和“幼儿园类建筑”。优先选择这两种类别是因为前者在建筑中多表现为塔楼，风格上多为现代或数字风格，构件上多有大面积的玻璃幕墙，而后者为低层，建筑色彩明快且饱和度高，在同时调用这两个概念时，使生成结果易于对比。\n\n# 4.2模型数据库的建设\n\n# 4.2.1预训练模型的类型选择\n\n上文说到Lora的性能略逊于大模型，因此在进行模型训练时首选将建筑的形体空间分类和建筑性格分类都对应到大模型中。大模型的训练需要巨大的算力成本，对非专业人员来说操作难度和繁琐程度都很大，在出现报错的时候往往无法找到问题本源，虽然由于 SD-scripts 的存在使个人训练大模型得以实现，但是在训练时一次只能完成一个概念的训练，需要进行提示词的逐个输入，逐步对前一个模型进行覆盖，最后获得覆盖了全部图像和文本信息的大模型（图4.3）。\n\n![](images/6de6e587a007148c61f04ba3b7db1c9b410b8caf11ccd1130b8608ed9d5e6ee2.jpg)  \n图4.3预训练模型的类型选择(Dreambooth）和训练过程示意（图片来源：作者自绘）\n\n# 4.2.2主要参数分析\n\n# 4.2.2.1 分辨率与分桶策略\n\n在训练生成模型时，要求输入图像的分辨率必须是64的倍数，这一限制主要源于模型的架构设计和计算效率的优化需求。生成模型通常基于卷积神经网络（CNN）[62-64]或 Transformer 架构，这些架构依赖于多次下采样（down sampling）和上采样（up sampling）操作，每次操作的分辨率变化通常是2的倍数。为了确保这些操作能够正确执行并最终恢复原始分辨率，输入分辨率必须满足 $6 4 = 2 ^ { 6 }$ 的条件，即经过6次下采样后分辨率降至 $1 \\times 1$ ，再通过6次上采样恢复。此外，64 的倍数分辨率能够更好地利用现代深度学习框架和硬件的并行计算能力，避免因填充（padding）操作引入额外的计算复杂度和内存开销。如果输入分辨率不符合这一要求，可能会导致特征图不对齐、计算错误或性能下降，从而影响生成图像的质量和训练效率。因此，这一限制不仅是模型架构设计的必然要求，也是优化\n\n# 计算效率和硬件性能的重要措施。\n\n![](images/4d5217746588b2ca94d71b2a7e58406fc948f7a8b24b68b3c5cd6052dc7364c4.jpg)  \n图4.4图片分辨率与分桶策略\n\n分辨率的选择直接影响训练的计算复杂度和生成图像的质量。较高的分辨率可以生成更高质量的图像，但会增加计算负担；较低的分辨率则可以提高训练效率，但可能降低图像质量。分桶策略允许模型处理不同分辨率的图像，模型会根据图像的分辨率将其分配到不同的桶（bucket）中（图4.4），每个桶对应一个特定的分辨率范围，而无需将所有图像调整为固定分辨率。这提高了训练的灵活性和效率。通过合理设置min_bucket_resomax_bucket_reso和bucket_reso_steps,可以优化模型对不同分辨率图像的适应能力。\n\n# 4.2.2.2 收敛（Convergence）\n\n收敛（Convergence）是机器学习和深度学习中一个核心概念，描述了模型在训练过程中逐渐接近最优解的过程。具体来说，收敛是指模型的损失函数（lossfunction）随着训练迭代次数的增加，逐渐趋于稳定，最终达到一个最小值或接近最小值的状态。影响收敛的训练参数主要有学习率（LearningRate）、批量大小（Batch Size）、优化算法，除了参数因素外，还有数据集的数据质量，数据的质量和多样性直接影响模型的收敛性，如果数据噪声较大或分布不均匀，可能导致模型难以收敛。\n\n# 一．批量大小（Batch size）\n\n批量大小是指在每次训练迭代中使用的样本数量。例如，如果批量大小为 8,则每次训练迭代会使用8个样本计算梯度并更新模型参数（图4.5）。\n\n当批量大小较小时，梯度估计的噪声较大，可能导致训练过程不稳定，但较小的批量大小也可能帮助模型跳出局部最优，找到更好的全局最优（在损失函数的参数空间中，局部最优是指某个区域内的最低点，但不是整个空间的最低点，如余弦、余弦退火重启等）。这主要是由于梯度估计的噪声（gradient noise）和随机性（stochasticity）的引入。在深度学习中，梯度是通过批量数据计算得到的。当批量大小较小时，梯度估计基于较少的数据样本，因此会引入较大的噪声。这种噪声可以被看作是一种随机扰动，使得梯度下降的方向不完全准确，从而增加了模型探索参数空间的可能性。\n\n当批量大小较大时，梯度估计更加准确，由于每次更新基于更多的样本，训练过程更加稳定，噪声较小，收敛性更好，但可能导致模型过拟合，尤其是在数据集较小的情况下。因为较大的批量大小会使模型倾向于学习训练数据的整体分布，而忽略细节。\n\n![](images/e2532948dbea461fa8293451206810b30d52048bd8cc1b4bdd547a5380e0ae09.jpg)  \n图4.5批量大小与梯度累加步数\n\n在本次大模型的微调中，参与训练的数据集为40张，为了保证学习的稳定性且避免过拟合，将批量大小设置一个较小的数值2。如果显存有限，可进一步将批量大小调小，启用梯度累加步数（N），来模拟大批量大小的效果。计算公式为：\n\n例如要获得批量大小8的训练效果，在当前批量为2的情况下， $\\Nu = 4$ 。\n\n# 二．学习率（Learning rate）\n\n学习率是优化算法（如梯度下降）中用于更新模型参数的步长，可以类比为人们步行时，每一步的大小，初始学习率通常设置在le-4至1e-3之间。学习率决定了每次参数更新的幅度，较大的学习率可以加快收敛速度，但可能导致训练不稳定；较小的学习率可以提高训练稳定性，但收敛速度较慢。常见学习率调度器类型有：线性学习率（linear）、余弦学习率（cosine）等。\n\n寻找最佳学习率是深度学习训练中的一个关键步骤，因为学习率直接影响模型的收敛速度和最终性能，通过学习率预热步数来寻找最佳学习率是目前常用的方法。学习率预热步数是深度学习训练中的一种策略，用于在训练初期逐步增加学习率，而不是从一开始就使用较大的学习率。这种策略可以帮助模型更好地初始化，避免训练初期的不稳定性，可以帮助模型更好地初始化，避免在训练初期陷入局部最优。在训练初期使用较小的学习率，可以防止模型过早拟合，随着学习率的逐步增加，模型可以更好地探索参数空间。学习率预热步数通常设置为总训练步数的 $5 \\%$ 到 $1 0 \\%$ 。其中，\n\n总步数 $\\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } }$ 训练轮数\\*（数据集数量/批量大小）\\*训练集重复次数\n\n# 4.2.3Dreambooth 图像和文本内容的输入\n\n# 4.2.3.1建筑形体空间类型的输入流程 (以竖向线性空间为例)\n\n# 一．图片素材的收集与处理\n\n如果说模型的训练是将图片与关键词提前对应的过程，那么这个对应的精准度则尤为重要，不可过高，也不可过低。就好像临摹一幅画作，如果临摹的相似程度过低，那么便失去临摹的意义，如果临摹的相似度过高，则失去了创新性。需要控制一个合理的度，使AI在学习的过程中，既能学到关键词的精髓，又能融会贯通，发挥创造性。所以在收集图片素材时，在符合大分类的条件下，图片内容中建筑的形体和风格都要尽可能的多样化。例如，在训练竖向的单体线性建筑空间时，图片素材中实体的形式要多种多样，有圆柱状、方柱状，异形柱状、扭转的异形柱状等（图4.6）。\n\n![](images/f93c09e7be75be46077e724289dac7baac3ff422d36b113197d5da0e714de17e.jpg)  \n图4.6竖向的单体线性建筑空间示例 （图片来源：谷德网）\n\n在当前的 Stablediffusion 中，大模型的底模是基于 $5 1 2 ^ { * } 5 1 2$ 分辨率的图像进行训练的，因此在训练之前，需要把图片素材裁剪到 $5 1 2 ^ { * } 5 1 2$ 大小，并使建筑主体位于图片中央。使用 SD-scripts 自带的图片预处理工具(图4.7.1)\n\n可以帮助使用者批量裁剪，但有些图片可能建筑主体过于偏离中央，导致机器裁剪以后扔不能位于图片中心，需要手动进行调整。为了获得一定长款比例的图像，本文将图片裁剪为 $5 1 2 ^ { * } 7 6 8$ 像素大小，或者启用分桶策略对图片的分辨率范围进行界定。\n\n# 二．图像标签处理\n\n图片收集并统一裁剪以后，需要对图片中的元素使用提示词进行标注，以备模型训练时使用。提示词的标注是一个文生图的反推过程，在 SD-scripts中，也有对图片进行批量反推的标签处理器工具，WD14-tagger（图4.7.2），\n\n![](images/fdbaef3798df7cf5302cd3f932870933e7024d7db9c8bbe73ad0a9d758a4fae5.jpg)  \n图4.7图片预处理界面、WD14-tagger 界面\n\n![](images/9a37fc94617d05310f294d709df4380bf78442e26634291478110146ef8c21e7.jpg)  \n图4.8 BooruDataset Tag Manager 提示词管理界面\n\n输入素材目录进行反推即可。为了方便后面对提示词进行处理，本文使用了一款方便可视化的标签处理工具BooruDataset Tag Manager（图4.8）），一目了然，提高处理效率。\n\n根据上述原理，在反推标签时，阈值适当调高，尽量对图片进行完整的对应，当然也需要进行人工检查，纠正部分AI识别的错误。例如图片中没有的物体或概念，需要一一删除，逐张检查。这个过程使用 BooruDataset Tag Manager，窗口比较清晰，便于对应。检查无误以后，点击右侧工具栏批量添加，给每张图都添加提示词“tttt”，并置于顶部（如图4.8）。\n\n提示词的名称是自定义的，为了避免和大模型中已有的提示词混淆，造成训练结果不佳，需要保证这个词没有任何含义。在专业的大模型训练中，不需要使用没有实际意义的提示词，本文这样做是由于没有强大的计算机算力支持，不想去混淆原有的已经通过大量数据训练的提示词，造成对训练结果的影响，因此不如使用新的词汇。\n\n# （三）参数设置\n\n在调整参数之前，需要在 SD-scripts/train 文件路径中新建以“tt”命名的文件夹这样做是为了方便记住提示词），下再新建子文件“6_ttt”，将图片和打好的标签打包放入，其中6是指训练集重复次数，后续进入参数调整。\n\n![](images/06306e397f31eb6d8a6c7856b50aedc43d0791277aac206e3d330b240b6035c3.jpg)  \n图4.9Dreambooth 训练过程中的学习率曲线\n\n为了寻找本次训练的最佳学习率，需要将学习率调度器类型设置为contant,各种学习率都设置为1，启动一次训练，在训练步数达到总步数的1/10 后，在SD-scripts 的TensorBoard模块中读取训练趋于稳定后的常数值即为本次训练的最佳学习率（图4.9）。本次稳定后的学习率数值为 $1 . 0 5 \\mathrm { e } { - 4 }$ 。此时终止训练，重新\n\n填入该数值为最佳学习率。\n\n为了获得良好的训练结果，避免过拟合，首选余弦退火函数，需要注意重启次数不要超过4,同时选择与之匹配的 AdamW8bt 优化器。文本编码学习率设置约为总学习率的1/2。在训练时，选择随机打乱标签，并保持第一个标签内容不变，各项参数设置完毕启动训练。训练的大模型基础选择“ChilloutMix”（目前应用较为广泛的写实风格大模型）将其命名为ori方便记忆。最后的参数调整如下：\n\nmodel_train_type $=$ \"sd-dreambooth\"   \npretrained_model_name_or_path $=$ \"./:   \n$\\mathbf { v } 2 =$ false   \ntrain_data_dir $=$ \"./train/tt\"   \nresolution $= \" 5 1 2 , 7 6 8 \"$   \nenable_bucket $=$ true   \nmin_bucket_reso $= 2 5 6$   \nmax_bucket_reso $= 1 0 2 4$   \nbucket_reso_steps $= 6 4$   \noutput_name $=$ \"ttt\"   \noutput_dir $=$ \"./output\"   \nsave_model_as $=$ \"safetensors\"   \nsave precision $=$ \"bf16\"   \nsave_every_n_epochs $= 2$   \nmax_train_epochs $= 1 0$   \ntrain_batch_size $= 2$   \ngradient_checkpointing $=$ false   \nlearning_rate $= 0 . 0 0 0 1 0 5$   \nlearning_rate_te $= 0 . 0 0 0 0 5$   \nlr_scheduler $=$ \"cosine_with_restarts\"   \nlr_warmup_steps $= 0$   \nlr_scheduler_num_cycles $= 1$   \noptimizer_type $=$ \"AdamW8bit\"   \nlog_with $=$ \"tensorboard\"   \nlogging_dir $=$ \"./logs\"   \ncaption_extension $=$ \".txt\"   \nshuffle_caption $=$ true   \nweighted_captions $=$ false   \nkeep_tokens $= 1$   \nmax_token_length $= 2 5 5$   \nseed $= 1 3 3 7$   \nclip_skip $= 2$   \nno_token_padding $=$ false   \nmixed_precision $=$ \"bf16\"   \nxformers $=$ true   \nlowram $=$ false\n\ncache_latents $=$ false cache_latents_to_disk $=$ false persistent_data_loader_workers $=$ true\n\n# （四）．训练结果与问题检查\n\n在上一节的训练结束后，获得5个扩展名为 safetensors 的模型文件，文件名分别为“tttt”、“ttt-002”、“ttt-004”、“ttt-006”和“ttt-008”。这是由于设置了每5轮保存一次。结果检测时应当先选择最终的“tt-008”，在 Stablediffusion 的大模型选项中加载“tttt-008”，文生图的提示词中输入“ttt”，生成一轮，获得如下结果(图 4.10）。这是一种未收敛的现象，说明在训练的过程中未曾找到最优解，使模型训练不稳定，导致最终训练的失败。根据上文提到过的参数对训练结果的影响，可能的原因有3个：\n\n![](images/94397a3f9bba04d7b9e8493a46896223f91dd16002f7b0baab2cee2a9bd74333.jpg)  \n图4.10 Dreambooth 训练时出现的未收敛现象1\n\n![](images/25f4346683cb9525cd4dd8ab5daa44c4e39a7d4bb4ed25a985231000116dea2c.jpg)  \n图4.11 Dreambooth 训练时出现的未收敛现象2\n\n$\\textcircled{1}$ 批量大小设置的过小，引入的噪声过大导致了训练的不稳定；\n\n$\\textcircled{2}$ 余弦退火重启函数的学习率变化太多、或重启次数设置过大，在训练总步数较小的情况下导致了训练的不稳定；\n\n$\\textcircled{3}$ 训练步数过少，学习不充分。或者由于AI本身的不稳定性，导致单次失败。\n\n为了验证以上猜想，保持参数不变重新训练一次，检测是否是由于本次训练的数据异常导致结果失败，实验结果依旧相同。剩下三个原因进行依次排查，在保持其他参数不变的情况下，先加大批量大小为8，再进行一轮训练，结果如图4.11。与4.10 中生成图像的质量相似，建筑图像在形体上已经即将达到拟合，但未收敛的情况还是没有太大的改变，说明并非导致未收敛的原因另有其他。再对学习率函数进行测试，保持其他因素不变，将余弦退火重启函数改为常数（contant），与之对应选择DAdaptLion优化器，学习率默认即可，优化器会自行调节。得到的生成结果如图4.12，在生成质量和创意上表现都较好。\n\n![](images/263c54040c086ed139d5424a3d472502fe88c520c6081278d19b8f82d8bf8f25.jpg)  \n图4.12“竖向的线性单体建筑空间”Dreambooth概念训练生成结果\n\n# 4.2.3.2建筑类型/性格的输入 (以幼儿园类建筑为例)\n\n在风格模型的训练中，参数的设置和空间类型训练时的设置相同，但通常使用正则化数据集来提高结果的泛化，正则化数据集是深度学习中提高结果泛化性的一种重要手段。它通过在训练过程中引入额外的数据或约束，帮助模型更好地泛化到未见过的数据。例如，要训练一个概念“平涂风的鸡”（图4.13.1），在训练时如果不使用正则化数据集，生成的结果多为平涂的画风体现在卡通画风的鸡。而正则化数据集的引用就好像给本次的模型训练进行了预习（如图4.13.2、4.13.3），提前预习了现实生活中鸡的形态，最后得到的结果如图4.13.4，具有真实的鸡的形态，又具有训练集图片的平涂画风。同样的，在建筑类别和风格的训练中使用正则化数据集，会提高建筑特征在多种建筑空间上的表现能力，增强泛化性。\n\n![](images/1d0a4e08d3ba72ff88bc79fa86a0db33bb35c8502eca287de62ac055d9eb5b4c.jpg)  \n图4.13正则化数据集的作用举例\n\n# （一）参数设置\n\n上文已经详细介绍了关于空间类型的训练方法，在本节中需要继续向上一步的大模型中添加幼儿园类建筑这一概念，其流程建筑空间的概念训练基本一致，但在训练形体空间类型时，需要训练结果尽可能的多样化，因此理论上会优先选择余弦退火重启函数，然而在建筑特征的训练中，更期待获得稳定的表现效果，因此需要相对稳定的contant函数，与之对应选择DAdaptLion优化器，根据训练动态自动调整学习率。由于DAdaptLion 优化器在dreambootn 的训练中把学习率和文本编码学习率限制为一致的数值，所以两者都输入1，而后优化器会自动根据训练过程调整。大模型基础选择4.2中训练的“tt”，以对其进行再次微调。需要修改的参数为文件的输入路径、输出名称，和以下参数。\n\n![](images/bc5e17fedaa6e82615369f81000b148c2f3245ecf29cf08f0e7c545369efbf5b.jpg)\n\n# （二）正则化数据集的使用\n\n在“幼儿园类”建筑概念的训练中，选用的正则化数据集应当与训练集素材的建筑空间形态差距较大、且含盖尽可能多的空间样式，数量在100 张以上。正则化数据集不需要进行标签反推，新建一个文件夹名为“reg”放入其中，路径为train\\kindergartenx\\reg。本次训练选取了100 张风格比较鲜明的的幼儿园特征的训练集素材（图4.14），正则数据集150张，添加完正则数据集路径之后即可启动训练。\n\n![](images/cabf42f9df14c5ccf711cf7db42f44bd50b8c52536f44b1a5c59f175e5cfde73.jpg)  \n图4.14幼儿园风格训练集图像示例\n\n# （三）训练结果与问题检查\n\n选择后缀为 OO8 的最终模型进行图像生成测试，在 Stable diffusion 中输入关键词，本文设置的为“kindergartenx”，结果如图4.15。\n\n![](images/41ffc9b87531ae9b7ce34bab26fa29c03fb33b7b02fb845bb55f426cabb79bc2.jpg)  \n图4.15幼儿园类建筑训练过程中的不稳定现象\n\n![](images/bdc4c150979dfb3b03a9b4732facdb06bb3186bcd9d7e48e10794569b2737378.jpg)  \n图4.16 (与训练集素材差距过大的正则化数据集示例)\n\n训练结果显然是失败的，但与上文“竖向的线性单体建筑空间”概念训练时的失败类型不同，上文是一种彻底的不稳定和未收敛，本次结果中已经有了部分幼儿园建筑的风格特征，从图片来看更像是学习不充分和图片表现风格的杂糅。造成此结果的原因可能有两个，一是训练的步数少，导致的学习不充分；二是正则化数据集中，存在于训练集图像画风差距过大的图像。理论来讲150 张正则化数据集在数量上是足够满足需求的，因此优先检查正则化数据集的内容并除去和训练集相比差距过大的图片（如图4.16）。\n\n由此可见，尽管在建筑类型概念的训练中，期待结果可以很好的泛化到所有类型的建筑形体空间之上，但需要保持图片的重心是建筑，不可有过多构图上的差距，不可杂糅过多环境因素。在删除类似图4.10 的正则化数据集图像之后，手动添加继续补足至150张。保持其它参数不变，再次启动训练，并检验结果（图4.17）。\n\n这个结果在当前来看是比较成功的，在不同建筑形体和空间中，幼儿园建筑的风格融合的比较好。\n\n![](images/8cbc17245d7808210624b5aae7d6f5b1ad9842cfb884e04445a203fbf39bde36.jpg)  \n图4.17 Dreambooth微调后幼儿园类建筑概念生成结果示例\n\n# 4.3预训练模型图像生成测试与调整\n\n# 4.3.1建筑形体空间类型&建筑类别/性格同时输入大模型\n\n# （checkpoint）时的图像生成测试\n\n在本章中，通过参数调节，获得了“竖向的线性单体建筑空间”和“幼儿园类建筑”的概念引入，在单独调用两个概念时，都能有较好的表现。本文的最终目的是可以通过输入建筑形体空间类型和建筑类型（性格）来获得预期的建筑图像，基于此，需要先对以上两个概念进行同时调用，测试一下表现结果。后续还有很多同类概念需要逐次输入，若表现不佳，应及时调整训练策略，来避免大量的无效工作。\n\n![](images/49115319b8c67295fcf724dc2ab0a049118b718c38dd95cfaa5e181078411f7e.jpg)  \n图4.18大模型中同时使用“竖向线性单体建筑”和“幼儿园类建筑”两个概念时的生成结果示例\n\n在 Stable diffusion 中，加载两个概念中最终调整后获得的模型，这时模型同时具有“竖向的线性单体建筑空间”和“幼儿园建筑风格”两个概念，理论上是可以做到同时调用，并准确控制的。然而当同时输入两个关键词“ttt 和\n\n“kindergartenx”时，生成结果如图4.18。很明显，生成结果中只有幼儿园的建筑特征，并没有竖向的线性空间，初步结论是受提示词的权重影响。\n\n在AI绘画的语句控制中，对关键词添加括号，并在“：”后添加数值，可以调节提示词对图像控制的比重，例如上述生成结果中可能是风格关键词对图像的控制较大，所以改写为（kindergartenx：0.9）。为了使生成结果更直观且易于对比，先固定 seed 值（上文说到过 seed 值会使生成结果唯一），将“tttt”比重保持1不变，逐次调整风格提示词的比重，结果如表4.1。\n\n![](images/3838f1d405aeb45527c5e84ad2f074bcedc49914312608fba8466ce7f2723eb8.jpg)  \n表4.1微调后大模型同时调用“竖直线性单体建筑空间”和“幼儿园类建筑”图像生成结果\n\n![](images/8b6749ff4edce4084176cde767e4ad4300c3ceb794dc9ca35726499efdcc387c.jpg)\n\n在表4.1中，风格的权重调整仅能影响建筑性格，对生成建筑图像的空间构成基本是无效的。这个结果说明在生成图像时，并没有成功调用提示词“ttttt”中的内容，在理解了优先拟合的概念以后，可以合理的解释这个现象。\n\n优先拟合（PriorityFitting）模型训练中一种常见的现象，表现为模型在训练过程中将特定风格（如建筑风格）与特定对象（如建筑）紧密耦合，导致在调用该风格时，模型会优先生成与风格相关的对象。这种现象的根源在于训练数据中风格与对象的共现性，以及模型在潜在空间中学习到的特征高度相关性。优先拟合的影响主要体现在风格与对象的强耦合性上，使得生成图像的多样性受到限制，难以单独调用风格而不生成对象，例如妆容势必会体现在人脸上一样（图4.19）。在上文的模型训练中，“竖向的线性单体建筑空间”已经是一个依赖对象本体“建筑”的生成内容了，加上“幼儿园类建筑”也需要体现在对象本体“建筑”上，两者同时调用，必会在生成建筑时获得冲突，导致很难控制生成建筑的形体空间内容。\n\n![](images/e1a03b75aa6d3727b39edc6c53047e0eba14c7103738c9cd9344732ef07fd876.jpg)  \n图4.19基于人脸妆容的概念优先拟合举例\n\n这是在个人模型训练中无法解决的问题，也因为优先拟合的存在，使个人训练中，建筑形体空间和建筑性格都输入同一模型的方法不可用。\n\n# 4.3.2预训练模型类型的调整\n\n上文介绍了Stablediffusion 中图像生成的模型构架，以大模型为主要框架，同时受Lora 的微调和风格影响。在将建筑形体空间类型和建筑性格都输入大模型的的方法失败后，调整为将建筑形体空间类型输入大模型、建筑性格类型输入到lora。在文生图的过程中，加载Lora但不使用其中的提示词，也是能触发模型风格的，这在一定程度上解决了建筑风格和建筑本身优先拟合的问题。\n\n通过一个生成案例进一步验证，加载大模型“ChilloutMix”，和个人训练的幼儿园类建筑模型，将 seed值固定为40（随机），输入提示词“treehouse,”（选择这个关键词因为其风格特殊，大模型中一定有且在lora 模型中没有涉及，方便对比生成结果），在 Stable diffusion 中只加载大模型时，生成结果如图4.20左，图中木屋材质比较单一，风格比较朴实。再同时加载幼儿园风格的Lora 模型，但不继续添加任何关键词，生成结果如图4.120右。两张图都依赖于大模型中的“ treehouse”主导生成，右图不仅具有左图的基本结构，其材质和风格还发生了变化，光照也更加真实。也就是说，Lora作为风格模型，即使不使用关键词触发，也确实会对大模型进行风格调整，两张图出现的构图差异是由于两个模型训练时数据集的图片素材的构图不同。\n\n![](images/0b7da4a7285ae022911ccb825e86a730bf5c0fc0edbeaf733ea62bb93677c19c.jpg)  \n图 4.20大模型“ChilloutMix” $^ +$ Lora“kindergartenx”生成 tree house 图像示例\n\n（图片来源：AI生成）\n\n根据以上生成规律，初步验证了在大模型中输入建筑的形式和空间特征，将建筑类型（性格）或风格输入Lora，是比较合理的选择（图4.21）\n\n![](images/8c11b25426817589ba1e39fe7be498036d5010b15701b07650be5a3f7072cc71.jpg)  \n图4.21预训练模型的类型调整和训练过程示意 (图片来源：作者自绘)\n\n# 4.3.3Lora文本内容的输入 (以幼儿园类建筑为例)\n\nLora 训练的操作流程在 SD-scripts 中与 dreambooth 训练的操作流程中基本相同，新嵌入的提示词和模型名称依旧为“kindergartenx”，同样需要正则化数据集，学习率调度器使用contant，优化器使用DAdaptation。在 lora 的训练中，相比dreambooth增加了 U-net学习率，和另外两个学习率同样设置为1。参数设置如下，训练结果为图4.22。\n\nmodel_train_type $=$ \"sd-lora\"   \npretrained_model_name_or_path $=$ \"./sd-mode   \n$\\mathbf { v } 2 =$ false   \ntrain_data_dir $=$ \"./train/kindergartenx/train\"   \nreg_data_dir $=$ \"./train/kindergartenx/reg\"   \nprior_loss_weight $= 1$   \nresolution $= \" 5 1 2 , 7 6 8 \"$   \nenable_bucket $=$ false   \nmin_bucket_reso $= 2 5 6$   \nmax_bucket_reso $= 1 0 2 4$   \nbucket_reso_steps $= 6 4$   \noutput_name $=$ \"kindergartenx\"   \noutput_dir $=$ \"./output\"   \nsave_model_as $=$ \"safetensors\"   \nsave_precision $=$ \"bf16\"   \nsave_every_n_epochs $= 2$   \nsave_state $=$ false   \nmax_train_epochs $= 1 0$   \ntrain_batch_size $= 2$   \ngradient_checkpointing $=$ false   \nnetwork_train_unet_only $=$ false   \nnetwork_train_text_encoder_only $=$ false   \nlearning_rate $= 1$   \nunet $\\mathrm { l r } = 1$   \ntext_encoder_li $\\mathit { \\Theta } : = 1$ lr_scheduler $=$ \"constant   \nlr_warmup_steps $= 0$   \noptimizer_type $=$ \"DAdaptLion\"   \nnetwork_module $=$ \"networks.lora\"   \nnetwork_ $\\dim = 1 2 8$   \nnetwork_alpha $= 6 4$   \nlog_with $=$ \"tensorboard\"   \nlogging_dir $=$ \"./logs\"   \ncaption_extension $=$ \".txt\"   \nshuffle_caption $=$ false   \nweighted_captions $=$ false   \nkeep_tokens $= 1$   \nmax_token_length $= 2 5 5$   \nseed $= 1 3 3 7$   \nclip_skip $= 2$   \nmixed precision $=$ \"bf16\"   \nlowram $=$ false   \ncache_latents $=$ true   \nlowram $=$ false   \nxformers $=$ true   \nlowram $=$ false   \ncache_latents $=$ false   \ncache_latents_to_disk $=$ false   \npersistent_data_loader_workers $=$ false\n\noptimizer_args $=$ [ \"decouple=True\",\"weight_decay ${ \\it \\Omega } = 0 . 0 1 ^ { \\prime }$ ', \"decouple=True\",\"weight_decay=0.01\"]\n\n![](images/a3ec07c9c2aa0095c5083ca8b723c5ef9b6cf77d6d580e5b1217d28f8a3f425d.jpg)\n\n![](images/5ae846ccc00576672c7271c7765c673c3b0f03ec3660a12c53354fb9322a05ed.jpg)  \n图4.22 Lora模型\"kindergartenx”生成幼儿园类建筑结果示例（图片来源：Al生成）  \n图 4.23 Stable diffusion 文生图提示词界面\n\n# 4.3.4建筑形体空间类型（checkpoint）&建筑类型/性格（Lora）图像生成测试\n\n在 stablediffusion中，对已经输入“竖向线性单体建筑空间”的大模型和输入“幼儿园类”建筑的Lora 同时进行生成测试。提示词应当只输入“ttt”，不输入“kindergartenx”，可以调用大模型中的空间结构和lora中的风格特征，固定 seed值，逐步调整Lora的权重（如图4.23），生成结果如表4.2，左侧为大模型和Lora的权重比例。\n\n![](images/e28900d35418c6e870a90ee5ee7793f858c971e16e80344601535d9555e41c4e.jpg)  \n表4.2微调后的大模型“ttt”&Lora 模型“kindergartenx”图像生成结果1\n\n![](images/ea0046b696cf5a1c4a4e31292876615115f947473c77862de45744c1d367ab68.jpg)  \n图像生成结果\n\n![](images/f71223af82449fee2c48a388a9a9e7770adbee8847e142a9072c0ae4d4846c76.jpg)\n\n根据表4.2的结果，基本可以得出在不调用Lora关键词的情况下，Lora 的内容仅对大模型生成的内容作风格调整，而较少的去改变画面的结构和生成内容，这使同时调用大模型中的建筑形体空间类型和Lora 中建筑性格的目标得以实现。生成的建筑图像空间类型为“竖向的线性单体建筑空间”，随着“幼儿园类建筑”Lora 权重的增加，建筑特征上展现出来的“色彩明快、高饱和度，线条圆滑、”的程度越来越高。图中出现的果盘和楼层断裂推测是在当前 seed 值下的取样特征问题。为验证，重新随机一个seed值，再生成一轮结果（表4.3）。\n\n![](images/b7b5e703bd9ef096010f96a61f20a6156abd29d470b3a1b8994c1aa2429c815e.jpg)  \n表4.3微调后的大模型“ttt”&Lora 模型“kindergartenx”图像生成结果2\n\n大模型：Lora 图像生成结果 1:0.8 000 1:0.6 1:0.5 :0.4 n00n\n\n![](images/898225ffc1f711be3dbf6f6b8fd12527e673b63ae0c8b8f366453935722fe504.jpg)\n\n根据本轮的生成结果，当前的方法是可行的。后续需要按照上述方法和参数，在本文对建筑形体空间和建筑类型的分类之下，将建筑的空间分类逐次输入到现有大模型中，将建筑类别(性格）分别输入多个Lora 模型中，完成对当前数据库的建设，以及对文生图中的文本内容的完善。\n\n# 4.4 本章小结\n\n本章通过系统性实验与参数化调优，构建了面向建筑生成的专业化AI训练框架，揭示了模型架构选择与训练策略对生成结果的关键性影响。研究发现，将建筑形体空间特征嵌入大模型（Checkpoint）而将建筑性格特征微调至轻量化模型（Lora）的技术路径，可有效规避传统方法中的优先拟合困境。基于StableDiffusion开源生态，研究采用 SD-scripts工具实现了非专业人员对复杂模型的跨领域训练——通过Dreambooth将“竖向线性单体空间”等建筑形体空间类型编码至大模型潜在空间，同时利用Lora 的低秩适配特性捕捉幼儿园建筑特有的色彩明快性、界面圆润度等类型学特征。\n\n训练过程中，参数配置的精细化控制成为决定性因素。 $5 1 2 \\times 7 6 8$ 分辨率与分桶策略的协同应用，在保留建筑几何精度的同时优化了计算效率；批量大小与学习率的动态平衡（如余弦退火重启策略）则破解了小样本训练下的过拟合问题。尤为关键的是，正则化数据集的引入重构了模型的泛化逻辑——通过150 张跨类型建筑图像的先验输入，使风格特征脱离特定空间形态的绑定，实现“幼儿园性格”在塔楼、展廊等异质空间中的自适应转译。\n\n最终测试表明，当大模型承载空间生成内核、Lora作为风格调节插件时，系统可突破传统文生图的语义耦合限制。通过权重系数调控（0.3-0.7区间），生成的竖向线性单体建筑既能保持核心筒结构逻辑，又可渐进式叠加高饱和色彩与弧形界面元素（图4.19）。这种分层控制机制，恰似柯林·罗（Colin Rowe）的“拼贴城市”理论在数字维度的重生——既尊重类型学本体的自洽性，又允许风格要素的弹性介入。\n\n本文建立的“本体-特征”的建筑图像生成方式，为建筑AI生成技术提供了可扩展的框架基础。其价值不仅在于突破现有平台的生成瓶颈，更预示着一种新型设计协作模式的诞生：建筑师通过调节模型架构的认知权重，即可在保持空间理性的同时探索形式的无限可能。\n\n# 第5章AI绘画生成建筑图像方法的优化结果\n\n# 5.1优化后的建筑图像生成方法归纳\n\n经过上文对AI绘画中文生图生成建筑图像步骤的优化和对现有数据库的补充，形成了一套基于建筑师设计思维的建筑图像生成方法（图5.1）。\n\n![](images/33f699d9fd2416b5bdc50a7800554bde884e3ae7ec8a227fe9a5f3a45f60a944.jpg)  \n图5.1优化后的建筑图像生成步骤图解 (图片来源：作者自绘)\n\n使用优化后的建筑图像生成方法，以生成“面延伸形成的空间类型”为例，步骤如下：\n\n（1）建筑骨架（即形体空间）的生成。在文生图界面输入提示词“面延伸”，进行多轮的批量生成（图5.2）。由于没有项目信息，没有输入其他描写建筑特征的提示词。本次选择了每轮4张的的生成数量，根据个人计算机的算力，可以减小或者提升每轮生成的张数。\n\n（2）图像筛选。在没有实际建筑项目相关因素（如场地、甲方等）的约束下，本次的图像内容选择环节中也没有指定的筛选标准，为了生成步骤的演示，假定第三轮的建筑图像生成结果在建筑的形体空间上比较符合预期，之后停止生成，并选择第三轮的结果为初步的预期图像。如果这一步的生成结果满足了使用者的需求（如使用者只是想生成初步的形体空间，或者生成结果展现出来的建筑特征符合预期要求），就不需要继续进行建筑类别（性格）的加入了，如果觉得生成结果中已选图像的建筑空间还需要进行调整，或建筑性格不符合预期，则需要继续加入关于调整建筑类别的Lora 模型。\n\n![](images/12295bf6450ebbbd02c50408a114c4117252fe2864a0b54611f067cd16b3be9e.jpg)  \n图5.2“面延伸”提示词控制的的空间类型的三轮图像批量生成结果 (每轮生成4张)\n\n（3）建筑类型Lora的加入。上文说到过，在不改变提示词，且 seed 值相同的情况下，生成的图像关键内容是不变的。在加载建筑类型的Lora 模型之前，需要将上一步中第三轮图像生成时的 seed值（3342737306）固定住，在 Stablediffusion 的Lora 模型选择界面加载符合预期的建筑类别模型，本次依旧以幼儿园类建筑为例。加载后逐步调整Lora模型的控制权重，在获得比较满意的效果时停止，或者将Lora 的控制权重从0-1，每次递增0.1变化，获得“面延伸”的建筑空间类型在“幼儿园类建筑”不同权重影响下的空间变化过程，再进行筛选或者获取灵感（表5.1）。\n\n表5.1中的图像生成结果说明，在建筑类别的Lora权重由0到1变化的过程中，本类建筑形体空间的主要特征“面的延伸”一直存在，但体现形式会随着Lora权重的增减逐步变化。在Lora 权重为0.1-0.4的数值范围内，图像中建筑主体的框架和线条都不会发生变化，多数在门窗等构建和线脚上会有微小的风格变化；Lora 权重的数值在0.5-0.6时，图像中建筑主体的框架会有细微变化，视觉特征上也有轻微改变，但总体特征上依旧保持；当Lora权重的数值在0.7-1时，图像中建筑的框架相比未添加Lora 时的图像已经发生了较大的变化，视觉特征上也有了较大差距。虽然在类别模型比重为0.1和1时，图像中建筑的表现差距较大，\n\n![](images/bc41837c2020b9073444d96693d6e72d2c885bd5f278bd9532b02395badeb00f.jpg)  \n表5.1面延伸空间类型（大模型）&幼儿园类(Lora）生成结果示例\n\n![](images/b3ed4ae10702acc7e622b37d8adec3fca3fb3cc540fd4c7ec0bbed7c3f06acc4.jpg)\n\n但观察权重值以0.1为差逐次增加时生成图像的对比，可以发现这个变化过程是一点一点在前者的基础上累积的，且后一次生成结果较前一次相比有规律可循。它不是像对建筑上色般简单的将建筑的类别特征固定在已经生成的线条中，而是在权重逐渐增加的过程中，逐次轻微的添加建筑类别的特征，包括结构上的和风格上的。当Lora权重较高时（0.7及以上），获得的图像更像是空间和风格同时融合的结果。因此，在本环节中，对生成结果的需求决定了Lora 权重值的选择。某些图像在Lora 权重值变化的过程中会发生结构突变，如表5-1-4，其原因由训练本身的不稳定因素导致，是普遍存在的现象。另外，如果严格的要求建筑的结构和线条都不发生变化，那就回归到了目前建筑行业最常使用的controlnet边缘控制，在线条的约束下添加风格和细节，当下已经比较普遍就不展开说明了。\n\n（4）其他细节调整。假设在上一步的图像生成中，Lora为0.7时，生成的一组图像较符合预期，但还想对图中的建筑进行材质的调整。一般来讲，这一步是需要使用controlnet插件进图像的边缘控制的，因为更改材质必然需要输入关于材质的提示词，而提示词的改变必然会在一定程度上会造成图像内容的变化。例如，Lora权重为0.7时，在文本框添加关于材质的提示词如“清水混凝土和木质表皮”，图像内容已经发生了改变（图5.3）。\n\n![](images/b1bdfcbb2268608e1ef6ce14fa977635aed31aceb055395399c2483d0fe36c49.jpg)  \n图5.3添加提示词使图像内容发生变化示例\n\n总的来说，本文对AI绘画生成建筑图像方法的优化主要体现在生成思路的优化，即更符合建筑师思维的“空间优先”的生成步骤，通过对建筑形体空间的梳理和数据库的构建，使得在建筑骨架和建筑类别特征的生成过程中，可以为使用者提供更快速更多样的选择。但在最终的细节调整时，如建筑材质和配景的添加，当前的边缘控制依旧是最好的选择。\n\n# 5.2优化后的建筑图像生成方法在案例中的快速设计应用举例\n\n# 5.2.1项目背景与设计需求\n\n长三角某新城现拟建一文化展览中心，基地位于城市中心城区，用地面积约$5 0 0 0 ~ \\mathrm { m } ^ { 2 }$ 。周边为低层居住区和公园。旨在打造集展览、教育、文化交流、公共休闲于一体的综合性文化地标，体现地域文化特色与现代艺术融合。满足多样化展览需求，兼顾常设展与临时展；营造沉浸式文化体验路径，增强公众参与性；实现建筑与自然环境的有机融合，体现可持续发展理念。设计要求在满足社区文化服务功能基础上，通过建筑形态塑造具有标识性的公共空间。\n\n在设计之初，对该项目进行解读，其核心理念是建造开放性、打破传统封闭式布局，提供灵活互动的空间；同时需要兼顾建筑的在地性和与周边环境的融合；通过空间形态和材料创新，打造标志性的视觉形象。\n\n# 5.2.2项目设计要素的提取\n\n在使用AI绘画生成建筑图像之前，依照本文优化后的方法，需要明确预期的建筑属于上文建筑形体空间分类中的具体类别，即对想要一个什么样式的建筑有大体的判定。这离不开对项目设计背景和需求的分析，需要设计师先理清设计的要求。根据上文对项目的描述，本人将设计中需要满足的要素初步提取为以下几点（图5.4）。认为该文化展览中心的空间形态应当是通过介质连接的、多个实体构成的连续建筑空间，用来满足多功能文化场所的需求并同时体现一定的叙事性；连接实体的介质应当部分属于灰空间，实现对话性和与环境的融合性，同时体现一定的艺术性；在空间的围合关系上应当存在院落的特征，坡屋顶和文化符号的加入完善了建筑对“在地性”的需求。\n\n![](images/37631bcef15a0c42827646446dff8eca2a48e1e94e6246dc60eceb8743ca8e3e.jpg)  \n图5.4长三角某市文化展览中心项目设计要素的提取\n\n# 5.2.3使用优化后的建筑图像生成方法进行建筑项目的快速设计过程\n\n在进行了项目的设计要素提取之后，对生成的建筑图像已经有了比较清晰的目标和预期。按照本文优化后的建筑图像生成方法，下文为一种参考式的快速设计思路。\n\n（1）建筑基本形体空间的生成。先在 Stablediffusion 中加载训练好的大模型，输入提示词“增加式的建筑群组”，也可输入当前模型中已经覆盖的、辅助性的建筑空间提示词如“灰空间”，但不要输入“院落空间”，因为依照上文的建筑形体空间分类，本文没有进行院落空间的概念训练，输入“院落空间”在模型的数据中可能会和“院子”混淆；也不要这一步输入关于建筑构件“如门、窗、屋面”等的细节描述，定向化会影响空间的生成多样性。调整好提示词后，进行建筑骨架的批量生成。生成可以在获得心仪的结果时停止，也可多次生成，对结果进行筛选，下文主要讨论大量结果中对建筑图像的筛选。\n\n（2）一轮筛选。由于AI绘画本身存在的不可控因素，在建筑图像生成的结果中必然存在一部分不符合预期方向的图像，需要使用上文提取的设计要素作为筛选原则，把这部分图像生成结果减去。在所有的设计要素中，除去个人的审美因素之外，理应先考虑建筑的形体和空间因素。在文化展览中心的设计要求中，空间应当是多个实体构成的、连续的，但由于其功能的多样性，个人认为空间的间断可以存在2处，一处可分离出单独的空间作为小型的公众参与型活动场地，\n\n另\n\n![](images/16dcbb5417830d01b23635bc3eb66d6ab7f340575c26bfcf20903e36222c854a.jpg)  \n图5.5允许空间间断类型的2种示例（左为单独的小型空间、右为建筑主入口。\n\n图片来源：笔者自绘)\n\n一处允许建筑间断处的空间作为文化综合体的出入口，因此，设定第一个筛选原则为“多个实体构成的、连续性的、可间断但不大于2处”，对上一步生成的图像进行筛选（表5.2）。\n\n![](images/8d1d80bc4f71d359b155a811041029fa88a6694174e117f90aab8bff532f6bf7.jpg)\n\n![](images/57dfeb8da3e2c255bf9bb480e9db55ffea144eb5ddbae5d4618e10d62f8ba745.jpg)\n\n![](images/29f7566beca71e33440d41a4930c7a9320b1cc750cb3bc189b00b032758c0969.jpg)\n\n根据表5.2的图像筛选情况，可以看出，在本轮设置的“连续空间、空间可间断但不大于2处”条件，主要是为了除去一部分形体特征过于不规则和空间分布过于零散细碎的图像,保留建筑的整体性。\n\n（3）二轮筛选。经过一轮的筛选后，去掉了一些建筑空间上比较散碎的图像，但剩余的结果还较多。为了筛选更加符合设计要求的建筑图像，根据提取出的设计要素，再加入“院落空间”的筛选条件。\n\n院落空间是指通过建筑墙体、柱廊、连廊或其他结构形成边界，从而形成的封闭或半封闭的围合空间（如四合院、西班牙内庭院）。空间具有内向型，焦点指向内部，与外部环境形成对比，强调私密性和领域感。传统的院落通常是四面围合的，当代建筑常通过三面建筑和一面透空结构（玻璃、格栅）或虚体界面（水体、台阶）形成“弱围合院落”（如安藤忠雄的住吉长屋中庭）。在城市中的密集城区，为了增强城市的适配性，本文把三面建筑和一面城市界面形成的空间也归为“院落空间”（图5.6）。表5.3为依据是否存在“院落空间”对生成的建筑图像进行筛选的情况。\n\n![](images/761009daba41603f50bef877d1260fcc5a49fa694263919f31d0c347590f5ac8.jpg)  \n图5.6本文中的“院落空间”示意图（图片来源：笔者自绘）\n\n![](images/6bf9cca8975fc6f1e3aa32d683c2106fe67c59f96e0b5a34fefe86102db9019f.jpg)  \n表5.3“是否存在院落空间”条件下对图像的筛选\n\n![](images/bffe79351be7a07e9a1cce998c40faf01d9781e8c94914cdcd267b13176f7f6c.jpg)\n\n1处院落空间 √ 1处院落空间 √\n\n（4）三轮筛选。在该文化展览中心建筑的功能要求中，强调空间的序列性和沉浸性，因此参展路线应当尽量是单一，没有迁回和交叉的，将其转化为筛选指标应当是“建筑空间的连续部分流线无交叉”，本环节设定的交叉包含T形、Y形、X形（图5.7），表5.4为筛选结果。\n\n![](images/1bd3939e52a39d39ebb030890dd4eb471a6c5e2f297ecb0d19d418f7b71a6e22.jpg)  \n图5.7本节的流线交叉示意 (图片来源：笔者自绘)\n\n表5.4“建筑空间的连续部分流线无交叉”条件下对图像的筛选经过本轮的筛选，剩余4个可选择的初步方案（图5.8）。\n\n![](images/710f23c5e7e1366f40c4f11f496b9659dc089a2bf19dbf400ad02a3572d90f14.jpg)\n\n![](images/d68de4ad3ca44e63ed148fe36bffccb979c6c537aae385f8728d36f7593861be.jpg)\n\n（5）其他因素对图像的筛选。至此图像中的建筑满足了空间的连续性、叙事性、部分开放性、与自然的融合性。根据场地的面积大小 $5 0 0 0 ~ \\mathrm { m } ^ { 2 }$ ，图5.8中1和4可能存在单个实体体量过大的问题，使建筑呈现视觉上的笨重感，因此将5.8中的图2和4作为空间形态上比较符合预期的图像。此时图像中的建筑还缺少展览类建筑的性格特征，下一步需要加载Lora模型来获得更接近预期的图像。\n\n![](images/5b882c6f48634b70bdc4325c255167679a3522a6c3c348fb711903051fcc8c9f.jpg)  \n图5.8三轮筛选后剩余的建筑图像 (图片来源：AI生成）\n\n在图5.8中图2和图4的设计要素中，图2更容易实现在地性的原则，具备中国传统建筑中院落空间的段落感，有倾斜的斜面空间便于传统坡屋顶元素的引入，来完善建筑风格和细节；图4则更具备地标性的原则，便于实现传统与现代元素的结合。在剩余方案较少的情况下，已经接近最后的决策性筛选，影响选择的因素往往是设计师或者投资方的个人取舍。例如在图5.8的2和4中，本人更偏向传统院落与现代化曲线的有机结合，所以最终会选择4。\n\n（6）建筑特征的加入。选取最后的生成图像，加入展览馆类建筑的的lora模型，可选择固定 seed值、不调用Lora 关键词但调整权重的情况下进行空间演变和建筑特征融合的过程，再次进行挑选（图5.9）；或者使用边缘控制，加入更多的细节，如场地环境等。\n\n![](images/129710c3b28ed36c20885290deb349a35780c316918e097eaaf492a310224e10.jpg)  \n图5.9文化展览中心最终可能的方案效果示例 (图片来源：AI生成)\n\n本章的AI绘画生成建筑图像过程仅提供参考，实际的应用流程中筛选原则多种多样，应综合考虑每个项目的多种因素。\n\n# 第6章总结与展望\n\n# 6.1主要研究结论\n\n本文按照点、线、面、体及其变化规律,在单体建筑、群组建筑和组团式建筑中对建筑形体空间进行了特征上的分类,输入到模型数据中,通过参数调整和方法改进,可以在较大程度上解决使用AI绘画进行文生图时,无法控制生成结果的问题。使用本文的方法,可以用简短的描述,短时间内获得大量接近生成意图的图像，为了满足对建筑风格的需求,在Lora中输入建筑性格,可以帮助使用者在无需边缘控制的情况下,通过对权重值的控制进行不同程度的建筑特征的加入。\n\n在当下多数的建筑设计流程中，由于数据集中缺少对建筑形体空间的认知,无法使用文生图对AI绘画进行控制,因此多使用 ControlNet 的边缘控制功能,在Stable diffusion 中将自己构思的方案体块生成效果图。然而，建筑也是来源于生活的艺术,在建筑的形体空间的生成过程中，建筑师不再期望只是构思纯粹的几何体进行空间上的推敲,而是希望建筑本身的视觉效果可以伴随在建筑空间的形成过程中。在过去的建筑设计中,受技术水平的限制，随时进行建筑效果图的可视化成本巨大,但在 AI 绘画中,却变成了可能。通过在模型中对建筑的形体空间类型和建筑风格分类输入，可以实现简短的几个词汇,以及鼠标轻点，轻松完成从空间类别到最终效果的可视化。这个生成过程是快速的、数量极大的,即使在生成结果中,没有完全符合预期的方案，也可以为设计流程提供大量灵感。\n\n# 6.2主要创新点\n\n# 6.2.1人机交的设计互模式下,更符合建筑师思维的生成步骤\n\n在传统AI生成范式与建筑创.\n\n作之间,始终横亘着难以逾越的认知鸿沟一一机器以概率模型堆砌视觉符号,建筑师则以类型学思维编织空间叙事。本研究突破此困境,通过构建分层认知嵌入模型,在 StableDiffusion 框架内重构了符合建筑师思维惯性的生成法则,使 AI 不仅理解建筑形式的表层语法,更能捕捉空间组织的深层结构。\n\n人机交互的核心技术路径体现为类型学解耦策略：将建筑形体空间编码至大模型（Checkpoint）的潜在空间,而将建筑性格特征微调至轻量化模型（Lora）。这使AI绘画从“图像合成工具”向“认知扩展伙伴”的转变。\n\n# 6.2.2快速、定向的文字生成建筑图像的方法\n\n本研究通过对当前AI绘画中文生图不足之处的总结,结合建筑从业者的惯用设计思维与前人的著作,思考了一种按照建筑形体空间对建筑图像类型进行分类的办法。之后,通过数据库的小范围修改和多次参数调整的模型训练过程，得出一套简单、快速的、较接近预期方向的文字生成建筑图像的方法。\n\n这种方法的核心是数据库的的建设,数据库建设的逻辑则是对建筑形体空间的分类依据，实现的途径是大量的参数调节和模型训练，最后是对训练结果的人工筛选和调整。\n\n# 6.2.3 建筑教学指导和实践参考\n\n这种快速且目的性强的文字生成建筑图像的方法,在当前的建筑教学中,具有一定的指导意义。在对学生建筑空间思维的训练教学中,往往需要即时的互动与思维的发散,教学中的一大难点也是将抽象的空间与具象的建筑相联系。本研究这种简单快速的图像生成方式正满足了教学中的需求,在有了某类空间的生成意向后,只需简单的几个字母,就可以实现空间在建筑中的具象化,帮助学生发散思维,提高教学效率和质量。通过控制Lora 的权重,还可以实现建筑性格和建筑立面的教学。\n\n在建筑设计的实践中,这种图像生成方法可以在设计的初期为使用者提供大量的灵感,在多方沟通的过程中,快速且意图较为准确的建筑图像生成,提高了沟通效率,省去了方案初期在建筑意向方面大量繁琐的修改流程,节约了人力成本。\n\n# 6.3未来展望与适应性分析\n\n# （一）技术迭代路径\n\n基于现有研究基础,未来可在模型训练范式层面进行深度优化。跨模态融合将成为关键突破方向。当前研究虽已实现文字到图像的映射,但若引入BIM参数化模型作为中间媒介,可构建\"语义-BIM-图像\"三重映射系统。例如将Revit族库中的构件参数与 StableDiffusion 的潜在空间建立关联,使窗墙比、开间进深等建筑学专属参数直接参与图像生成过程。初步测试表明,这种技术路线可使立面细部生成准确度提升 $42 \\%$ ，尤其对柯布西耶模度体系等比例控制严格的建筑风格具有特殊价值。\n\n# （二）教育应用深化\n\n在建筑教学领域，建议开发专用教学辅助系统（Arch-EduGPT），整合本研究方法论与认知心理学原理。系统应包含三个核心模块：空间类型识别器（识别学生手绘草图的形体特征）、意图解析引擎（将模糊设计概念转化为精确提示词）、生成轨迹追溯器（可视化AI的决策过程）。当学生输入“流动空间”概念时，系统可自动关联赖特草原学派、扎哈参数化设计等12种典型处理手法，并生成对比分析图谱。需特别关注生成式AI对空间认知模式的革命性影响，传统教学中，学生需经历“图纸-实体模型-效果图”的认知链条，而本研究技术可将该过程压缩为“语义-空间意象”的直接转换。教学实验表明，采用AI辅助组学生在空间组合复杂度指标上较传统教学组有所提升,但在尺度感知准确性方面存在了一定的偏差,这提示未来需研发空间标定补偿算法。\n\n# （三）设计流程重构\n\n建筑设计实践中，建议建立“AI生成-人工筛选-参数反哺”的闭环工作流。当设计师输入“山地聚落-参数化表皮-生态优先”复合指令时，系统不仅生成视觉图像，同时输出对应的场地坡度分析、日照模拟数据等技术参数。该模式使方案初期阶段的构思效率提升，但需警惕形式先于功能的设计异化风险。\n\n当前行业生态将呈现两极分化趋势：一方面，基础性效果图制作岗位可能减少 $43 \\%$ （麦肯锡 2023年预测数据），另一方面催生出“AI建筑策展人”等新兴职业，负责训练特定地域风格的Lora 模型。建议建立建筑语料质量标准体系，对训练数据中的风格特征要素进行量化标注，如将徽派建筑的“马头墙高宽比1:0.33”等参数纳入模型约束条件。\n\n# （四）技术伦理框架\n\n应当正视风格趋同化风险。当前模型在生成新中式建筑时，有超过大半的结果呈现雷同的坡屋顶组合模式，这源于训练数据中王澍、马岩松等代表作品的过度采样。建议引入风格变异系数（StyleVariation Index）作为模型评估指标，通过对抗生成网络（GAN）技术主动制造风格冲突，保持 $1 0 \\% - 1 5 \\%$ 的非确定性生成比例。\n\n版权归属问题需建立新的认定体系。当AI生成方案与既有建筑相似度超过阈值时（如设定为SSIM指数0.75），应启动溯源验证程序。可借鉴区块链技术，对训练数据中的每个建筑案例进行特征值哈希加密，在输出结果时自动生成权属关\n\n系图谱。\n\n# （五）适应性发展路径\n\n针对发展中国家建筑实践特点,建议开发低显存适配版本。通过知识蒸馏（KnowledgeDistillation）技术将原模型压缩至4GB显存可运行,在保持核心功能的前提下，降低渲染精度。\n\n#\n\n参考文献[1］夏娟,高俊涛．基于自动机主动学习的研究综述［J]．计算机与数字工程,2024， 52 (12): 3536-3540+3642.[2]韩慧,陆建峰．基于Actor-Critic帧间预定位的改进 SiamRPN模型［J]．计算机与数字工程，2021，49（11)：2222-2228.[3］黄伟．计算机视觉技术及产业化应用态势分析［J]．信息通信技术与政策，2018，(09): 59-62.[4]刘泽润,尹宇飞,薛文灏,等．基于扩散模型的条件引导图像生成综述［J]．浙江大学学报(理学版)，2023，50 (06)：651-667.[5] Song Yifei， Zhang Wei,et al. A Survey on Talking Head Generation[J].Journal of Computer-Aided Design & Computer Graphics， 2023， 35(10):1457-1468.[6] HUANG S, DONG L，WANG W,et al. Language is not all you need: aligningperception with language models. arXiv [J]. 2023， 27.[7]] NIU S，WU J， ZHANG Y，et al. Towards stable test-time adaptation indynamic wild world[C]//Proceedings of the International Conference onLearning Representations. Addis Ababa: ICLR， 2023:1-27.[8］王迁．再论人工智能生成的内容在著作权法中的定性［J]．政法论坛，2023,41 (04): 16-33.[9］朱禹,叶继元．人工智能生成内容（AIGC)研究综述：国际进展与热点议题［J].信息与管理研究，2024，9(04)：13-27.[10］余青龙．AI绘画软件的创作特征研究——以绘画软件Novel AI生成的动漫人物形象为例［J]．信阳师范学院学报(哲学社会科学版)，2023，43（03)：127-132.[11]王常圣．人工智能驱动的数字图像艺术创作：方法与案例分析［J]．智能科学与技术学报，2023，5（03)：406-414.[12]程大锦．建筑：形式，空间和秩序·第四版[M]．天津：天津大学出版社，2018.\n\n[13]EDUCATION 唐JAI．媒介思辨——从 AI 绘画到义务教育阶段的学校美术教育［J]．2024，14:562.  \n[14]张泽宇,王铁君,郭晓然,等．AI 绘画研究综述［J]．计算机科学与探索，2024,18 (06): 1404-1420.  \n[15]Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarialnets//Proceedings of the 27th International Conference on NeuralInformation Processing Systems. Montreal， Canada， 2014: 292672 -2680.[16]常永波，姚亦非，陈俊琰．大模型驱动下的产业应用生态：内涵、演进与挑战［J]．可持续发展，2024，14(3)：710-718.  \n[17]张仲维,陈涛，贾旭东,等．一种大规模推理用文本知识数据集的构建方法[J]．五邑大学学报(自然科学版)，2024，38(03)：38-47.  \n[18]张芯悦．面向大规模数据的多视图聚类算法及应用研究[D]．西南科技大学,2024. DOI:10. 27415/d. cnki. gxngc. 2024. 001693.  \n[19]Yen-Chun Chen,Linjie Li,Licheng Yu,et al. Uniter: Universal image-textrepresentation learning；In European conference on computer vision, 2020,pp. 104-120.Springer  \n[20]CHEN Y C， LI L， YU L， et al. Uniter: Universal im age-textrepresentation learning[C]//Proceedings of the European Conference onComputer Vision,2020: 104-120.  \n[21]陈晋音,席昌坤,郑海斌,等．多模态大语言模型的安全性研究综述［J/OL].计算机科学，1-32[2025-05-26].  \n[22]邓梓,何相腾,彭宇新．文本到视频生成：研究现状、进展和挑战［J]．电子与信息学报，2024，46(05)：1632-1644.  \n[23]李白杨，白云，詹希旎，etal．人工智能生成内容（AIGC）的技术特征与形态演进［J]．2023，40(1)：66-74.  \n[24]CROWSON K, BIDERMAN S,KORNIS D，et al． Vqgan-clip: Open domain imagegeneration and editing with natural language guidance; proceedings of theEuropean conference on computer vision， F， 2022 [C]. Springer.\n\n[25]LEE S JI JOA C T. Transforming Text into Video: A Proposed Methodology for Video Production Using the VQGAN-CLIP Image Generative AI Model [J]. 2023.Semantic Scholar\n\n[26] ZHU L，WEI F, LU Y,et al. Scaling the codebook size of vqgan to 100,000 with a utilization rate of $9 9 \\%$ ［J]．2024.\n\n[27]赖丽娜,米瑜,周龙龙,等．生成对抗网络与文本图像生成方法综述［J]．计算机工程与应用，2023，59（19)：21-39.\n\n[28]唐贤伦,杜一铭,刘雨微,等．基于条件深度卷积生成对抗网络的图像识别方法［J]．自动化学报，2018，44（05)：855-864.\n\n[29]REED S, AKATA Z, YAN X, et al. Generative adversarial text to image synthesis; proceedings of the International conference on machine learning, F，2016 [C]．PMLR.\n\n[30]RADFORD A， METZ L， CHINTALA S J A P A. Unsupervised representation learning with deep convolutional generative adversarial networks [J]. 2015.\n\n[31]ZHANG H, XU T，LI H, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks; proceedings of the Proceedings of the IEEE international conference on computer vision，F, 2017 [C].\n\n[32]YIN G, LIUB, SHENG L,et al. Semantics disentangling for text-to-image generation; proceedings of the Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,F, 2019 [C].\n\n[33]XU T, ZHANG P， HUANG Q，et al. Attngan: Fine-grained text to image generation with attentional generative adversarial networks; proceedings of the Proceedings of the IEEE conference on computer vision and pattern recognition，F, 2018 [C].\n\n[34]QIAO T， ZHANG J， XU D， et al. Mirrorgan: Learning text-to-image generation by redescription; proceedings of the Proceedings of the IEEE/CVF conference on computer vision and pattern recognition，F, 2019 [C].\n\n[35]岳增营,叶霞,刘睿珩．基于语言模型的预训练技术研究综述［J]．中文信息 学报，2021，35（09)：15-29.   \n[36]熊凯,杜理,丁效,等．面向文本推理的知识增强预训练语言模型［J]．中文 信息学报，2022，36（12)：27-35.   \n[37]RAMESH A, PAVLOV M, GOH G,et al. Zero-shot text-to-image generation; proceedings of the International conference on machine learning，F, 2021 [C]．Pmlr.   \n[38]Razavi A, Van den Oord A, Vinyals O. Generating diverse high-fidelity images with vq-vae-2[J]. Advances in neural information processing systems, 2019，32.   \n[39]VASWANI A， SHAZEER N, PARMAR N,et al. Attention is all you need [J]. 2017，30.   \n[40]TONG K，WU Y JJO V C， REPRESENTATION I. Rethinking PASCAL-VOC and MS-COCO dataset for small object detection ［J]. 2023，93: 103830.   \n[41]RADFORD A,KIM J W, HALLACY C,et al. Learning transferable visual models from natural language supervision; proceedings of the International conference on machine learning，F， 2021 [C]. PmLR.   \n[42]PATASHNIK O， WU Z， SHECHTMAN E， et al. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery (Supplementary Material） [J].   \n[43]RAMESH A, DHARIWAL P，NICHOL A,et al. Hierarchical text-conditional image generation with clip latents [J]. 2022，1(2): 3.   \n[44]YANG L, ZHANG Z, SONG Y,et al. Diffusion models: A comprehensive survey of methods and applications [J]. 2023， 56(4): 1-39.   \n[45]HO J, JAIN A, ABBEEL P JA I N I P S. Denoising diffusion probabilistic models [J]． 2020，33:6840-51.   \n[46]SAHARIA C， CHAN W, SAXENA S， et al. Photorealistic text-to-image diffusion models with deep language understanding ［J]. 2022， 35: 36479-94. [47]NICHOL A， DHARIWAL P， RAMESH A，et al. Glide: Towards photorealistic image generation and editing with text-guided diffusion models [J]. 2021.\n\n[48]ROMBACH R， BLATTMANN A， LORENZ D， et al. High-resolution image synthesis with latent diffusion models; proceedings of the Proceedings of the IEEE/CVF conference on computer vision and pattern recognition， F, 2022 [C].\n\n[49]TOMCZAK J， WELLING M. VAE with a VampPrior； proceedings oftheInternational conference on artificial intelligence and statistics，F,2018 [C]．PMLR.  \n[50]BURGESS C P，HIGGINS I，PAL A，et al. Understanding disentangling in$\\$ 8$ \\beta $\\$ 9$ -VAE[J]．2018.  \n[51]PEARTON S J，REN F J A M. GaN electronics [J]． 2000，12(21):1571-80.[52]PEARTON S， ZOLPER J， SHUL R，et al． GaN:Processing， defects，anddevices [J]. 1999，86(1): 1-78.  \n[53]SHMELKOV K， SCHMID C，ALAHARI K. How gOOd is my GAN?；proceedings ofthe Proceedings of the European conference on computer vision (ECCV)，F,2018 [C].  \n[54]徐光宪,冯春,马飞．基于UNet 的医学图像分割综述［J]．计算机科学与探索,2023，17 (08): 1776-1792.  \n[55]向阳,赵银娣,董霁红．基于改进UNet李生网络的遥感影像矿区变化检测［J].煤炭学报，2019，44（12):3773-3780.D0I:10.13225/j.cnki. jccs.SH19.1026.[56]SHARMA P,ALLISON JP J S. The future of immune checkpoint therapy ［J].2015， 348(6230) : 56-61.  \n[57]ROEDER G S，BAILIS JM J T I G. The pachytene checkpoint ［J]. 2000,16(9) : 395-403.  \n[58]DEVALAL S, KARTHIKEYAN A. LoRa technology-an overview； proceedings ofthe 2018 second international conference on electronics,communication andaerospace technology （ICECA)，F，2018 [C]． IEEE.  \n[59]于鹏,张毅．从特征辨识到图像生成:基于AIGC 范式的苗族服饰设计［J]．丝绸，2024，61(03)：1-10.[60]张旭龙,瞿晓阳,谢骏飞,等．人工智能生成式内容技术概述［J/OL]．大数据,1-37[2025-05-26].  \nhttp://kns.cnki.net/kcms/detail/10.1321.G2.20250408.1439.004.html.[61]杜佳俊，兰红．基于多模态特征融合的图像编辑模型［J]．2024．DOI：10.12677/csa. 2024. 146153  \n[62]周飞燕,金林鹏,董军．卷积神经网络研究综述［J]．计算机学报，2017，40(06): 1229-1251.  \n[63]徐冰冰,岑科廷,黄俊杰,等．图卷积神经网络综述［J]．计算机学报，2020,43(05): 755-780.  \n[64]李彦冬,郝宗波,雷航．卷积神经网络研究综述［J]．计算机应用，2016，36(09): 2508-2515+2565.",
    "chunked": true,
    "vectorizeStatus": "success",
    "chunkCount": 115
  },
  {
    "id": "7043d733-602f-4c82-b672-eee8f515a586",
    "title": "2024-知识与数据驱动的遥感图像智能解译：进展与展望_孟瑜",
    "fileName": "2024-知识与数据驱动的遥感图像智能解译：进展与展望_孟瑜.pdf",
    "fileType": "pdf",
    "fileSize": 3609923,
    "uploadTime": "2025-12-29T22:04:53.257180",
    "parsed": true,
    "parseStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251229220453_2024-知识与数据驱动的遥感图像智能解译：进展与展望_孟瑜.pdf",
    "markdownContent": "# 知识与数据驱动的遥感图像智能解译：进展与展望\n\n孟瑜 1 ，陈静波 1 ，张正 1 ，刘志强 1，2 ，赵智韬 1，2 ，霍连志 1 ，史科理1，2 ，刘帝佑1 ，邓毓弸1 ，唐娉1\n\n1. 中国科学院空天信息创新研究院 国家遥感应用工程技术研究中心, 北京 100094;2. 中国科学院大学 电子电气与通信工程学院, 北京 101408\n\n摘 要：知识与数据是贯穿遥感图像解译数十年发展历程的两大要素。随着传感器平台的不断丰富，以及深度学习、大数据、多模态、长时序解译方法的快速突破，数据驱动的智能解译成为了近年来的热点研究方向。然而在不断深入扩展的研究与应用中，数据驱动方法迁移复用难、样本依赖强、可解释性弱等局限开始显露。在长期解译实践中积累的各类知识具有客观实在性、确定性、场景适应性、解释推理性等特点，可以与数据驱动的方法互为补充，知识与数据双驱动正逐渐成为遥感图像解译的新方向。本文首先回顾了遥感图像解译发展的几个主要阶段以及知识和数据在各个阶段分别发挥的作用，继而总结了 类遥感图像解译涉及的主要知识类型。知识与深度学习的融合是实现知识与数据双驱动的重要路径，本文梳理了 5大类 15小类知识与深度神经网络的融合方法并例举了相关案例。以知识类型为主要脉络，本文进一步对现有知识与数据联合的遥感解译应用进行了综述，通过典型案例分析了效益能力增量。最后本文对知识与数据联合驱动的遥感图像智能解译框架及关键技术进行了展望。\n\n关键词：遥感图像解译，知识驱动，数据驱动，人工智能，知识图谱，深度学习，自然资源，综述\n\n中图分类号：TP701/P2\n\n引用格式： 孟瑜，陈静波，张正，刘志强，赵智韬，霍连志，史科理，刘帝佑，邓毓弸，唐娉.2024.知识与数据驱动的遥感图像智能解译：进展与展望.遥感学报，28（11）：2698-2718Meng Y， Chen J B， Zhang Z， Liu Z Q， Zhao Z T， Huo L Z， Shi K L， Liu D Y， Deng Y P and Tang P. 2024.Knowledge and data driven remote sensing image interpretation： Recent developments and prospects. NationalRemote Sensing Bulletin，28（11）：2698-2718［DOI：10.11834/jrs.20243547］\n\n# 1　引 言\n\n遥感图像解译是自然资源调查监测等领域中一项关键且复杂的任务，具有显著的综合性与系统性特点 （Du 等，2014；张继贤 等，2021）。遥感图像解译的综合性在于其既是光谱、几何、空间、纹理、时序、参量等遥感信息的综合，也是地形地貌、气候水文、环境生态、社会发展、随机事件等地理与人文要素的综合；遥感图像解译的系统性在于其构建自一系列自然资源要素对象定义、分类体系、行政区划、业务规范、制图标准之上，彼此支撑、相互约束。\n\n遥感图像解译方法的发展一直伴随着数据条件与认知水平的提升，数据与知识是贯穿解译方 法 演 进 的 两 条 主 线 （Li 等 ， 2019； Ge 等 ，2022）。本文中数据主要指遥感观测数据，包括多光谱、高光谱、合成孔径雷达 （SAR）、激光雷达 （LiDAR）、夜光、红外等多模态数据；而知识的概念则比较广泛，包括各种与遥感解译相关的定义、机理、常识、模型、特征、参量、阈值、规则、关系、范围，以及众包地理信息等已知的辅助数据。在知识与数据的不断推动下，遥感 图 像 解 译 方 法 的 发 展 经 历 了 4 个 主 要 阶段 （图1）。\n\n![](images/77e66f62f5f7013a2dad3f5981ef33e928a5c2a1626d1d450cf29e4d969ed3f5.jpg)  \n图 　遥感图像解译方法发展 个主要阶段中的知识与数据  \nFig. 1 Knowledge and data in the four major stages of remote sensing image interpretation methodology developmen\n\n第一阶段遥感图像数据尚比较匮乏，基于专家知识的目视解译与实地调查是主要的作业手段，在这一阶段知识发挥着相对主导性的判别作用，关于不同地物类型在遥感图像中的形状、尺寸、色调、纹理、空间关系等特征形成了一系列经验准则。\n\n第二阶段随着遥感数据的逐渐丰富，基于统计分析的浅层智能方法得以发展，代表性方法包括随机森林 （Belgiu 和 Drăguţ，2016）、支持向量机 （Mountrakis 等 ， 2011）、 K-Means （Petitjean等，2012）、马尔可夫随机场（Gu等，2017）等，图像解译也开始走向自动化与智能化，在这一阶段数据与知识同时发挥了重要作用，数据可以作为训练样本与验证样本，而知识则引导了模型选择、参数取值、类别标记、特征提取等环节。\n\n第三阶段迎来了遥感大数据与人工智能发展的双重高峰，数据驱动的深度智能成为了这一阶段的主轴（Chi等，2016；Ma等，2019）。在数据层面，遥感传感器的种类与数量快速增加，高分辨率、长时间序列、多模态、全天时数据不断涌现。在解译方法层面，深度神经网络可以从大量样本中学习数据的内在规律与高层特征表示，这种由数据驱动学习而来深度特征的判别能力大幅超越了以往浅层特征与人工特征的判别能力，进而引发了人工智能与遥感图像智能解译方法的一次颠覆性变革。陆续出现的卷积神经网络（Kattenborn 等，2021）、循环神经网络 （Mou 等，）、自注意力网络（ 等， ）、图卷积网络（Baroud等，2021）等网络结构在地表覆盖与土地利用分类、场景识别、语义分割等任务中不断突破图像解译能力的上限。这一阶段更加强调数据驱动的价值，知识与人工设计在一定程度上被置于数据驱动的对立面，在大量训练数据存在的前提下知识所带来的增益被认为是可覆盖和可超越的。\n\n第四阶段利用深度学习进行遥感图像智能解译的研究开始进入平台期，数据驱动的局限性逐渐显露，利用知识驱动弥补数据驱动的弱点，知识与数据双轮驱动的理念回归 （王志华 等，）。在遥感图像智能解译领域数据驱动的局限性主要在于：（1）深度学习模型的空间/地物类型/时间/分辨率/数据模态迁移能力弱，场景可复用率低，存在“一地一议”、“一事一议”、“此一时彼一时”等问题；（2）对训练样本的依赖度高，既要求数量也要求质量，而解译样本的获取长期面临难度大、更新慢、错误多、靠人力、时空分布与类别分布不均等问题；（3）对业务规范与边界约束贴合不严，容易出现结果大致正确，但与业务要求还差“最后一公里”问题，可解释性与定向调整能力差，依赖后期加工与人工复核；（4）模型量级膨胀加快，随着GPT等千亿参数大模型对小模型优势的逐渐确立，深层模型研究及其应用的算力/成本/规模门槛开始提高。\n\n自然资源遥感解译对于知识的融入有着更加天然和迫切的需求，首先，遥感图像解译任务的综合性与系统性，包括其地学属性和社会属性，主要是通过知识的形式得以表现，单纯的数据驱动方法难以满足解译任务的复杂性与业务要求，需要利用知识进行增强或约束。其次，领域知识与观测数据同样都是对地物重要的认知资源，两者的共存是一种常态，但目前对数据的开发程度在经过深度学习的浪潮后远超对知识的开发程度，对知识这种宝贵认知资源的利用不够充分。最后，各种形式的知识与深度神经网络等智能解译方法的联合已经得到重视与初步研究，知识图谱等知识的关联与表达结构也逐渐成熟，知识与数据双驱动的遥感图像智能解译成为了潜力巨大的突破方向与研究热点。\n\n知识的加入对于自然资源遥感解译的提升是多维度的，具体的说，知识驱动有助于：（）在数据驱动的基础上通过知识判别、知识推理、知识约束等方式进一步提升解译精度；（2） 利用不同时空场景、不同地物类型的特有知识，增强数据驱动模型的可迁移性与可复用性；（3）通过对共性知识的迁移缩减模型收敛的搜索空间、知识生成样本与类别标记等方式减少模型对训练样本的依赖；（4） 对解译结果进行定向引导与约束，促使解译结果更贴合业务规范，避免二次加工；（5）使数据驱动模型产生的结果更具可解释性，更符合人的认知与领域实践。\n\n本文在分析知识与数据结合必要性的基础上，以知识为主视角，梳理遥感图像解译涉及的主要知识类型，探讨知识与以深度学习为代表的数据驱动方法的融合策略，阐述各类知识与数据相结合的遥感解译进展，最后展望体系化的知识与数据联合驱动的遥感图像智能解译框架及其中的关键技术。\n\n# 2　遥感图像解译中的知识类型\n\n遥感图像解译涉及的知识门类较为丰富，既包括解译对象的概念定义及其在各类遥感影像上反映出的时空谱特征、指数特征、地理特征，也包括解译对象的地学属性、社会属性、解译规范，还包括所使用解译模型的相关知识以及多种知识相互关联的知识图谱。如表1所示，本文将遥感图像解译涉及的主要知识划分为14种类型，并对每种知识类型的内涵进行了概述，以增强相关研究与论述的系统性，启发更多知识种类的遥感解译应用。\n\n表1　遥感图像解译涉及的主要知识类型划分  \nTable 1　Categories of knowledge regarding remote sensing image interpretation   \n\n<table><tr><td>序号</td><td>遥感解译知识类型</td><td>知识内涵</td></tr><tr><td></td><td>概念定义定律</td><td>地理实体与解译对象定义、相近地物类型的概念区分、地学规律/定律/常理、地表覆盖/土地利用/专题产品 分类体系等背景框架知识</td></tr><tr><td>2</td><td>光谱色调特征</td><td>色调、色相、饱和度、明度及其变化范围、差异性、均一性、阴影、典型地物光谱曲线、波谱库等光学特征</td></tr><tr><td>3</td><td>空间几何特征</td><td>邻域地物分布、邻域拓扑关系、尺寸、形状(线/块/条带/圆等)、边界(圆滑/清晰/模糊等)纹理(细腻/粗糙/均 一等)等空间与几何特征</td></tr><tr><td>4</td><td>时序物候特征</td><td>物候期、季相、趋势、植物生长曲线、生长周期、时序波谱、枯水/平水/丰水期等描述不同地物类型随时间变 化规律的特征</td></tr><tr><td>5</td><td>雷达回波特征</td><td>雷达散射特性、穿透性、极化性、相干性、成像几何与纹理特征、所处环境的噪声特征、地面、海上、空中杂 波，以及激光雷达的多回波特征、强度特征、点属性等</td></tr><tr><td>6</td><td>地形地貌特征</td><td>海拔高程、坡度、朝向、土壤类型、地貌(山地/高原/平原/河谷/盆地/丘陵等)、山地垂直带谱等描述不同地物 类型所处地形地貌的特征</td></tr><tr><td>7</td><td>气象水文特征</td><td>温度、湿度、降水量、光照强度、土壤含水量及其最高/最低/平均等统计值、晴空日数、降雨日数、降雪日数等 不同地物类型所对应的气象水文特征</td></tr><tr><td>8</td><td>遥感指数特征</td><td>通过计算得到的描述不同领域的专题指数,例如植被指数、叶面积指数、叶绿素含量、植被覆盖度、光合有 效辐射吸收比例、水体指数、城市建筑指数、裸土指数等</td></tr><tr><td>9</td><td>地理位置分区</td><td>不同地物类型在各种生态分区、地理分区、行政区划、场景等空间位置的先验分布情况,包括某些地物特有 的区域和场景分布</td></tr><tr><td>10</td><td>社会地理信息</td><td>Open Street Map（OSM）、Point of Interest（POI）、手机定位信息、手机拍摄典型地标、街景等开源或众包地理 信息，主要在城市等人类活动区域提供先验解译信息</td></tr></table>\n\n续表\n\n<table><tr><td>序号</td><td>遥感解译知识类型</td><td>知识内涵</td></tr><tr><td>11</td><td>行业准则规范</td><td>最小图斑面积、图斑完整性、连续性、边缘齐整性、内外轮廓、解译对象整体性、独立性、分割规则等行业约 定与规则，是智能解译结果所应遵循的约束或达成的目标</td></tr><tr><td>12</td><td>历史数据产品</td><td>过往的解译结果、土地利用与地表覆盖分类产品、几何/辐射/植被/大气/水分/能量平衡专题遥感产品、已训 练的智能解译模型等，支持不同时空场景的知识参考或迁移</td></tr><tr><td>13</td><td>现有解译模型</td><td>已有的成熟解译模型,既包括传统的统计学习方法也包括深度学习与人工智能方法,以支持多种解译模型 的集成学习、模型之间的迁移学习、或生成样本预标记类别</td></tr><tr><td>14</td><td>知识关联图谱</td><td>以关联关系的形式进行表达的知识,可将特定主题或场景所涉及的多领域、多对象、多层级知识进行充分 发掘、关联、组合，提升领域知识的维度、深度、广度</td></tr></table>\n\n一个值得注意的问题是表1中的部分知识类型是以特征来描述，这里的特征是指已经知晓其有效性的特征，并非解译过程中提取的那些效果未知的中间特征。\n\n# 3　知识与深度学习的融合方法\n\n近年来深度学习已经成为智能遥感解译前沿研究所使用的主要方法（Pouyanfar等，2018）。深度学习被认为是一种典型的数据驱动方法，可以提炼出数据内在规律与深度特征，然而深度学习是一种黑盒模型，其核心机制较为封闭且缺乏可理解性。各种表达形式的知识与深度学习的有机融合是实现知识与数据双驱动的必由之路，二者融合的方法尚需进一步探索。如图2所示，本文尝试总结了5大类15小类知识与深度神经网络的融合方法及其在网络流程中的切入位置，旨在不影响网络自身抽象与泛化能力的基础上自然的融入遥感解译知识。\n\n![](images/c825fe65371b2dd58e8977892ddac5b0c4380d1abe6c623357ec9f735064e547.jpg)  \n图2　知识与深度神经网络的融合方法与切入位置概览  \nFig. 2 Overview on the combination of knowledge and deep neural networks： mode and position\n\n# 3.1　数据级融合\n\n数据级融合 （Data Level Incorporation） 是指利用知识生成、标记、或加工训练数据，将知识内化在训练数据中，使用这样的数据训练出的解译模型一般会倾向于得到知识所属意的结果。数据级融合是一种较为间接的知识融合方法，其优点主要表现在不会对既有的深度网络结构或解译算法带来改变，只需要从训练数据的角度进行操作。数据级融合包括以下具体方式：\n\n（1） 知识模拟训练样本。利用已知的物理过程模型、时间序列模型、数据生成模型、数据概率分布等知识和生成式对抗网络、生成式人工智能等工具，通过随机或指定初始条件生成模拟训练样本，可以实现知识向训练样本的注入，图3示意了上述知识模拟训练样本的主要过程。\n\n![](images/fbebbbdc3c0114a2d7a3ac48f409cb0e4aa155adb41ad7cb6b793a90386b512f.jpg)  \n图3　知识模拟训练样本示意图  \nFig. 3 Knowledge driven training sample generation\n\n（2）知识赋予类别标记。深度学习所面对的数据条件一般是标记样本少但未标记样本很多，利用已知的浅层分类聚类模型或判别知识给部分未标记样本赋予初步类别继而参与网络训练，可以利用网络的鲁棒性同时融入知识。\n\n（3） 知识加工输入数据。利用关于解译对象的知识，可以对训练数据进行诸如噪声添加、角度旋转、尺度变换、随机抠取、色彩抖动、几何变换、成分分解等处理实现样本数量扩增，也可以通过知识过滤错误数据实现样本质量提升。\n\n# 3.2　特征级融合\n\n特 征 级 融 合 （Feature Level Incorporation） 是指将知识通过特征的形式加入到解译模型中，与原有的数据特征进行联合共同作为解译结果判别的依据。不同表达形式的知识需根据所采用解译模型的要求转化为指定形式的特征，特征联合的时机可以在输入数据时、原有特征提取过程中、或最终判定解译结果之前。特征级融合有时需要对神经网络结构或解译流程进行调整，以容纳知识所对应的特征。特征级融合包括以下具体方式：\n\n（1）知识扩增特征维度。对于已知有效的传统解译特征、遥感指数，或来自地学、人文、社会等其他领域的知识特征，可以在数据输入神经网络前进行特征提取量化然后以扩增数据维度的方式加入到输入数据中，进而与数据驱动的深度学习融合。图4示意了上述知识扩增特征维度的主要方式。\n\n![](images/f20efb53237036b01de8b6d96c360657946c17774c3acb1e2ed61199d03db63c.jpg)  \n图 　知识扩增特征维度示意图  \nFig.4 Knowledge driven multi-dimensional feature augmentation\n\n（2）知识强化关键特征。在已知某些关键的光谱范围、时间窗口、空间邻域等特征对特定类别的解译效果显著时，可以主动调整这些关键特征的权重或弱化干扰特征，放大类别间特征的距离，引导数据驱动模型快速找到解译的突破口。\n\n（3） 知识量化图谱嵌入。将知识图谱 （Hao等，2021）嵌入到深度神经网络的一种方式是将图谱中所描述的对象关系量化为特征向量，其中特征值的大小与关系强弱相关，而后将特征向量与神经网络在输入端或过程中进行结合，实现关系型知识的自然融入（Li等，2021b）。图5示意了上述知识量化图谱嵌入的主要过程。\n\n![](images/c770c9078914a640a215c1caf676b98fcb0fc3521112d9f8c47f686d5bcb00d7.jpg)  \n图5　知识量化图谱嵌入示意图  \nFig. 5 Knowledge graph quantization and embedding\n\n# 3.3　网络级融合\n\n网络级融合 （Network Level Incorporation） 是指将知识反映在深度神经网络等模型的结构中，利用知识引导或启发网络结构的设计，从而较为直接的贯彻知识的内涵与意图。神经网络可以对已知的公式或逻辑单元进行结构模拟和数值逼近，也可以通过支路网络、层级架构、超参数设置等方式主动关注知识所指向的特征，还可以利用知识定义或约束损失函数，通过误差反传过程将知识渗透在网络参数中。网络级融合包括以下具体方式：\n\n（1） 知识映射网络结构。对于公式、组合逻辑等结构化的解译知识，可以通过卷积核、编码器、激活函数、池化层等神经网络单元的设计与组合实现知识复现与数值逼近，并通过训练进一步优化网络结构和参数，实现知识在数据驱动框架下的继承与发展。\n\n（）知识添加支路网络。深度神经网络支持多路并行最后收束，对于已知有效的某些传统特征或不同时—空—谱维度、不同数据模态、不同尺度等特征可以通过支路网络进行提取然后组合，实现多种深度特征与传统特征的有机协同。\n\n（3）知识构造损失函数。损失函数直接影响到网络输出结果，并且是以白盒的数学形式直观表达，通过已掌握的知识选择损失函数种类、定义新损失函数、构造或添加正则约束项，可以将知识按照主观意图自然融入智能模型中（ $\\mathrm { X u }$ 等，2018）。\n\n# 3.4　学习级融合\n\n学习级融合 （Learning Level Incorporation） 是指通过各种机器学习模式将知识学习到所用的解译模型当中，实现知识的迁移、泛化、或强化，提升智能解译模型能力。迁移学习可以将不同领域的知识或网络模型进行迁移，元学习可以将更底层更通用的知识或网络模型泛化到所需要的领域，强化学习可以增量式的持续对新知识进行吸收。学习级融合包括以下具体方式：\n\n（1） 迁移学习。迁移学习 （Transfer Learning）（Xie 等，2016） 旨在将已有解译模型或训练样本迁移到不同时空场景、不同解译对象、或不同数据条件，实现模型化知识的复用，对预训练模型利用领域数据进行微调是迁移学习最常见的方式之一。\n\n（2） 元 学 习 。 元 学 习 （Meta Learning）（Hospedales等，2022） 以判断个体之间是否相似、是否属于同一类的通用判别知识为基础，通过判断未知样本与已知样本的同类性实现类别赋予，采用度量学习、孪生网络、关系网络等具体形式可实现小样本乃至零样本学习。\n\n（3） 强 化 学 习 。 强 化 学 习 （ReinforcementLearning）（Shen 等，2020） 提供一种增量式的模型强化能力，对于多种异构知识或训练数据可分别采用不同强化学习方式对同一个模型进行协同强化，实现知识的化零为整、求同存异、持续融合，响应新的知识与变化。\n\n# 3.5　决策级融合\n\n决策级融合 （Decision Level Incorporation） 是指在最终的决策环节引入知识，利用已知的业务规范、行业惯例、判别规则等对结果进行判定、筛选过滤或二次加工处理，也可以利用已知的其他方法生成解译结果，与当前所使用智能模型产生的结果进行对比与融合，进一步增强结果的可靠性、可用性、规范性。决策级融合一般只在末端进行操作，对智能解译模型结构的侵入性较小。决策级融合包括以下具体方式：\n\n（1）知识引导后期处理。由于知识影响解译结果的间接性以及深度神经网络的黑盒特性，端到端的解译结果难以人为控制，为了使结果更符合行业惯例、准则与规范、可利用知识制定或约束后处理过程以直接优化解译结果。\n\n（2）知识定制判别规则。非端到端的神经网络主要用于特征提取而不直接得到解译结果，此时可以根据知识选择或定制末端的分类器和分类规则，包括孪生网络 （Liu等，2019） 或关系网络（ 等， ）中的邻域投票规则，深度聚类中的聚类算法选择等。图6展示了知识在孪生网络中定制判别规则的一个例子。\n\n![](images/362926d49478c529e62c268bd47eefac0601e3c567e8193d85f3eb80701ff763.jpg)  \n图6　知识定制判别规则示意图（以孪生网络为例） Fig. 6 Knowledge driven decision rule customization （an example of siamese network）\n\n（3）知识集成多分类器。在深度学习出现之前已知一些较成熟或针对某些特定对象效果较好的分类器，将这些相对弱的分类器通过一定策略（Adaboost、GBDT等）与各种深度学习分类器进行组合可以产生更强的分类器，取长补短，优势互补（Zhang等，2022）。图7展示了通过组合多个分类器的结果纠正错误分类的例子。\n\n![](images/58c15d0ada902801f43ef1b09b25efc86c8c88a2001f5d856b10e9c6a5c0a4d5.jpg)  \n图 　知识集成多分类器示意图  \nFig. 7 Knowledge driven ensemble learning\n\n# 4　知识与深度学习的融合进展\n\n本文梳理的知识与深度学习融合的5大类方法在遥感图像解译领域尚处于发展阶段，应用案例有限，而这些方法在相近的计算机视觉 （Cui 等，2023）、医学影像解译（Xie等，2021）等领域已经取得了较多应用。为了进一步加强对这5类方法内涵与效果的描述，启发其在遥感领域的应用，本节对计算机视觉、医学影像解译等领域的知识与深度学习融合典型案例进行描述。\n\n# 4.1　数据级融合案例\n\nMottaghi等（2016）为了实现静态图像中物体的运动趋势预测，利用游戏引擎构建了视觉牛顿动力学数据集，包含了自由落体、抛物线、下坡等12类牛顿力学典型场景的动态视频与对应静态图像，利用数据集训练网络从而将物理学知识融入视觉网络模型中。\n\nXie等（2019）针对肺结节训练样本影像不足的问题，将肺结节3D模型通过3类9种已知的诊断视角进行2D影像抽取实现诊断知识引导的数据扩增，继而对ResNet-50预训练模型进行专业化再训练，提升肺结节的良性恶性判别准确率。\n\nYang等（2019）通过甲状腺结节影像的放射科医生标记、影像中的超声特征、病灶硬度信息等专业知识在小样本条件下训练出一个DScGANs双路径半监督条件生成对抗网络，利用网络模拟训练样本对分类器进行训练实现甲状腺结节精准诊断。\n\n# 4.2　特征级融合案例\n\n等（ ）提出一种在卷积神经网络中融入人工设计特征同时支持多种人工或深度特征融合的动作识别网络模型，通过设计特征放大器结构将人工设计光流场与卷积特征进行融合，然后通过空间变化乘性融合方法组合多源特征，共同实现动作识别精度提升。\n\nKushibar等（2018）提出一种结合深度卷积特征与先验空间结构特征的皮质下脑部磁共振图像分割方法，将以往研究中形成的大脑空间组成结构概率图以概率向量形式与深度卷积网络提取的中间特征进行组合，共同参与网络损失函数计算与判别过程，有效提升了分割精度。\n\nLi等（2022d）针对知识图谱固有的实体与关系类型异质问题，提出了一种基于注意力机制的异质图神经网络，通过关系注意力机制提取并聚合高价值关系并为关系赋予权重，实现知识图谱所包含特征与图神经网络的有效融合嵌入，提升网络的推理预测能力。\n\n# 4.3　网络级融合案例\n\nRaissi等（2019）提出了一类内嵌物理规律的深度神经网络，将偏微分方程形式的物理规律作为正则化限制条件加入到网络损失函数中，将网络输出层的每个单元视为对微分方程函数项的拟合，从而将网络训练为物理规律的一种非线性逼近器进行应用或逆向发现新的物理规律方程。\n\nZheng等（2019）为了将解剖学中关于肝脏形状和位置的专业知识加入到当前以深度学习为主的医学影像分割方法中，提出了一种将分割目标出现位置和目标形状的先验概率加入到神经网络损失函数中的半监督对抗学习模型，有效提升了分割精度。\n\nLuo等（2020）基于图像中对象之间的关系有助于理解对象的身份这一直觉，提出了一种构造关系图先验知识并在网络中加入关系推理层的图像检测识别模型，利用条件随机场对局部上下文信息和关系图先验进行联合推理，在公开数据集上提升了识别正确率并支持零样本识别。\n\n# 4.4　学习级融合案例\n\nEsteva等（2017）针对皮肤癌早期依赖视觉检查但漏诊误诊率较高的问题，构建了包含两千余种皮肤疾病的图像数据集并利用数据集对已训练的InceptionV3视觉模型进行专业领域调优，通过引入专业知识将模型的皮肤癌诊断正确率提升到了职业医生的水平。\n\nYin等（2020）提出了一个从已经过训练的大模型中根据指定风格恢复出训练数据或者中间数据的网络框架，这种框架将已训练模型作为一种知识的聚合体，通过从模型中恢复出带有指定领域知识的训练数据或中间数据将知识转移到其他网络中，实现轻量化的知识蒸馏。\n\nLiu等（2023）提出了一种将图像—文本通用知识迁移到视频—文本领域的时空建模方法，充分利用已经过大规模训练的图像—文本多模态通用知识，通过时空辅助网络在不破坏跨模态通用高层语义知识的情况下将底层视觉模式知识扩展到时空场景。\n\n# 4.5　决策级融合案例\n\nXie等（2018）提出一种利用灰度共生矩阵提取纹理特征、利用傅里叶形状描述子提取形状特征、利用卷积网络提取深度特征，分别训练分类器然后通过Adaboost策略进行组合判别的CT扫描影像肺结节分类方法，利用已知经验特征补足了对肺结节异质性的捕捉。\n\nBonettini等（2021）针对视频换脸技术带来的风险，提出一种多个卷积网络集成决策的视频换脸检测技术，将已知有效的多种经典网络通过孪生网络架构中的多元组损失函数进行集成，通过多个网络判断结果的综合降低错分概率，准确判别人脸图像是否经过替换篡改。\n\nZhuang等（2021）针对当前知识蒸馏方法主要适用于分类任务而无法有效用于目标跟踪的问题，基于目标跟踪的前景背景二元权重差异、关键负样本等已知特性设计了两种新的损失函数并通过孪生网络结构训练了三路网络联合实现高精度目标跟踪。\n\n# 5　知识与数据联合的遥感解译进展\n\n在现有的遥感解译应用中数据的使用相较于知识是更为普遍的，在数据的基础上融入知识是未来遥感解译方法与应用发展的一个重要方向，因此本文在介绍知识与数据联合的遥感解译进展时选择以知识作为主视角，按照前文划分的14类主要知识脉络分别对典型的知识与数据联合应用进行综述。\n\n# 5.1　概念定义定律\n\n现有研究中应用到的概念定义定律类知识包括解译对象的定义、对象之间的常理关系、地学定律等。例如，机场跑道的空间定义和特征有助于完成机场跑道的提取，这种基于知识的方法可以有效地缓解由影像背景复杂、样本量小、机场跑道两端变化给机场跑道变化检测带来的问题（Ding 和 $\\mathbb { W } \\mathbf { u }$ ，2020）。此外，利用森林定义的先验知识，Nguyen等 （2022）构建了基于规则决策的深度学习模型，让森林制图过程具备更强解释性的同时，还可以获取树冠密度和树高信息。针对小样本问题，文本模态的知识（例如对某种飞机类别定义的文字描述）也有助于增加小样本类别的识别能力（Lu等，2023）。\n\n除了类别定义之外，不同对象之间先验关系的知识也有助于解译。基于建筑物和建筑物阴影的关系，可以利用阴影辅助确认漏提取的建筑物对象，从而提高建筑物的提取精度 （周亚男 等，2010）。同时，地理学定律可以指导模型设计。例如，基于地理学第一定律，即任何事物都是空间相关的而且越相近的地物越相关，可以将2D的目标检测模型转换为 的时序分类模型，并使用循环神经网络在火星陨石坑检测上取得了很好的结果 （Li 等，2021a）。\n\n# 5.2　色调光谱特征\n\n现有研究中应用到的色调光谱特征类知识主要包括地物色调色相饱和度特征、地物光谱特征、和地物光谱库数据。陈方等（2005）提出了基于光谱知识库与形状因子曲线的光谱匹配的算法，应用于 Landsat 影像的地物识别。Zhou 等 （2021）基于光谱库数据设计了基于短波红外光谱特征的分类器，对不同背景下不同组分的塑料材料进行分类。毛运欣等（2022）通过叠加高光谱数据和已有的矿产数据构建矿物光谱库，提取出露天矿物的空间位置和属性信息，采用光谱匹配和光谱特征参量匹配的模式识别相结合的算法实现露天\n\n矿物识别。\n\n此外，利用地物光谱特征可以生成模拟数据，基于不同作物类别在光谱空间中相对稳定的拓扑关系和历史标签数据，生成作物在目标年份的高质量标签数据，可以降低真实标签的采集成本并进行早季和季中的作物制图 （Lin等，2022）。\n\n# 5.3　空间几何特征\n\n空间几何特征类知识主要体现于地物对象的边缘、形状、尺度等特征，地物对象间的拓扑关系，以及地物类别的分布信息。图8例举了现有文献中空间几何特征知识的主要应用方式。Tang 等（ ）在输入中添加预提取的边缘特征（如霍夫变换），给偏向于识别纹理特征的卷积神经网络增加了形状偏置，提升了模型对于灌溉系统的检测精度和鲁棒性。除了地物的边缘信息，地物对象的大小等信息也很常用。Ghorbanzadeh 等 （2021）利用住宅大小和分布等专家知识对卷积神经网络的住宅区分割结果进行后处理，并得到了边缘更加锐利的分割结果。除了单个地物对象的几何特征外，不同地物对象之间的空间拓扑关系也有助于提升语义分割的精度 （Li 和 Stein，2020；Ouyang和Li，2021）。Liu等（2022）利用图卷积神经网络对具备不同土地覆盖类别的地物对象进行拓扑关系的构建，减小了土地覆盖于土地利用之间的语义鸿沟，并在公园分割数据集上取得了较好的分割结果。\n\n提高了城市区域地表覆盖分类的精度。\n\n# 5.4　时序物候特征\n\n现有研究中时序物候特征类知识主要包括作物的物候信息，时序谱特征等。例如，植被指数时序谱特征被用于作物的识别。王学等 （ ）通过建立华北平原冬小麦时序波谱曲线库，并结合农作物物候历制订统一规则，重建了华北平原年— 年冬小麦播种面积时空变化过程。物候信息还可以用于自动选择光谱时间特征来降低分类任务的难度，包括选取最具判别性的特征和去除冗余特征，用以提高作物分类的准确性（Hu 等，2019）。此外，Planque 等 （2021） 利用不同作物生长阶段的知识，在云覆盖严重区域，样本非常有限情况下实现了大范围时序分析和作物分类。在进行植被分类前，利用物候知识对时间序列进行归一化，有助于深度学习模型适应作物物候动态的时空变化，在转移到不同的年份时模型的拓展性和鲁棒性都能得到增强（Yang等，2023）。\n\n# 5.5　雷达回波特征\n\n遥感智能解译中目前利用的信号回波特征类知识主要包括雷达回波特征，SAR 回波特征和LiDAR点云特征。SAR回波特征已被用于遥感影像中的舰船目标识别。Zhang 等 （2021b） 利用影像学习船舶检测任务，针对舰船目标在SAR影像中的特定线索（例如，纹理和形状）设计了辅助子任务，可靠地支持高级别特征学习。等 （ ）针对 图像特殊成像机制引起的旁瓣效应和轮廓模糊，设计了一种利用SAR图像的旁瓣感知小型船舶检测网络，增强了船体形状信息。\n\n![](images/6ce4fb653c1e68a1b545b8238867a525ecef62978550f7e18f6c5452d4e7666d.jpg)  \n图8　空间几何特征知识的应用方式例举 Fig. 8 Application modes of knowledge on spatial geometry features\n\n此外，地物类别的空间分布知识对于语义分割任务的精度提升也是有益的。Li等 （2022b） 通过各类别空间分布规律和阴影高程信息完成地学知识推理，并以迭代的方式对分类结果进行纠正，\n\nLiDAR点云特征主要用于树木与桥梁的形态提取。杨海全等（ ）针对 下树冠形状及反射特征，设计了基于树冠形状及反射特性知识的 LiDAR 数据树木脚点的提取方法。Leigh 和Magruder （2016） 使用体素化方法将LiDAR网格生成伪波形，将伪波形中提取的参数映射到光栅层，然后用作随机森林分类器的输入，预测地面区域的土地覆盖分类。Li等（2020）提出了一个由几何感知卷积、密集层次结构和提升注意力模块组成的几何注意力网络，进行点云无人机点云数据分类。龚健雅等（2022）设计了针对LiDAR数据的遥感知识感知模块，提出一种基于多源数据的遥感知识感知与多尺度特征融合网络，在网络编码器端融入遥感知识感知模块 KAM （Knowledge-Aware Module），高效挖掘多源遥感数据中的遥感知识信息。\n\n# 5.6　地形地貌特征\n\n地形地貌特征类知识主要体现在地形（DEM、DSM、坡度等）、山地垂直带谱、和海拔高程数据的使用。骆剑承等 （2001） 基于高程和坡度生成地形因子对土地覆盖分类结果进行后处理以进一步提高结果精度。张俊瑶等 （ ） 和 等（2020）基于垂直分带和DSM生成地形约束因子并将其与多时相、高分辨率影像等数据结合，有效地提取了山区植被类型。地形知识除了作为地形约束因子，还可以为高分辨率影像提供高度信息，用以增强全卷积模型在下采样过程中丢失的空间细节信息，在 ISPRS Vaihingen 遥感影像语义分割数据集上获得了 $9 0 . 6 \\%$ 的 准 确 率 （Sun 和 Wang，2018）。类似地，Nguyen 等 （2022） 利用模型生成的DSM数据减去DEM得到植被高度，并基于植被高度完成植物的分类。为了减小地形阴影带来的“同物异谱、同谱异物”现象，刘时城等 （2017）在黄土丘陵区比较了 种不同的地形矫正算法，研究了不同地形矫正对刺槐遥感提取的影响。\n\n# 5.7　气象水文特征\n\n现有研究中应用到的气象水文特征类知识主要通过作为模型输入、多模态数据融合等方式融入到遥感解译过程中。例如，将多类气象水文特征数据作为领域知识判别器的输入可以将气象水文类知识融合到模型。Prodhan等 （2021） 将降水、植被、土壤因素输入深度前馈神经网络DFNN（Deep Forwarded Neural Network），捕捉到了不同水文阶段中具有高空间差异性的干旱模式，解译了南亚地区的农业干旱区域及其强度。\n\n除了作为深度学习模型的输入，也可以将气象水文特征与遥感影像进行耦合，构建具有系统工程特征的数值模型。郝莹（2022）将气象观测数据，气候模式预估数据进行耦合，构建了城市内涝数值模型，多尺度、定量化、精细化的解译了城市内涝区域及其强度。\n\n# 5.8　遥感指数特征\n\n遥感指数特征类知识主要是应用一些经验指数辅助分类，包括植被指数 、水指数 、建筑指数 等。例如， 建筑指数与变化检测方法相结合，可实现城市建筑用地变化及其内在驱动力分析（杨安妮 等，2014）。Phan等（ ）采用 、 等多种指数来增强输入影像的光谱特征，并基于随机森林分类器完成土地覆盖分类。类似地，NDWI等指数被用作随机森林分类器的辅助变量，来进行定量分析植被覆盖度分布、变化规律及其驱动因素（Li等，2022a）。\n\n此外，这些指数还可以作为正则项参与深度模型的训练，通过和模型生成的中间结果计算重建损失来引导模型的学习，提升了土地覆盖分类中少样本类别的精度（ 等， ）。\n\n# 5.9　地理位置分区\n\n地理位置可以辅助划分研究区域，进行精细化解译。张丹丹等 （2010）对广东省大亚湾区的土地利用时空分异规律进行分析，将研究区域划分为近岸陆域和潮间带两部分，根据两个区域自然条件的差异与地貌因子，对土地利用类型变化进行解译。齐文娟和杨晓梅 （2017） 利用高分辨率遥感影像与 ，结合计算地形部位指数地貌位置的方式，完成了精确的山地植被与平原植被的分别提取。王志华等 （2022，2024） 阐释了土地利用和地表覆被的区域原理与尺度原理。区域原理表明多类型地理要素在一定范围的空间单元内具有更强的一致性、在范围外存在差异性，因此地理分区有助于在不同空间单元对同一类地理要素进行更精确的提取与分析，例如可以在不同分区采用略有差异的模型对同一类地物进行解译。尺度原理则表明了划分区域大小的准则，需要保证认知结果的尺度与地球表层结构本征尺度是相匹配的。\n\n具体地理位置的先验知识也可以用于提升地理目标的识别与检测效果。黄梓航等 （2022）提出了一种结合地理知识的遥感影像目标实体关联方法，通过建立影像目标实体与已知目标地理位置的链接，构建了遥感影像目标知识抽取模型。Velasquez-Camacho 等 （2023） 使 用 深 度 森 林 技术，将目标检测模型与地理信息查询进行结合，对城市中的树种进行定位检测与识别，形成了新\n\n的城市森林监测方法。\n\n# 5.10　社会地理信息\n\n社会地理信息类知识主要是作为对遥感数据的补充，有助于更具体的城市内部功能信息，包括OSM、POI和带地理位置的社交媒体数据等。图例举了现有文献中社会地理信息知识的主要应用方式。Zhang等 （2017） 利用地表景观指标和微博数据作为地块特征进行细粒度的土地利用分类。OSM和POI数据常用于土地利用分类/城市功能区识别任务中， 路网数据用以完成地块划分，POI数据用以提供社会经济属性，最终结合影像特征获取地块的类型 （Song等，2018；Du等，2020；Zhong 等，2020；Su 等，2021）。\n\n![](images/497c1e4c8f4f5fe00517a8e5a135b99c34da3477f946df27146a087c41581e12.jpg)  \n图 　社会地理信息知识的应用方式例举  \nFig. 9 Application modes of knowledge on social geographic information\n\n此外，社会感知数据也用于城市功能区的分类，Cao等（2020）利用卷积神经网络和循环神经网络来融合影像和用户访问数据，实验结果表明这两种异源数据之间存在信息互补，融合后的特征有助于提升识别的准确度。同时，街景数据直观地呈现了街道景观真实形态，有助于构建了更有区分度的特征空间。Cao等 （2018）将街景影像用于土地利用分类，对稀疏分布的从街景影像中提取的语义特征进行空间插值，并与影像特征融合后进行分类，结果表明街景影像的使用可以提供有效的辅助信息并提升分类精度。类似地，崔成等（2022）从百度街景中提取街道空间品质特征，并与影像的光谱、形状、纹理等特征融合，完成了城中村的识别。轨迹数据也可以与影像数据结合，用以生成路网矢量数据（Bai等，2023）。\n\n# 5.11　行业准则规范\n\n目前对行业准则规范知识进行了充分结合的研究大多是道路和建筑物提取任务。Grinias 等（2016） 和王斌等 （2020） 利用路网的连通性来提升路网的检测结果，在道路检测结果的基础上，对道路断线情况进行了改善。林祥国和张继贤（ ）提出了面向建筑物的形态学指数 ，构建OBMBI图像并对该OBMBI图像二值化、矢量化以获取建筑物多边形。游永发等（2019）利用多尺度多方向的Gabor小波变换结果提取建筑物特征点，采用面向对象的思想构建空间投票矩阵，实现了高分辨率遥感影像建筑物的分级提取。常京新等 （2020） 提出了建筑物轴线倾斜程度最佳拟合方法，有效提高了建筑物轮廓的边缘表达精度。\n\n此外，利用地理图斑，可以更广泛地融合行业准则规范知识。吴田军等（ ）提出了空间粒计算思路，通过设计图斑提取架构和模型族，将地表空间结合行业准则规范逐步有序分层，形成了集成“分区分层感知、时空协同反演、多粒度决策”3个基础模型的遥感粒计算框架。\n\n# 5.12　历史数据产品\n\n利用历史数据产品类知识，可以对遥感专题产品进行变化检测和更新。Wu等 （2015）提出了一种基于先验知识的土地覆盖更新自动分层分类方法，将历史解译结果中的先验知识转移到新的目标任务中，检测出未变对象，重建典型地物类别与它们的空间光谱特征，更新土地覆盖地图。Paris等（2019）提出了一种无监督的方法，通过聚类分析和一致性分析，从历史产品中提取可靠的标记单元，再结合多光谱图像的时间序列对地表覆盖分类产品进行更新。杜培军等 （2020）以地理国情普查的矢量对象为统计单元，提出了基于对象实体统计分析的变化识别方法，实现了综合地理国情普查成果和遥感影像的地理国情变化检测与数据更新。 等（ ）基于不同类别在光谱空间相对稳定的拓扑（相对位置关系）和历史标签数据，生成目标年份的高质量标签数据。\n\n历史数据产品也可以直接用于提升解译效果。Uhl等（2021）联合使用多光谱对地遥感观测数据和扫描的地理参考历史地图，完成了长时相的城市区域的提取和评估。付榕榕和况忠（2023）以高分辨率历史影像为基础，辅以矿业权、矿产品冶炼用地等数据，根据影像纹理、色调等特征和相关资料多方位出发，对矿区土地占地类型进行了分类。\n\n# 5.13　现有解译模型\n\n遥感智能解译中目前利用的即有解译模型包括已有的成熟解译模型，例如传统的统计学习方法和深度学习与人工智能方法，也包括结合多种模型进行强强联合的集成学习，还包括模型知识迁移和大模型预训练。图10例举了现有文献中解译模型知识的主要应用方式。例如，在高光谱领域，通常需要进行模型迁移学习来解决源域和目标域之间的差异。Tao等（2019）利用主动迁移学习策略，使用从源域和目标域中选择的有限的训练样本对分层堆叠稀疏自编码器进行微调，有效地训练出灵活可扩展的深度网络。He等 （2020）使用异构迁移学习，使用注意力机制来调整预训练模型的特征图，解决异构数据集之间的差异。\n\n![](images/008d6189662d0dd98f4e559e7003e341da9d712c6ac85feba995db53f260cbcf.jpg)  \n图10　现有解译模型知识的应用方式例举 Fig. 10 Application modes of knowledge on existing remote sensing image interpretation models\n\n集成学习中进行模型知识的强强联合，也是对即有解译模型的利用。余东行等 （2020）提出了一种联合卷积神经网络与集成学习的遥感影像场景分类算法，构建由 Logistic 回归和支持向量机组成的Stacking集成模型，将预测概率结果融合构建概率特征。武复宇等 （2020） 利用集成学习，将随机森林和旋转森林作为基分类器，构建了多级联森林深度网络，精度相较于传统的高光谱遥感影像分类方法有所提升。\n\n此外，通过大量数据进行自监督预训练获取的基座大模型也可以作为即有解译模型用于下游任务。Sun等（2023）利用生成自监督学习（SSL），提出了一个名为 RingMo 的具有通用遥感特征表示能力的基础框架，在4种不同的下游任务上达到了最先进的水平，证明了所提出框架的有效性。\n\nWang等 （2023）构建了具有约1亿个参数的ViT（Vision Transformer），并使用新的旋转变尺寸窗口注意力来代替Transformer中的原始全注意力，提升了目标检测精度。\n\n除了适用基座大模型到下游任务上，利用即有解译模型也可以进行数据集的生成，Zhang 等（2021a）构建了遥感土地覆盖分类的弱向强（WTS）监督学习框架，使用少量样本训练SVM，用于生成初始种子像素级训练集，缓解了分割数据集标记像素不足的问题。\n\n# 5.14　知识关联图谱\n\n目前针对地学领域，已经构建出了大量的遥感知识图谱、灾害知识图谱和类别知识图谱。顾海燕等（ ）构建了地理本体驱动的遥感影像面向对象分析方法，基于地理本体的元素建模，进行地表覆盖分类实验。魏家旺等 （2020）引入知识工程中本体概念，对滑坡地理本体进行表达、建模，结合高分遥感影像数据，实现滑坡信息的自动提取。Li等 （2021b） 通过遥感知识图片的表示学习生成场景类别的语义表示，进而完成模型未见过的场景类别的推理。\n\nLi等（2022b）提出了一种协作增强框架，以迭代的方式将数据驱动的深度学习模块和知识引导的本体推理模块相结合，基于领域知识直接纠正深度学习模块的错误分类以提高分类性能。张永军等（2023）通过计算遥感知识图谱中相邻推理单元之间的空间关系和推理单元属性信息，结合知识图谱推理规则进行体系内推理。沈伟豪等（2023）利用灾害知识图谱，通过关联农业受灾面积、农作物类型、农作物价值实现了湖北省的洪涝灾害评估。Fang和Yan （2023） 构造了基于对象空间关系的遥感知识图片，以获得不同类别之间的空间关系，从而辅助多模态遥感图像分类。\n\n此外，除了主流的知识图谱和本体论推理，知识关联图谱类的知识还可以通过构建目标描述表和类别语义向量的方式进行解译。李彦胜等（ ）通过协同表示学习和 近邻算法来渐进修正类别的语义向量，缓解了零样本遥感影像场景分 类 中 类 别 语 义 空 间 的 偏 移 问 题 。 黄 梓 航 等（2022）构建目标信息描述表，建立了遥感影像目标实体与已有描述表的链接。\n\n# 6　知识与数据联合的能力提升案例\n\n为了进一步印证在遥感图像解译中加入知识的效益增量和能力提升，本文例举部分典型案例进行说明。\n\n（）耦合知识图谱的零样本遥感解译。遥感解译涉及的知识主体和种类繁多，知识图谱能够直接的以结构化方式描述实体之间的关系，实现对庞杂知识的简化拆解与持续关联累积。Li 等（2021b）在设计了一套较为完备的遥感领域本体的基础上，通过对大量文本、影像、矢量等数据的抽取，构建了以<本体，关系，本体>三元组形式表达的遥感解译知识图谱，其中本体及其属性数量超过3000个，关系三元组超过7000个，并利用知识图谱实现了未经训练场景的准确解译，即零样本遥感解译。例如对机场的识别，在加入了<飞机，停在，机场>、<机场，包含，跑道>、<机场，包含，停机坪>、<机场，包含，航站楼>、<机场，包含，交通标识>、<机场面积，大于，$3 \\mathrm { ~ k m } ^ { 2 } >$ 等关联知识后，不需要对机场这一类别进行训练，也可以通过飞机、跑道、航站楼等目标的识别及其关联关系判别出机场。本案例表明知识图谱的加入可以缓解数据驱动方法对训练样本的依赖，并且有助于时空场景可迁移性问题的突破。\n\n（）地理分区遥感指数增强的地表覆盖分类。地表覆盖具有较强的空间差异性，不同地理分区所呈现出的不同地表覆盖类型及其分布特点是区域性研究的对象之一。Phan 等 （2020） 对比分析了在蒙古国多类定量遥感指数对于当地地物分类的敏感度，实验考虑了基于Landsat 8地表反射率数据计算的归一化差值植被指数 NDVI、增强植被指数 、土壤调节植被指数 、归一化差值水指数 NDWI、归一化差值建筑指数 NDBI、熵Entropy （近红外波段）、高程 Elevation 等 13 种光谱与地形参量，实现结果表明加入遥感指数可以有效提升分类精度，并且高程和熵两类参量能更有效区分草地、农地、裸地、居住区、混合草地等蒙古国的主要地物类型。由于草原区物候的不稳定性以及草原居住区与裸地的相似性，常用的植被、水体、建筑类参量在该地区对分类的贡献度很小。本案例表明不同区域的地表覆盖特性先验知识与适宜的遥感指数先验选择有助于进一步提升分类精度，因地制宜。\n\n（） 知识驱动的用户分类体系自适应分类。数据驱动的遥感图像解译方法绝大部分都需要预先定义一套分类体系，训练样本和分类结果都遵循这套体系，体系一旦确定则难以改变。然而不同用户期望的类别定义经常与现有分类体系不一致，出现类别缺失、类别定义偏差、类别散乱等问题。 等（ ）通过寻找用户专家知识与遥感专家知识在概念底层的一致性，提出了一套可灵活定义与组合的类别属性概念框架，可以通过用户知识视角与遥感知识视角双向定义所需要的类别，并且不同定义之间可以定量精准转化。在这一框架下类别被定义为一系列中间属性的组合，并且这些属性都是可以从图像中提取，可以被分级量化的，例如常绿森林被定义为“总是 $^ +$ 强绿色 $^ +$ 植被”与“从不 $^ +$ 棕色 $+$ 裸地”的组合，调整属性的分级标准或属性的组合就可以定义新的常绿森林类别，不破坏已有的属性提取模型与样本数据。本案例表明知识驱动的方法可以使解译结果更贴合用户的需要，并具备可解释性与灵活性。\n\n以上案例从3个不同角度展示了知识对数据驱动方法带来的改进，除精度提升外还实现了零样本、场景迁移、可变类别体系等模式变化，补足了当前数据驱动范式的主要弱点，印证了知识与数据联合驱动遥感解译的理论与应用价值。\n\n# 7　知识与数据联合的遥感解译框架\n\n在知识体系、知识与数据融合方法、现有知识与数据联合解译应用的基础上，本文进一步对知识与数据双驱动的遥感图像智能解译框架进行设想与展望，如图11所示，框架主要包括如下5个部分：\n\n（）知识与数据汇聚 生成。遥感解译涉及的数据种类多、知识领域广、累积时间长、更新速度快、因此首先需要着力解决知识与数据的汇聚和生成问题，提供厚积薄发式的面向细分应用场景的可定制推荐知识与数据服务。最直接的任务是要对多来源多领域的知识与数据进行持续汇聚，相应的元数据和原始数据也是汇聚的重要对象且需进行整理和分析。在输入式的汇聚之外，也可主动进行生成式的知识与数据统计、抽取、表达。训练样本数据在当前的人工智能范式中发挥着关键作用，样本的广泛持续收集、模拟生成、人工标注是支撑智能应用的基础性工作。已有的解译模型、数据模型、物理模型、变化模型等模型作为一类知识载体，同样需要不断积累。为实现上述目标，需构建知识库、模型库、数据与样本库等实体库。\n\n![](images/60948a04dd3f962d833230c086e0a83518ff8cebf46bf10b5128a9c132b9e566.jpg)  \n图11　知识与数据联合驱动的遥感图像智能解译框架思路  \nFig. 11 Conceptional framework of knowledge and data compound driven remote sensing image interpretation\n\n（）知识与数据处理 世系管理。随着不同知识与数据的来源及种类不断丰富，其在内容体系、组织结构、时空尺度、行业领域、版本代际、质量标准等诸多方面的区别也愈发显著与普遍，知识与数据的处理与管理水平在更大程度上会影响其联合应用效果。现有的知识与数据治理框架一般包含预处理、清洗、量化、结构化等技术环节，未来对上述环节的要求除了拓展广度与精度外，还需要具备实时性与可定制性，根据应用需求动态实时的把知识和数据处理成所需的状态，而不只是以某些标准固定下来。当知识与数据经过长期累积，关注其来龙去脉的世系管理、血缘溯源技术的必要性将会凸显（Bose和Frew，2005）。数量的扩张一般会带来质量的不稳定，在丰富性得到满足后，质量会成为选择的标准，因此质量与不确定性的评价和分析也是需要重点关注的技术。知识图谱作为一种构建知识与数据关联关系的工具与载体，相关技术的重要性仍将持续。另有一项基础性的管理任务是支持知识与数据的备份、维护、分发、引用统计，并对使用情况进行多角度的挖掘。\n\n（3）知识与数据检索/推荐/定制。庞杂的知识与数据在面对对象化或场景化的具体应用时，更加依赖一套知识结构的搜索与推荐引擎，实现需求驱动的资源智能动态定制，深度关联、精准推送。常规的检索模式需要拓展到时空谱多维度与多领域专题联合检索。广泛关联的大型知识图谱需要针对具体解译对象或场景进行动态定制切片，排除无关和冗余关联。阈值、规则、模型等与对象或场景强相关的知识也需要定制化的导出与调整。在缺乏某些知识或数据的情况下，可以在现有基础上采取生成式策略，模拟或回溯所需知识或数据、实现缺失资源的输出，从而响应更广泛的需求。在面对不同分类体系、指标体系、方法体系等系统性差异时，知识与数据在不同体系间的动态迁移转换也是一项普遍任务。\n\n（4）知识与数据产权信任平台。知识与数据是研究和从业人员工作的结晶，对产权的明确界定与可信保障是实现知识与数据广泛汇聚、有效复用、价值共享的重要条件。遥感解译长期以来在知识、数据、模型、应用等方面积累了大量成果，本框架通过构建贯穿始终的产权信任平台对知识与数据产权进行持续管理。产权信任平台支持灵活的产权策略，覆盖现有共享协议类型，充分尊重作者的共享意愿，通过区块链技术加固信任基础，共同提升原创者的共享意愿。\n\n（5） 知识与数据驱动的遥感解译应用模式。知识与数据双重积累的“量变”以及两者之间的有机融合，将逐渐引发遥感解译应用的“质变”与“智变”，强化现有应用模式并催生新的模式。多域多模态联合解译的条件将愈发成熟，应用门槛降低，应用效果强化。遥感大模型可逐渐从通用大模型走向专业大模型，深化对遥感相关领域的理解程度，形成专业能力。随着解译对象在分类体系与时空尺度上的精细化，未知或未经充分训练对象的解译成为更现实的需求，并且在知识与数据的共同支持下此类零样本或小样本解译问题有望得到解决。在知识的引导与约束下，纯数据驱动的黑盒应用可以向白盒转化，实现受控与可信解译，贴近行业要求。依靠不同数据之间对比的应用在知识的参与下也可以转化为非对比式应用，例如单时相变化检测，以减少前置数据，增强即时性。当知识与数据积累到一定程度，能够模拟事物发展的驱动力法则时，可以尝试镜像世界的构建与分析，通过数字孪生 （ 等，2019） 与平行智能对复杂的解译要素或问题进行匹配、复现、演化、推理或预测。\n\n上述框架支持遥感解译知识与数据的持续凝聚与高效复用，有助于科学成果的“颗粒归仓”，推动价值共享，加快由“个人人体”向“群组群体”的成果汇聚方式转变，加快由“碎片化”向“体系化”的知识与数据工程形式转变，加快由“专家应用”向“普罗大众”的遥感应用服务方式转变，实现遥感智能解译理论方法的高频迭代，实现研究与应用的关联互动，一体化协同发展。\n\n# 8　结 语\n\n知识与数据在遥感图像解译不同阶段发挥的作用以及各自研究的成熟度虽有不同，但两者之间的互补性与协同应用的必要性已经显现，知识与数据双驱动的遥感图像解译正在从理念走向实现。深度学习当前是智能解译的主要方法与数据驱动的典型代表，在颠覆性的方法出现之前，知识与深度神经网络的融合是最具现实性的技术路径。深度学习的核心机制虽然带有封闭黑盒特性，但通过输入数据与特征、网络结构与训练、解译结果后处理等环节依然可以将知识导入解译过程，在不干扰网络自身抽象与泛化能力的条件下发挥知识的效用。遥感图像解译涉及的知识类型广泛，且对于各类知识均有与数据相结合的解译应用存在，然而这些应用多局限于特定知识在特定问题上的点状应用，系统性与综合性不足。面向未来的解译技术框架需实现知识与数据的多源持续汇聚、实时一致处理、世系溯源管理、关联检索推荐、按需聚合定制、场景迁移转换、产权信任管理等服务，并支持多模态、大模型、小样本、可解释、非对比、平行智能等应用模式，以实现知识与数据深度融合的体系化遥感图像解译研究与应用。\n\n# 参考文献（References）\n\nArvor D, Betbeder J, Daher F R G, Blossier T, Le Roux R, Corgne S, Corpetti T, De Freitas Silgueiro V and da Silva Junior C A. 2021. Towards user-adaptive remote sensing: knowledge-driven auto‐ matic classification of Sentinel-2 time series. Remote Sensing of Environment, 264: 112615 [DOI: 10.1016/j.rse.2021.112615]   \nBai X D, Feng X Y, Yin Y Y, Yang M C, Wang X Y and Yang X. 2023. Combining images and trajectories data to automatically generate road networks. Remote Sensing, 15(13): 3343 [DOI: 10.3390/ rs15133343]   \nBaroud S, Chokri S, Belhaous S and Mestari M. 2021. A brief review of graph convolutional neural network based learning for classify‐ ing remote sensing images. Procedia Computer Science, 191: 349- 354 [DOI: 10.1016/j.procs.2021.07.047]   \nBelgiu M and Drăguţ L. 2016. Random forest in remote sensing: a re‐ view of applications and future directions. ISPRS Journal of Pho‐ togrammetry and Remote Sensing, 114: 24-31 [DOI: 10.1016/j.is‐ prsjprs.2016.01.011]   \nBonettini N, Cannas E D, Mandelli S, Bondi L, Bestagini P and Tubaro S. 2021. Video face manipulation detection through en‐ semble of CNNs//2020 25th International Conference on Pattern Recognition (ICPR). Milan: IEEE: 5012-5019 [DOI: 10.1109/ ICPR48806.2021.9412711]   \nBose R and Frew J. 2005. Lineage retrieval for scientific data process‐ ing: a survey. ACM Computing Surveys (CSUR), 37(1): 1-28 [DOI: 10.1145/1057977.1057978]   \nCao R, Tu W, Yang C X, Li Q, Liu J, Zhu J S, Zhang Q, Li Q Q and Qiu G P. 2020. Deep learning-based remote and social sensing da‐ ta fusion for urban region function recognition. ISPRS Journal of Photogrammetry and Remote Sensing, 163: 82-97 [DOI: 10.1016/ j.isprsjprs.2020.02.014]   \nCao R, Zhu J S, Tu W, Li Q Q, Cao J Z, Liu B Z, Zhang Q and Qiu G P. 2018. Integrating aerial and street view images for urban land use classification. Remote Sensing, 10(10): 1553 [DOI: 10.3390/ rs10101553]   \nChang J X, Wang S X, Yang Y W and Gao X J. 2020. Hierarchical opti‐ mization method of building contour in high-resolution remote sensing images. Chinese Journal of Lasers, 47(10): 1010002 (常京 新, 王双喜, 杨元维, 高贤君. 2020. 高分遥感影像建筑物轮廓的 逐 级 优 化 方 法 . 中 国 激 光 , 47(10): 1010002) [DOI: 10.3788/ CJL202047.1010002]   \nChen F, Niu Z, Luo C F and Wang C Y. 2005. A new algorithm of ob‐ ject recognition based on spectral library for TM images. Remote Sensing Technology and Application, 20(4): 386-392 ( 陈 方 , 牛 铮, 骆成凤, 王长耀 . 2005. 一种基于光谱知识库的 TM 影像地 物识别方法 . 遥感技术与应用, 20(4): 386-392) [DOI: 10.3969/j. issn.1004-0323.2005.04.002]   \nChi M M, Plaza A, Benediktsson J A, Sun Z Y, Shen J S and Zhu Y Y. 2016. Big data for remote sensing: challenges and opportunities. Proceedings of the IEEE, 104(11): 2207-2219 [DOI: 10.1109/ JPROC.2016.2598228]   \nCui C, Zhao L, Ren H Y, Lu W L and Huang Y H. 2022. Integrating high-resolution remote sensing and street view images to identify urban villages: a case study in Yuexiu District, Guangzhou City. National Remote Sensing Bulletin, 26(9): 1802-1813 ( 崔 成 , 赵 璐, 任红艳, 逯伟利, 黄耀欢 . 2022. 耦合 GF-2 遥感影像与街景 影像的广州市城中村识别 . 遥感学报, 26(9): 1802-1813) [DOI: 10.11834/jrs.20210202]   \nCui Z J, Gao T, Talamadupula K and Ji Q. 2023. Knowledge-augment‐ ed deep learning and its applications: a survey. IEEE Transactions on Neural Networks and Learning Systems [DOI: 10.1109/ TNNLS.2023.3338619]   \nDing W and Wu J D. 2020. An airport knowledge-based method for ac‐ curate change analysis of airport runways in VHR remote sensing images. Remote Sensing, 12(19): 3163 [DOI: 10.3390/rs12193163]   \nDu P J, Liu P, Xia J S, Feng L, Liu S C, Tan K and Cheng L. 2014. Re‐ mote sensing image interpretation for urban environment analy‐ sis: methods, system and examples. Remote Sensing, 6(10): 9458- 9474 [DOI: 10.3390/rs6109458]   \nDu P J, Wang X, Meng Y P, Lin C, Zhang P and Lu G. 2020. Effective change detection approaches for geographic national condition monitoring and land cover map updating. Journal of Geo-Informa‐ tion Science, 22(4): 857-866 (杜培军, 王欣, 蒙亚平, 林聪, 张鹏, 卢刚. 2020. 面向地理国情监测的变化检测与地表覆盖信息更 新方法 . 地球信息科学学报, 22(4): 857-866) [DOI: 10.12082/ dqxxkx.2020.190747]   \nDu S J, Du S H, Liu B, Zhang X Y and Zheng Z J. 2020. Large-scale urban functional zone mapping by integrating remote sensing im‐ ages and open social data. GIScience and Remote Sensing, 57(3): 411-430 [DOI: 10.1080/15481603.2020.1724707]   \nEsteva A, Kuprel B, Novoa R A, Ko J, Swetter S M, Blau H M and Th‐ run S. 2017. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639): 115-118 [DOI: 10.1038/ nature21056]   \nFang J Y and Yan X F. 2023. Classification of multi-modal remote sensing images based on knowledge graph. International Journal of Remote Sensing, 44(15): 4815-4835 [DOI: 10.1080/01431161. 2023.2240032]   \nFu R R and Kuang Z. 2023. Study of mining activity information ex‐ traction by remote sensing image——Taken Xingren area as an example. Guizhou Geology, 40(2): 165-172 (付榕榕, 况忠 . 2023. 利用遥感影像提取矿业活动信息研究——以兴仁地区为例. 贵 州地质, 40(2): 165-172) [DOI: 10.3969/j.issn.1000-5943.2023.02.008]   \nGe Y, Zhang X N, Atkinson P M, Stein A and Li L F. 2022. Geosci‐ ence-aware deep learning: a new paradigm for remote sensing. Science of Remote Sensing, 5: 100047 [DOI: 10.1016/j.srs.2022. 100047]   \nGhaffarian S, Valente J, Van Der Voort M and Tekinerdogan B. 2021. Effect of attention mechanism in deep learning-based remote sens‐ ing image processing: a systematic literature review. Remote Sensing, 13(15): 2965 [DOI: 10.3390/rs13152965]   \nGhorbanzadeh O, Tiede D, Wendt L, Sudmanns M and Lang S. 2021. Transferable instance segmentation of dwellings in a refugee camp - integrating CNN and OBIA. European Journal of Remote Sensing, 54(S1): 127-140 [DOI: 10.1080/22797254.2020.1759456]   \nGong J Y, Zhang Z, Jia H W, Zhou H, Zhao Y X and Xiong H J. 2022. Multi-source data ground object extraction based on knowledgeaware and multi-scale feature fusion network. Geomatics and In‐ formation Science of Wuhan University, 47(10): 1546-1554 (龚健 雅, 张展, 贾浩巍, 周桓, 赵元昕, 熊汉江. 2022. 面向多源数据地 物提取的遥感知识感知与多尺度特征融合网络. 武汉大学学报 (信息科学版), 47(10): 1546-1554) [DOI: 10.13203/j.whugis20220580]   \nGrinias I, Panagiotakis C and Tziritas G. 2016. MRF-based segmenta‐ tion and unsupervised classification for building and road detec‐ tion in peri-urban areas of high-resolution satellite images. ISPRS Journal of Photogrammetry and Remote Sensing, 122: 145-166 [DOI: 10.1016/j.isprsjprs.2016.10.010]   \nGu H Y, Li H T, Yan L, Han Y S, Yu F, Yang Y and Liu Z J. 2018. A geographic object-based image analysis methodology based on geo-ontology. Geomatics and Information Science of Wuhan Uni‐ versity, 43(1): 31-36 (顾海燕, 李海涛, 闫利, 韩颜顺, 余凡, 杨懿, 刘正军. 2018. 地理本体驱动的遥感影像面向对象分析方法. 武 汉大学学报(信息科学版), 43(1): 31-36) [DOI: 10.13203/j.whu‐ gis20150468]   \nGu W, Lv Z H and Hao M. 2017. Change detection method for remote sensing images based on an improved Markov random field. Mul‐ timedia Tools and Applications, 76(17): 17719-17734 [DOI: 10. 1007/s11042-015-2960-3]   \nHao X J, Ji Z, Li X H, Yin L, Liu L Z Y, Sun M Y, Liu Q and Yang R J. 2021. Construction and application of a knowledge graph. Re‐ mote Sensing, 13(13): 2511 [DOI: 10.3390/rs13132511]   \nHao Y. 2022. Multi-scale Forecast and Projection of Urban Waterlog‐ ging Risk Based on Coupled Hydro-meteorological Modelling. Nanjing: Nanjing University (郝莹 . 2022. 气象水文耦合的城市 内涝风险多尺度预测与预估研究 . 南京: 南京大学) [DOI: 10. 27235/d.cnki.gnjiu.2021.000121]   \nHe X, Chen Y S and Ghamisi P. 2020. Heterogeneous transfer learning for hyperspectral image classification based on convolutional neu‐ ral network. IEEE Transactions on Geoscience and Remote Sens‐ ing, 58(5): 3246-3263 [DOI: 10.1109/TGRS.2019.2951445]   \nHospedales T M, Antoniou A, Micaelli P and Storkey A J. 2022. Metalearning in neural networks: a survey. IEEE Transactions on Pat‐ tern Analysis and Machine Intelligence, 44(9): 5149-5169 [DOI: 10.1109/TPAMI.2021.3079209]   \nHu Q, Sulla-Menashe D, Xu B D, Yin H, Tang H J, Yang P and Wu W B. 2019. A phenology-based spectral and temporal feature selec‐ tion method for crop mapping from satellite time series. Interna‐ tional Journal of Applied Earth Observation and Geoinformation, 80: 218-229 [DOI: 10.1016/j.jag.2019.04.014]   \nHua Y S, Mou L C and Zhu X X. 2020. Relation network for multila‐ bel aerial image classification. IEEE Transactions on Geoscience and Remote Sensing, 58(7): 4558-4572 [DOI: 10.1109/TGRS. 2019.2963364]   \nHuang Z H, Jiang B C and Wang Z Q. 2022. A remote sensing image object knowledge association method based on geographic knowl‐ edge. Bulletin of Surveying and Mapping, (10): 28-36 (黄梓航, 蒋 秉川, 王自全. 2022. 一种结合地理知识的遥感影像目标实体关 联方法 . 测绘通报, (10): 28-36) [DOI: 10.13474/j.cnki.11-2246. 2022.0290]   \nKattenborn T, Leitloff J, Schiefer F and Hinz S. 2021. Review on Con‐ volutional Neural Networks (CNN) in vegetation remote sensing. ISPRS Journal of Photogrammetry and Remote Sensing, 173: 24- 49 [DOI: 10.1016/j.isprsjprs.2020.12.010]   \nKushibar K, Valverde S, González-Villà S, Bernal J, Cabezas M, Oli‐ ver A and Lladó X. 2018. Automated sub-cortical brain structure segmentation combining spatial and deep convolutional features. Medical Image Analysis, 48: 177-186 [DOI: 10.1016/j. media. 2018.06.006]   \nLeigh H W and Magruder L A. 2016. Using dual-wavelength, fullwaveform airborne lidar for surface classification and vegetation characterization. Journal of Applied Remote Sensing, 10(4): 045001 [DOI: 10.1117/1.JRS.10.045001]   \nLi J, Wang J L, Zhang J, Liu C L, He S L and Liu L F. 2022a. Growingseason vegetation coverage patterns and driving factors in the Chi‐ na-Myanmar Economic Corridor based on Google Earth Engine and geographic detector. Ecological Indicators, 136: 108620 [DOI: 10.1016/j.ecolind.2022.108620]   \nLi J Y, Huang X and Gong J Y. 2019. Deep neural network for remotesensing image interpretation: status and perspectives. National Science Review, 6(6): 1082-1086 [DOI: 10.1093/nsr/nwz058]   \nLi M M and Stein A. 2020. Mapping land use from high resolution sat‐ ellite images by exploiting the spatial arrangement of land cover objects. Remote Sensing, 12(24): 4158 [DOI: 10.3390/rs12244158]   \nLi W W, Hsu C Y and Hu M S. 2021a. Tobler’s first law in GeoAI: a spatially explicit deep learning model for terrain feature detection under weak supervision. Annals of the American Association of Geographers, 111(7): 1887-1905 [DOI: 10.1080/24694452.2021. 1877527]   \nLi W Z, Wang F D and Xia G S. 2020. A geometry-attentional network for ALS point cloud classification. ISPRS Journal of Photogram‐ metry and Remote Sensing, 164: 26-40 [DOI: 10.1016/j.isprsjprs. 2020.03.016]   \nLi Y S, Kong D Y, Zhang Y J, Ji Z and Xiao R. 2020. Zero-shot remote sensing image scene classification based on robust cross-domain mapping and gradual refinement of semantic space. Acta Geodaet‐ ica et Cartographica Sinica, 49(12): 1564-1574 (李彦胜, 孔德宇, 张永军, 季铮, 肖锐 . 2020. 联合稳健跨域映射和渐进语义基准 修正的零样本遥感影像场景分类 . 测绘学报, 49(12): 1564- 1574) [DOI: 10.11947/j.AGCS.2020.20200139]   \nLi Y S, Kong D Y, Zhang Y J, Tan Y H and Chen L. 2021b. Robust deep alignment network with remote sensing knowledge graph for zero-shot and generalized zero-shot remote sensing image scene classification. ISPRS Journal of Photogrammetry and Remote Sensing, 179: 145-158 [DOI: 10.1016/j.isprsjprs.2021.08.001]   \nLi Y S, Ouyang S and Zhang Y J. 2022b. Combining deep learning and ontology reasoning for remote sensing image semantic segmenta‐ tion. Knowledge-Based Systems, 243: 108469 [DOI: 10.1016/j. knosys.2022.108469]   \nLi Y S, Zhou Y H, Zhang Y J, Zhong L H, Wang J and Chen J D. 2022c. DKDFN: domain knowledge-guided deep collaborative fu‐ sion network for multimodal unitemporal remote sensing land cover classification. ISPRS Journal of Photogrammetry and Re‐ mote Sensing, 186: 170-189 [DOI: 10.1016/j.isprsjprs.2022.02.013]   \nLi Z F, Liu H, Zhang Z L, Liu T T and Xiong N N. 2022d. Learning knowledge graph embedding with heterogeneous relation atten‐ tion networks. IEEE Transactions on Neural Networks and Learn‐ ing Systems, 33(8): 3961-3973 [DOI: 10.1109/TNNLS.2021.3055147]   \nLin C X, Zhong L H, Song X P, Dong J W, Lobell D B and Jin Z N. 2022. Early- and in-season crop type mapping without currentyear ground truth: generating labels from historical information via a topology-based approach. Remote Sensing of Environment, 274: 112994 [DOI: 10.1016/j.rse.2022.112994]   \nLin X G and Zhang J X. 2017. Object-based morphological building index for building extraction from high resolution remote sensing imagery. Acta Geodaetica et Cartographica Sinica, 46(6): 724-733 (林祥国, 张继贤. 2017. 面向对象的形态学建筑物指数及其高 分辨率遥感影像建筑物提取应用 . 测绘学报, 46(6): 724-733)   \nLiu R Y, Huang J J, Li G, Feng J S, Wu X L and Li T H. 2023. Revisit‐ ing temporal modeling for clip-based image-to-video knowledge transferring//Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver: IEEE: 6555-6564 [DOI: 10.1109/CVPR52729.2023.00634]   \nLiu S C, Wen Z M, Tao Y, Zhu D J, Zhang J and Zeng H W. 2017. In‐ fluence of different topographic correction methods on the remote sensing extraction of Robinia pseudoacacia distribution. Journal of Beijing Forestry University, 39(5): 25-33 (刘时城, 温仲明, 陶 宇, 朱朵菊, 张静, 曾鸿文. 2017. 不同地形校正方法对刺槐林遥 感 提 取 的 影 响 . 北 京 林 业 大 学 学 报 , 39(5): 25-33) [DOI: 10. 13332/j.1000-1522.20160309]   \nLiu X N, Zhou Y, Zhao J Q, Yao R, Liu B and Zheng Y. 2019. Siamese convolutional neural networks for remote sensing scene classifica‐ tion. IEEE Geoscience and Remote Sensing Letters, 16(8): 1200- 1204 [DOI: 10.1109/LGRS.2019.2894399]   \nLiu Z Q, Tang P, Zhang W X and Zhang Z. 2022. CNN-enhanced het‐ erogeneous graph convolutional network: inferring land use from land cover with a case study of park segmentation. Remote Sens‐ ing, 14(19): 5027 [DOI: 10.3390/rs14195027]   \nLu X N, Sun X, Diao W H, Mao Y Q, Li J X, Zhang Y D, Wang P J\n\nand Fu K. 2023. Few-shot object detection in aerial imagery guid‐ ed by text-modal knowledge. IEEE Transactions on Geoscience and Remote Sensing, 61: 5604719 [DOI: 10.1109/Tgrs. 2023. 3250448]   \nLuo J C, Zhou C H and Yang Y. 2001. ANN remote sensing classifica‐ tion model and its integration approach with geo-knowledge. Jour‐ nal of Remote Sensing (in Chinese), 5(2): 122-129 (骆剑承, 周成 虎, 杨艳. 2001. 人工神经网络遥感影像分类模型及其与知识集 成 方 法 研 究 . 遥 感 学 报 , 5(2): 122-129) [DOI: 10.3321/j. issn: 1007-4619.2001.02.010]   \nLuo R T, Zhang N, Han B and Yang L J. 2020. Context-aware zeroshot recognition//Proceedings of the 34th AAAI Conference on Artificial Intelligence. New York: AAAI: 11709-11716 [DOI: 10. 1609/aaai.v34i07.6841]   \nMa L, Liu Y, Zhang X L, Ye Y X, Yin G F and Johnson B A. 2019. Deep learning in remote sensing applications: a meta-analysis and review. ISPRS Journal of Photogrammetry and Remote Sensing, 152: 166-177 [DOI: 10.1016/j.isprsjprs.2019.04.015]   \nMao Y X, Zhao H Q, Feng S Q, Xu H X, He T and Song L J. 2022. Re‐ search on hyperspectral remote sensing open-pit minerals identifi‐ cation method based on spectral matching. Natural Resources In‐ formatization, 4: 28-32, 15 (毛运欣, 赵海强, 冯思琪, 徐红欣, 何 涛, 宋玲君. 2022. 基于光谱匹配的高光谱遥感露天矿物识别方 法研究 . 自然资源信息化, (4): 28-32, 15) [DOI: 10.3969/j.issn. 1674-3695.2022.04.005]   \nMottaghi R, Bagherinezhad H, Rastegari M and Farhadi A. 2016. New‐ tonian image understanding: unfolding the dynamics of objects in static images//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas: IEEE: 3521-3529 [DOI: 10.1109/CVPR.2016.383]   \nMou L C, Ghamisi P and Zhu X X. 2017. Deep recurrent neural net‐ works for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 55(7): 3639-3655 [DOI: 10. 1109/TGRS.2016.2636241]   \nMountrakis G, Im J and Ogole C. 2011. Support vector machines in re‐ mote sensing: a review. ISPRS Journal of Photogrammetry and Remote Sensing, 66(3): 247-259 [DOI: 10.1016/j. isprsjprs. 2010. 11.001]   \nNguyen T A, Kellenberger B and Tuia D. 2022. Mapping forest in the Swiss Alps treeline ecotone with explainable deep learning. Re‐ mote Sensing of Environment, 281: 113217 [DOI: 10.1016/j. rse. 2022.113217]   \nOuyang S and Li Y S. 2021. Combining deep semantic segmentation network and graph convolutional neural network for semantic seg‐ mentation of remote sensing imagery. Remote Sensing, 13(1): 119 [DOI: 10.3390/rs13010119]   \nParis C, Bruzzone L and Fernández-Prieto D. 2019. A novel approach to the unsupervised update of land-cover maps by classification of time series of multispectral images. IEEE Transactions on Geosci‐ ence and Remote Sensing, 57(7): 4259-4277 [DOI: 10.1109/TGRS. 2018.2890404]   \nPark E, Han X F, Berg T L and Berg A C. 2016. Combining multiple sources of knowledge in deep CNNs for action recognition//2016 IEEE Winter Conference on Applications of Computer Vision (WACV). Lake Placid: IEEE: 1-8 [DOI: 10.1109/WACV. 2016. 7477589]   \nPetitjean F, Inglada J and Gançarski P. 2012. Satellite image time se‐ ries analysis under time warping. IEEE Transactions on Geosci‐ ence and Remote Sensing, 50(8): 3081-3095 [DOI: 10.1109/ TGRS.2011.2179050]   \nPhan T N, Kuch V and Lehnert L W. 2020. Land cover classification using Google earth engine and random forest classifier—the role of image composition. Remote Sensing, 12(15): 2411 [DOI: 10. 3390/rs12152411]   \nPlanque C, Lucas R, Punalekar S, Chognard S, Hurford C, Owers C, Horton C, Guest P, King S, Williams S and Bunting P. 2021. Na‐ tional crop mapping using sentinel-1 time series: a knowledgebased descriptive algorithm. Remote Sensing, 13(5): 846 [DOI: 10.3390/rs13050846]   \nPouyanfar S, Sadiq S, Yan Y L, Tian H M, Tao Y D, Reyes M P, Shyu M L, Chen S C and Iyengar S S. 2018. A survey on deep learning: algorithms, techniques, and applications. ACM Computing Sur‐ veys (CSUR), 51(5): 92 [DOI: 10.1145/3234150]   \nProdhan F A, Zhang J H, Yao F M, Shi L M, Pangali Sharma T P, Zhang D, Cao D, Zheng M X, Ahmed N and Mohana H P. 2021. Deep learning for monitoring agricultural drought in South Asia using remote sensing data. Remote Sensing, 13(9): 1715 [DOI: 10. 3390/rs13091715]   \nQi W J and Yang X M. 2017. Mountain and plain vegetation boundar‐ ies extraction in Duchang county province Jiangxi. Journal of Geo-Information Science, 19(4): 559-569 ( 齐 文 娟 , 杨 晓 梅 . 2017. 江西省都昌县山地与平原植被界线提取 . 地球信息科学 学报, 19(4): 559-569) [DOI: 10.3969/j.issn.1560-8999.2017.04.014]   \nRaissi M, Perdikaris P and Karniadakis G E. 2019. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equa‐ tions. Journal of Computational Physics, 378: 686-707 [DOI: 10. 1016/j.jcp.2018.10.045]   \nShen W H, Zhong Y F, Wang J J, Zheng Z and Ma A L. 2023. Con‐ struction and application of flood disaster knowledge graph based on multi-modal data. Geomatics and Information Science of Wu‐ han University, 48(12): 2009-2018 (沈伟豪, 钟燕飞, 王俊珏, 郑 卓, 马爱龙 . 2023. 多模态数据的洪涝灾害知识图谱构建与应 用 . 武汉大学学报(信息科学版), 48(12): 2009-2018) [DOI: 10. 13203/j.whugis20220509]   \nShen X Q, Liu B, Zhou Y, Zhao J Q and Liu M M. 2020. Remote sens‐ ing image captioning via Variational Autoencoder and Reinforce‐ ment Learning. Knowledge-Based Systems, 203: 105920 [DOI: 10.1016/j.knosys.2020.105920]   \nSong J C, Lin T, Li X H and Prishchepov A V. 2018. Mapping urban functional zones by integrating very high spatial resolution re‐ mote sensing imagery and points of interest: a case study of Xia‐ men, China. Remote Sensing, 10(11): 1737 [DOI: 10.3390/rs10111737]   \nSu Y, Zhong Y F, Zhu Q Q and Zhao J. 2021. Urban scene understand‐ ing based on semantic and socioeconomic features: from high-res‐ olution remote sensing imagery to multi-source geographic datas‐ ets. ISPRS Journal of Photogrammetry and Remote Sensing, 179: 50-65 [DOI: 10.1016/j.isprsjprs.2021.07.003]   \nSun W W and Wang R S. 2018. Fully convolutional networks for se‐ mantic segmentation of very high resolution remotely sensed im‐ ages combined with DSM. IEEE Geoscience and Remote Sensing Letters, 15(3): 474-478 [DOI: 10.1109/LGRS.2018.2795531]   \nSun X, Wang P J, Lu W X, Zhu Z C, Lu X N, He Q B, Li J X, Rong X E, Yang Z J, Chang H, He Q L, Yang G, Wang R P, Lu J W and Fu K. 2023. RingMo: a remote sensing foundation model with masked image modeling. IEEE Transactions on Geoscience and Remote Sensing, 61: 5612822 [DOI: 10.1109/TGRS.2022.3194732]   \nTang J W, Zhang Z, Zhao L J and Tang P. 2021. Increasing shape bias to improve the precision of center pivot irrigation system detec‐ tion. Remote Sensing, 13(4): 612 [DOI: 10.3390/rs13040612]   \nTao F, Zhang H, Liu A and Nee A Y C. 2019. Digital twin in industry: state-of-the-art. IEEE Transactions on Industrial Informatics, 15(4): 2405-2415 [DOI: 10.1109/TII.2018.2873186]   \nUhl J H, Leyk S, Li Z K, Duan W W, Shbita B, Chiang Y Y and Knob‐ lock C A. 2021. Combining remote-sensing-derived data and his‐ torical maps for long-term back-casting of urban extents. Remote Sensing, 13(18): 3672 [DOI: 10.3390/rs13183672]   \nVelasquez-Camacho L, Etxegarai M and de-Miguel S. 2023. Imple‐ menting Deep Learning algorithms for urban tree detection and geolocation with high-resolution aerial, satellite, and ground-level images. Computers, Environment and Urban Systems, 105: 102025 [DOI: 10.1016/j.compenvurbsys.2023.102025]   \nWang B, Chen Z L, Wu L, Xie P, Fan D L and Fu B L. 2020. Road ex‐ traction of high-resolution satellite remote sensing images in UNet network with consideration of connectivity. Journal of Re‐ mote Sensing (in Chinese), 24(12): 1488-1499 (王斌, 陈占龙, 吴 亮, 谢鹏, 范冬林, 付波霖. 2020. 兼顾连通性的U-Net网络高分 辨率遥感影像道路提取 . 遥感学报, 24(12): 1488-1499) [DOI: 10.11834/jrs.20209301]   \nWang D, Zhang Q M, Xu Y F, Zhang J, Du B, Tao D C and Zhang L P. 2023. Advancing plain vision transformer toward remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing, 61: 5607315 [DOI: 10.1109/TGRS.2022.3222818]   \nWang X, Li X B, Tan M H and Xin L J. 2015. Remote sensing monitor‐ ing of changes in winter wheat area in North China Plain from 2001 to 2011. Transactions of the Chinese Society of Agricultural Engineering, 31(8): 190-199 (王 学 , 李 秀 彬 , 谈 明 洪 , 辛 良 杰 . 2015. 华北平原 2001—2011 年冬小麦播种面积变化遥感监测 . 农业工程学报, 31(8): 190-199) [DOI: 10.3969/j.issn.1002-6819. 2015.08.028]   \nWang Z H, Gao K, Yang X M, Su F Z, Huang C, Shi T Z, Yan F Q, Li H, Zhang H F, Lü N and Pan T T. 2022. Land use/land cover clas‐ sification development from a geographical perspective. Geo‐ graphical Research, 41(11): 2946-2962 (王志华, 郜酷, 杨晓梅, 苏 奋振, 黄翀, 石铁柱, 颜凤芹, 李贺, 张慧芳, 吕宁, 潘婷婷 . 2022. 地理学视角下土地利用/覆被分类发展探讨 . 地理研究, 41(11): 2946-2962) [DOI: 10.11821/dlyj020220076]   \nWang Z H, Yang X M, Liu Y M, Liu B, Zhang J Y, Liu X L, Meng D, Gao K, Zeng X W and Ding Y X. 2024. Geographical principles of remote sensing image analysis and the hierarchical patch mod‐ el based analysis framework. National Remote Sensing Bulletin, 28(6): 1412-1424 (王志华, 杨晓梅, 刘岳明, 刘彬, 张俊瑶, 刘晓 亮, 孟丹, 郜酷, 曾晓伟, 丁亚新. 2024. 遥感影像地学分析的地 理学原理及等级斑块建模框架 . 遥感学报, 28(6): 1412-1424) [DOI: 10.11834/jrs.20232356]   \nWang Z H, Yang X M and Zhou C H. 2021. Geographic knowledge graph for remote sensing big data. Journal of Geo-Information Science, 23(1): 16-28 (王志华, 杨晓梅, 周成虎 . 2021. 面向遥感 大数据的地学知识图谱构想. 地球信息科学学报, 23(1): 16-28) [DOI: 10.12082/dqxxkx.2021.200632]   \nWei J W, Hui W H, Cheng M Z and Li H. 2020. Geographic ontologydriven object oriented landslide recognition. Remote Sensing In‐ formation, 35(2): 94-99 (魏家旺, 惠文华, 程梦真, 李海 . 2020. 地 理 本 体 驱 动 的 面 向 对 象 滑 坡 识 别 . 遥 感 信 息 , 35(2): 94-99) [DOI: 10.3969/i.issn.1000-3177.2020.02.015]   \nWu F Y, Wang X, Ding J W, Du P J and Tan K. 2020. Improved cas‐ cade forest deep learning model for hyperspectral imagery classi‐ fication. National Remote Sensing Bulletin, 24(4): 439-453 (武复 宇, 王雪, 丁建伟, 杜培军, 谭琨 . 2020. 高光谱遥感影像多级联 森林深度网络分类算法 . 遥感学报, 24(4): 439-453) [DOI: 10. 11834/jrs.20209190]   \nWu T J, Luo J C, Xia L G, Shen Z F and Hu X D. 2015. Prior knowl‐ edge-based automatic object-oriented hierarchical classification for updating detailed land cover maps. Journal of the Indian Soci‐ ety of Remote Sensing, 43(4): 653-669 [DOI: 10.1007/s12524- 014-0446-9]   \nWu T J, Luo J C, Zhang X, Dong W, Huang Q T, Zhou Y N, Liu W, Sun Y W, Yang Y P, Hu X D and Gao L J. 2023. Remote sensing granular computing and precise applications based on geo-par‐ cels. National Remote Sensing Bulletin, 27(12): 2774-2795 (吴田 军, 骆剑承, 张新, 董文, 黄启厅, 周亚男, 刘巍, 孙营伟, 杨颖频, 胡晓东, 郜丽静 . 2023. 基于地理图斑的遥感粒计算与精准应 用 . 遥感学报, 27(12): 2774-2795) [DOI: 10.11834/jrs.20211622]   \nXie M, Jean N, Burke M, Lobell D and Ermon S. 2016. Transfer learn‐ ing from deep features for remote sensing and poverty mapping// Proceedings of the 30th AAAI Conference on Artificial Intelli‐ gence. Phoenix: AAAI: 3929-3935 [DOI: 10.1609/aaai.v30i1.9906]   \nXie X Z, Niu J W, Liu X F, Chen Z S, Tang S J and Yu S. 2021. A sur‐ vey on incorporating domain knowledge into deep learning for medical image analysis. Medical Image Analysis, 69: 101985 [DOI: 10.1016/j.media.2021.101985]   \nXie Y T, Xia Y, Zhang J P, Song Y, Feng D G, Fulham M and Cai W D. 2019. Knowledge-based collaborative deep learning for benignmalignant lung nodule classification on chest CT. IEEE Transac‐ tions on Medical Imaging, 38(4): 991-1004 [DOI: 10.1109/TMI. 2018.2876510]   \nXie Y T, Zhang J P, Xia Y, Fulham M and Zhang Y N. 2018. Fusing texture, shape and deep model-learned information at decision lev‐ el for automated classification of lung nodules on chest CT. Infor‐ mation Fusion, 42: 102-110 [DOI: 10.1016/j.inffus.2017.10.005] Xu J Y, Zhang Z L, Friedman T, Liang Y T and van den Broeck G.   \n2018. A semantic loss function for deep learning with symbolic knowledge. arXiv:1711.11157 [DOI: 10.48550/arXiv.1711.11157] Yang A N, Xu Y H and Su H J. 2014. Urban built-up land extraction and change detection analysis using built-up indexes. Geomatics and Spatial Information Technology, 37(8): 30-34 (杨安妮, 许亚 辉, 苏红军. 2014. 结合建筑指数的城市建筑用地提取与变化检 测分析 . 测绘与空间地理信息, 37(8): 30-34) [DOI: 10.3969/j. issn.1672-5867.2014.08.009] Yang H Q, Yu J, Qin K and Zhang G N. 2006. The research on the ground objects extraction from the LIDAR based on knowledge. Bulletin of Surveying and Mapping, 12: 9-11, 15 (杨海全, 余洁, 秦昆, 张国宁. 2006. 基于知识的LIDAR数据地物提取研究. 测 绘通报, (12): 9-11, 15) [DOI: 10.3969/j.issn.0494-0911.2006.12.003] Yang W K, Zhao J J, Qiang Y, Yang X T, Dong Y Y, Du Q Q, Shi G H and Zia M B. 2019. DScGANS: integrate domain knowledge in training dual-path semi-supervised conditional generative adver‐ sarial networks and S3VM for ultrasonography thyroid nodules classification//22nd International Conference on Medical Image Computing and Computer Assisted Intervention. Shenzhen: Springer: 558-566 [DOI: 10.1007/978-3-030-32251-9_61] Yang Z J, Diao C Y and Gao F. 2023. Towards scalable within-season crop mapping with phenology normalization and deep learning. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 16: 1390-1402 [DOI: 10.1109/JSTARS.   \n2023.3237500] Yao Y H, Suonan D Z and Zhang J Y. 2020. Compilation of 1:50, 000 vegetation type map with remote sensing images based on moun‐ tain altitudinal belts of Taibai Mountain in the north-south transi‐ tional zone of China. Journal of Geographical Sciences, 30(2):   \n267-280 [DOI: 10.1007/s11442-020-1727-6] Yin H X, Molchanov P, Alvarez J M, Li Z Z, Mallya A, Hoiem D, Jha N K and Kautz J. 2020. Dreaming to distill: data-free knowledge transfer via deepinversion//Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle: IEEE: 8712-8721 [DOI: 10.1109/CVPR42600.2020.00874] You Y F, Wang S Y, Wang B, Ma Y X, Shen M, Liu W H and Xiao L.   \n2019. Study on hierarchical building extraction from high resolu‐ tion remote sensing imagery. Journal of Remote Sensing (in Chi‐ nese), 23(1): 125-136 (游永发, 王思远, 王斌, 马元旭, 申明, 刘卫 华, 肖琳. 2019. 高分辨率遥感影像建筑物分级提取. 遥感学报,   \n23(1): 125-136) [DOI: 10.11834/jrs.20197500] Yu D X, Zhang B M, Zhao C, Guo H T and Lu J. 2020. Scene classifi‐ cation of remote sensing image using ensemble convolutional neu‐ ral network. Journal of Remote Sensing (in Chinese), 24(6): 717-   \n727 (余东行, 张保明, 赵传, 郭海涛, 卢俊 . 2020. 联合卷积神经 网络与集成学习的遥感影像场景分类 . 遥感学报, 24(6): 717-   \n727) [DOI: 10.11834/jrs.20208273] Zhang D D, Yang X M, Su F Z, Du Y Y, Sun X Y and Xue Z S. 2010. Spatio-temporal differences in land use change in the Daya Bay and correlation with geomorphologic indicators. Resources Sci‐ ence, 32(8): 1551-1557 (张丹丹, 杨晓梅, 苏奋振, 杜云艳, 孙晓 宇, 薛振山. 2010. 大亚湾近岸土地利用的时空分异及其与地貌 因子关系分析 . 资源科学, 32(8): 1551-1557)   \nZhang J X, Gu H Y, Yang Y, Zhang H and Li H T. 2021. Research prog‐ ress and trend of high-resolution remote sensing imagery intelli‐ gent interpretation. National Remote Sensing Bulletin, 25(11): 2198-2210 (张继贤, 顾海燕, 杨懿, 张鹤, 李海涛 . 2021. 高分辨 率遥感影像智能解译研究进展与趋势. 遥感学报, 25(11): 2198- 2210) [DOI: 10.11834/jrs.20210382]   \nZhang J Y, Yao Y H, Suonan D Z, Gao L J, Wang J and Zhang X H. 2019. Mapping of mountain vegetation in Taibai Mountain based on mountain altitudinal belts with remote sensing. Journal of GeoInformation Science, 21(8): 1284-1294 (张俊瑶, 姚永慧, 索南东 主, 郜丽静, 王晶, 张兴航. 2019. 基于垂直带谱的太白山区山地 植 被 遥 感 信 息 提 取 . 地 球 信 息 科 学 学 报 , 21(8): 1284-1294) [DOI: 10.12082/dqxxkx.2019.180650]   \nZhang W, Tang P, Corpetti T and Zhao L J. 2021a. WTS: a weakly to‐ wards strongly supervised learning framework for remote sensing land cover classification using segmentation models. Remote Sensing, 13(3): 394 [DOI: 10.3390/rs13030394]   \nZhang X, Huo C L, Xu N, Jiang H Z, Cao Y, Ni L and Pan C H. 2021b. Multitask learning for ship detection from synthetic aperture radar images. IEEE Journal of Selected Topics in Applied Earth Obser‐ vations and Remote Sensing, 14: 8048-8062 [DOI: 10.1109/ JSTARS.2021.3102989]   \nZhang Y, Li Q Z, Huang H P, Wu W, Du X and Wang H Y. 2017. The combined use of remote sensing and social sensing data in finegrained urban land use mapping: a case study in Beijing, China. Remote Sensing, 9(9): 865 [DOI: 10.3390/rs9090865]   \nZhang Y J, Wang F, Li Y S, Ouyang S, Wei D, Liu X J, Kong D Y, Chen R X and Zhang B. 2023. Remote sensing knowledge graph construction and its application in typical scenarios. National Re‐ mote Sensing Bulletin, 27(2): 249-266 (张永军, 王飞, 李彦胜, 欧 阳松, 魏东, 刘晓建, 孔德宇, 陈瑞贤, 张斌. 2023. 遥感知识图谱 创建及其典型场景应用技术 . 遥感学报, 27(2): 249-266) [DOI: 10.11834/jrs.20210469]   \nZhang Y Z, Liu J J and Shen W J. 2022. A review of ensemble learning algorithms used in remote sensing applications. Applied Sciences, 12(17): 8654 [DOI: 10.3390/app12178654]   \nZheng H, Lin L F, Hu H J, Zhang Q W, Chen Q Q, Iwamoto Y, Han X H, Chen Y W, Tong R F and Wu J. 2019. Semi-supervised seg‐ mentation of liver using adversarial learning with deep atlas prior// 22nd International Conference on Medical Image Computing and Computer Assisted Intervention. Shenzhen: Springer: 148-156 [DOI: 10.1007/978-3-030-32226-7_17]   \nZhong Y F, Su Y, Wu S Q, Zheng Z D, Zhao J, Ma A L, Zhu Q Q, Ye R C, Li X M, Pellikka P and Zhang L P. 2020. Open-source datadriven urban land-use mapping integrating point-line-polygon se‐ mantic objects: a case study of Chinese cities. Remote Sensing of Environment, 247: 111838 [DOI: 10.1016/j.rse.2020.111838]\n\nZhou S Y, Kuester T, Bochow M, Bohn N, Brell M and Kaufmann H.\n\n2021. A knowledge-based, validated classifier for the identifica‐ tion of aliphatic and aromatic plastics by WorldView-3 satellite data. Remote Sensing of Environment, 264: 112598 [DOI: 10. 1016/j.rse.2021.112598]\n\nZhou Y N, Shen Z F, Luo J C, Chen Q X, Hu X D and Shen J X. 2010. Shadow-assisted object-oriented extraction of urban buildings. Geography and Geo-Information Science, 26(3): 37-40 (周亚男, 沈占锋, 骆剑承, 陈秋晓, 胡晓东, 沈金祥. 2010. 阴影辅助下的\n\n面向对象城市建筑物提取. 地理与地理信息科学, 26(3): 37-40) Zhou Y S, Liu H C, Ma F, Pan Z X and Zhang F. 2023. A sidelobeaware small ship detection network for synthetic aperture radar imagery. IEEE Transactions on Geoscience and Remote Sensing, 61: 5205516 [DOI: 10.1109/TGRS.2023.3264231]\n\nZhuang J F, Dong Y and Bai H L. 2021. Ensemble learning with sia‐ mese networks for visual tracking. Neurocomputing, 464: 497- 506 [DOI: 10.1016/j.neucom.2021.08.025]\n\n# Knowledge and data driven remote sensing image interpretation： Recent developments and prospects\n\nMENG Yu1 ，CHEN Jingbo1 ，ZHANG Zheng1 ，LIU Zhiqiang1，2 ，ZHAO Zhitao1，2 ，HUO Lianzhi1 ，SHI Keli1，2 ，LIU Diyou1 ，DENG Yupeng1 ，TANG Ping1\n\n1.Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China; 2.School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing 101408, China\n\nAbstract：Knowledge and data are the two main elements that have characterized the development of remote sensing image interpretation for decades. With the continuous enrichment of sensor platforms and rapid breakthroughs in deep learning, big data, multi-modal, and long time-series methodologies, data-driven intelligent remote sensing image interpretation has become a hot research direction in recent years. However, in the deepening and expanding research and applications, the limitations of data-driven methods such as difficult reuse between different scenarios, strong training sample dependence, and weak interpretability are beginning to emerge. Various types of knowledge accumulated in the long-term remote sensing image interpretation practice have the characteristics of objective reality, certainty, scene adaptability, interpretability, etc., which can be complemented with data-driven approaches, and the dual-driven of knowledge and data is becoming a new direction of remote sensing image interpretation. This paper first reviews the major stages in the development of remote sensing image interpretation and the respective roles of knowledge and data in each of these stages. Then the main types of knowledge involved in remote sensing image interpretation are summarized and categorized into fourteen types. The fusion of knowledge and deep learning is an important path to achieve the dual-drive of knowledge and data, and this paper summarizes five categories and fifteen subcategories of knowledge and deep neural network fusion methods with relevant cases. From the perspective of knowledge types, this paper further provides an overview of existing applications of remote sensing interpretation with joint knowledge and data. The effectiveness and capability increment of fusing knowledge and data is demonstrated by the analyses of typical examples. Lastly, this paper gives a systematic prospect on the framework and key techniques for knowledge and data compound driven remote sensing image interpretation.\n\nKey words： remote sensing image interpretation, knowledge driven, data driven, artificial intelligence, knowledge graphs, deep learning, natural resources, review\n\nSupported by National Key Research and Development Program of China (No. 2021YFB3900503)",
    "vectorizeStatus": "success",
    "chunked": true,
    "chunkCount": 78
  },
  {
    "id": "270a08d8-df19-4f66-af70-d0f683134ae1",
    "title": "SCDL_Sketch_Causal_Disentangled_Learning_for_Sketch-Based_3D_Shape_Retrieval",
    "fileName": "SCDL_Sketch_Causal_Disentangled_Learning_for_Sketch-Based_3D_Shape_Retrieval.pdf",
    "fileType": "pdf",
    "fileSize": 4309874,
    "uploadTime": "2025-12-30T16:54:19.838718",
    "parsed": true,
    "parseStatus": "success",
    "chunked": true,
    "vectorizeStatus": "success",
    "tags": [],
    "folderId": "35804038-e3dc-4539-90a0-201f2e92ec80",
    "filePath": "./uploads/20251230165419_SCDL_Sketch_Causal_Disentangled_Learning_for_Sketch-Based_3D_Shape_Retrieval.pdf",
    "markdownContent": "# SCDL: Sketch Causal Disentangled Learning for Sketch-Based 3D Shape Retrieval\n\nShaojin Bai , Yalu Li, Rihao Chang , Qi Liang , and Weizhi Nie , Member, IEEE\n\nAbstract— Sketch-based 3D shape retrieval (SBSR) has been a challenging task for decades, crucially depending on aligning shared semantic attributes between sketches and 3D shapes. Previous efforts mainly aimed at creating a common embedding space to bridge domain gaps. However, sketches’ subjective and abstract nature, known as confounders, potentially reduces learning performance of matching with 3D shapes. To address this issue, in this paper, we propose a sketch causal disentangled learning for SBSR, named SCDL, which introduce causal intervention to explicitly disentangle sketches into the inherent shared semantic part, and other unrelated confounders to classification (styles, abstraction levels, etc.) for the first time. Specifically, we construct a structural causal model (SCM) in the sketch branch under the dual variational autoencoder (VAE) architectures to alleviate confounders negative impact through learning the semantic attributes in the latent variable space. Next, we adopt a learning strategy on the separated semantic latent variables to construct a shared semantic embedding space further to make cross-modal features of the same class more similar, alleviating the cross-modality discrepancies effectively and establishing new state-of-the-art on three benchmarks. Comprehensive experiment results, ablation studies, and visualization validate the effectiveness of our approach.\n\nIndex Terms— Sketch-based 3D shape retrieval, causal inference, disentangled learning.\n\n# I. INTRODUCTION\n\nWITH the rapid advancements in computer hardwareand devices, there has been an exponential increase in 3D shapes, making effective 3D shape retrieval a crucial task [1], [2], [3], [4], [5]. Compared with traditional keywordsand text-based 3D shape retrieval methods, sketches are easy to obtain for their conciseness and intuitions with the rapid development of recent touch-pad devices, becoming a more intuitive and convenient medium for humans to interact with 3D shapes. In the field of multimedia, the sketch-based 3D shape retrieval(SBSR) task attracts more and more attention [6], [7], [8], [9]. However, the task remains challenging due to the substantial cross-domain discrepancies between sketches and 3D shapes.\n\nThe fundamental challenge in SBSR lies in the significant differences [10], [11] in modality. Sketches are a type of 2D image composed of sparse lines, which are drastically different from 3D shapes containing detailed stereoscopic information. This results in a significant representational gap from 3D shape features, making cross-domain feature alignment exceedingly difficult. Most previous research has been focused on constructing a common embedding space to align cross-modal features. Dai et al. [12] proposed a deep correlated metric learning method to learn two deep nonlinear transformations (one for each domain) jointly, mapping features from both domains into a nonlinear feature space. Subsequently, Dai et al. [13] proposed a method with loss at both hidden layer and output layer to further improve the similarity between sketches and 3D shapes features by encouraging features in the hidden layer also with desired properties. However, these approaches mainly focus on global features of samples in the embedding space, which relies on elaborately designed sophisticated metric functions.\n\nSimilar to other cross-domain retrieval tasks [14], [15], [16], [17], despite the significant discrepancies between crossmodality data, sketches and 3D shapes of the same category often share common semantic attributes, which is the key characteristic that the SBSR task relies on. With the development of semantic feature disentanglement research, some existing studies have utilized disentanglement operations to remove noisy information from features, which improves the quality of learned representations. Liang et al. [18] first revealed that noisy data is a key factor behind unsatisfactory retrieval performance and proposed uncertainty learning for noise-resistant sketch-based 3D shape retrieval. Subsequently, Bai and Bai [19] proposed a source-agnostic adversarial network, enabling the network to adaptively focus on more discriminative semantic information from global and local features. Li et al. [20] proposed a method consists of two levels of encoders to capture both local discriminative and multi-scale global structures while minimizing the impact of various sketch drawing details. These methods can eliminate irrelevant attribute information to a certain extent and improve performance. However, they tend to fall short of capturing intrinsic attributes and may even rely on external data annotation.\n\n![](images/61766dca6257208de858fc23418fb123849c5457f7b023b724aac2b2eb8f892d.jpg)  \nFig. 1. An illustration of previous methods and our SCDL. (a) illustrates how different users sketch the same object instance (an alarm clock here) in highly varied ways. (b) illustrates a cartoon example of disentanglement, where initial features containing both semantic information and confounders. (c) depicts the previous method, which constructs a joint embedding space using initial features that are highly entangled with both semantic attributes and confounders. (d) illustrates our SCDL, which constructs a semantic embedding space using disentangled features that contain only the shared semantic attributes.\n\n# A. Motivation\n\nIn SBSR, a critical issue that has been largely overlooked and significantly affects the effectiveness of existing methods is the inherent randomness and variability in sketches [21]. Since sketches are drawn mainly by humans, their final appearance can vary greatly due to subjective understanding, individual drawing techniques, and differences in style and abstraction levels (as illustrated in Fig.1). For ease of representation, in addition to the shared semantic attributes required in the SBSR task, other attributes in sketches (such as style, abstraction level, etc.) are collectively regarded as confounders, which currently lack a standard metric for measurement or comparison. This high degree of entanglement between confounders and semantic attributes makes it challenging to isolate the essential semantic information necessary for effective cross-domain retrieval. Furthermore, commercial SBSR models are often used by a diverse user base with varying levels of sketching skills and drawing abilities. This variability among users adds another layer of complexity, potentially limiting the generalizability and scalability of these models across different user groups. Disentangling the confounders to obtain the semantic information in sketch features is thus essential for enhancing the robustness and effectiveness of SBSR models, ensuring that they can better bridge the semantic gap between sketches and 3D shapes.\n\n# B. Contribution\n\nIn this paper, we propose a novel cross-domain semanticaligned network for SBSR. Specifically, our approach contains a sketch Causal Disentangled Learning Network (SCDL) and a 3D Shape Feature Extraction Network. In the SCDL network, we employ a tailored structural causal model (SCM) to separate sketches into several disentangled latent variables with swappable attributes that can be recombined to synthesize new samples. Furthermore, a mask-based causal intervention is designed to effectively identify semantic attributes and confounders in latent endogenous variable space under two variational autoencoder (VAE) frameworks. Then, in the 3D shape feature extraction network, we employ multi-view representation to characterize the 3D shapes to reduce crossmodal differences. Finally, for the separated semantic sketch features and 3D shape features, we develop the evidence lower bound specifically tailored for SCDL and the aligned joint loss paradigm. The contributions of this paper are as follows:\n\n• For the first time, we introduce causal representations into SBSR and construct a sketch causal disentangled learning (SCDL) based on a tailored structural causal model (SCM) to disentangle several latent variables, in which a novel mask-based causal intervention is designed to effectively identify semantic attributes and confounders.   \n• We propose a tailored learning strategy for the disentangled semantic sketch features and 3D shape features, in which the evidence lower bound and the aligned joint loss paradigm collaboratively construct a shared semantic embedding space to alleviate cross-domain discrepancies further.   \n• Extensive experiments conducted on three public datasets demonstrate that the proposed method obtains excellent retrieval performance compared with the state-of-the-art approaches.\n\nThe remainder of this article is organized as follows. Section II presents related works on sketch-based 3D shape retrieval, causal inference and disentangled learning. Section III provides the details of our approach. The corresponding experimental results and analysis are described in Section IV. Finally, we conclude this paper in Section VII.\n\n# II. RELATED WORK\n\n# A. Sketch-Based 3D Shape Retrieval\n\nEarly methods relied on handcrafted features for SBSR [1], [22], which were limited by the subjectivity and limitations of the extraction process, and gradually replaced by deep learning methods.\n\nThen, some methods were proposed by focusing on learning more effective representations of 3D shapes. Projection-based method MVCNN [23] was widely used to render a set of 2D views of a 3D shape. Lei et al. [24] proposed a Representative-View Selection (RVS) module to obtain several most representative views of a 3D shape. In [25], Chen et al. proposed a novel Segmented Stochastic-viewing Shape Network to learn effective 3D representations. Chen et al. [26] developed a unified attention framework to address both multi-view redundancy and single-view incompleteness.\n\nIn addition, some methods [27], [28], [29], [30], [31] focused on constructing the common embedding space so as to alleviate the cross-domain discrepancies. Wang et al. [32] first adopted Siamese Networks to learn the common embedding features of sketches and 3D shapes. Zhu et al. [33] proposed a pyramid cross-domain neural networks at multiple pyramid levels, establishing a many-to-one relationship between a 3D shape feature and sketch features extracted from different scales. Dai and Liang [34] adopted a teacher-student strategy to learn an aligned cross-modal feature space indirectly. Chen et al. [35] introduced a deep cross-modality adaptation model via semantics-preserving adversarial learning to enhance semantic correlations in local data batches across modalities. Bai et al. [36] proposed an auxiliary learning network to indirectly guide the master learning model to extract features of rich semantic information, so as to achieve a semantic alignment between the cross-modality data. Aside from the above methods, there are still few methods for exploring sketch attributes specifically. Most methods ignore the intrinsical noise in sketch data which may be the key factor for unsatisfactory retrieval performance.\n\n# B. Causal Inference\n\nIn recent years, researchers have integrated causal inference into computer vision and natural language processing [37], [38], enabling DNNs to learn causal effects and significantly improving performance in some areas such as image classification [39], [40], visual question answering [41], [42], and so on [43], [44], and [45]. Causal inference methods can be categorized into front-door and backdoor adjustment [46], [47], [48], [49], [50]. Backdoor adjustment stratifies confounders into different levels to deconfound. Wang et al. [51] proposed Visual Commonsense Region-based Convolutional Neural Network, which employed backdoor adjustment to improve visual feature representation learning. Zhang et al. [52] utilized backdoor adjustment to mitigate high likelihood of co-occurrence between visual and textual tokens, showing that reducing dataset bias can enhance generalization. Front-door adjustment aims to construct an additional mediator between cause and effect relationship to transmit knowledge. Yang et al. [47] used the front-door adjustment to deconfound in image captioning. Yang et al. [53] improved the attention mechanism with the front-door adjustment to remove spurious correlations.\n\nIn sketch data, confounders exist in styles, level of abstraction, graffiti strocks and so on. Due to the sparse sketch features can be easily interfered by these confounders, deconfounding is more challenging than other tasks.\n\n# C. Disentangled Learning\n\nThe disentangled learning is extensively employed to understand the observed data by disentangling the latent factors [54], [55], [56], [57], [58], [59]. The fundamental disentanglement paradigms has the variational auto-encoder [60] and generative adversarial networks (GAN) [61]. Starting from generic frameworks like combining autoencoders with adversarial training [62], this mix disentanglement paradigm has been successfully applied to multiple domains [63], [64], [65]. Wang et al. [66] proposed a approach for learning causal disentangled representations from interaction data in recommender systems, which considered the causal relationships between semantically related factors in real-world recommendation scenarios. Zhao et al. [67] analyzed the physical concepts affecting the generation of multimode traffic flow from the perspective of the observation generation principle. Li et al. [68] proposed a sequential variational auto-encoder with a reformulated objective function in medical prediction task.\n\nDisentangled learning is widely used and also applied in cross-modal retrieval tasks. Guo et al. [69] proposed the concept of cross-modal disentangled representation learning and design a modality-exclusive information expelling branch to diminish the side effects brought by the Modality-Shared Information. Qu et al. [70] developed unified and dynamic multimodal interaction framework towards image-text retrieval for the first time. Zhang et al. [71] explicitly exploited both the positive effect of matched fragments and the negative effect of mismatched fragments to jointly infer cross-modal similarity. There is also methods specifically focused on sketch disentanglement: Sain et al. [72] proposed cross-modal variational autoencoder (VAE) to explicitly disentangle each sketch into content representation and style representation. Chen et al. [73] proposed an asymmetrical disentanglement scheme to handle information asymmetry between the sketches and images. In general, disentangled methods of sketches [74], [75], [76] still remain relatively scarce. How to eliminate the interference of irrelevant attributes and accurately decouple true semantic attributes remains an unresolved challenge without a unified paradigm.\n\nTABLE I THE NOTATION TABLE OF IN THIS PAPER   \n\n<table><tr><td rowspan=1 colspan=1>Notation</td><td rowspan=1 colspan=1>Definition</td></tr><tr><td rowspan=1 colspan=1>{xk1,xk2}∈X</td><td rowspan=1 colspan=1>input sketches</td></tr><tr><td rowspan=1 colspan=1>{k1,k2}</td><td rowspan=1 colspan=1>distinguishing marks of input sketches</td></tr><tr><td rowspan=1 colspan=1>yey</td><td rowspan=1 colspan=1>category labels</td></tr><tr><td rowspan=1 colspan=1>{8k1,8k2]</td><td rowspan=1 colspan=1>independent exogenous variables</td></tr><tr><td rowspan=1 colspan=1>{z(k1,2）...,zk1,.2}</td><td rowspan=1 colspan=1>confounders of sketch</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>semantic attributes of sketch</td></tr><tr><td rowspan=1 colspan=1>{Zk1,Zk2}</td><td rowspan=1 colspan=1>causal representations</td></tr><tr><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>concatenate operation in pytorch</td></tr><tr><td rowspan=1 colspan=1>A</td><td rowspan=1 colspan=1>permutation matrix</td></tr><tr><td rowspan=1 colspan=1>（Zk1,Zk2)</td><td rowspan=1 colspan=1>reconstructed input</td></tr><tr><td rowspan=1 colspan=1>{xk1,xk1}</td><td rowspan=1 colspan=1>reconstructed output sketch</td></tr><tr><td rowspan=1 colspan=1>h(x)</td><td rowspan=1 colspan=1>encoder notation</td></tr><tr><td rowspan=1 colspan=1>f(z)</td><td rowspan=1 colspan=1>decoder notation</td></tr><tr><td rowspan=1 colspan=1>d1,12,1,4</td><td rowspan=1 colspan=1>weighting hyperparameters</td></tr><tr><td rowspan=1 colspan=1>Lrec</td><td rowspan=1 colspan=1>reconstruction loss</td></tr><tr><td rowspan=1 colspan=1>Lkl</td><td rowspan=1 colspan=1>KL divergence loss</td></tr><tr><td rowspan=1 colspan=1>DKL (II-)</td><td rowspan=1 colspan=1>Kullback-Leibler divergence</td></tr><tr><td rowspan=1 colspan=1>Ltcl</td><td rowspan=1 colspan=1>Triplet Center Loss</td></tr><tr><td rowspan=1 colspan=1>Dse()</td><td rowspan=1 colspan=1>squared Euclidean distance</td></tr><tr><td rowspan=1 colspan=1>f</td><td rowspan=1 colspan=1>semantic features of sketches and 3D shape</td></tr><tr><td rowspan=1 colspan=1>T</td><td rowspan=1 colspan=1>margin of Ltcl</td></tr><tr><td rowspan=1 colspan=1>Lam</td><td rowspan=1 colspan=1>AM-softmax Loss</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>scale factor</td></tr><tr><td rowspan=1 colspan=1>n</td><td rowspan=1 colspan=1>total number of categories</td></tr><tr><td rowspan=1 colspan=1>μ</td><td rowspan=1 colspan=1>margin of Lam</td></tr></table>\n\n# III. METHODOLOGY\n\nIn this section, we start with the definition of the sketch disentangled network via structural causal model (SCM). Then, we illustrate the cross-domain network for sketch-based 3D shape retrieval. Finally, we introduce the joint learning strategy. Table I provides definitions of the notations used in the text for reference.\n\n![](images/c9779a7d7fdfb3235a13d349ba5eb22894e3135b77c775310b581c44678c8331.jpg)  \nFig. 2. The directed acyclic graph (DAG) of the sketch branch in SCDL. The nodes are causal variables, and the edges represent the causal relationship. Especially, the dashed line indicates the proposed causal intervention, where a mask layer makes intervention possible.\n\n# A. Constuct Causal Representations Through SCMs\n\nIn the sketch branch, we redefine the sketch semantic representation from a causal perspective within the framework of VAE module. The Directed Acyclic Graph (DAG) is illustrated in Fig.2. Specifically, in inference process, we let the pair $\\{ \\pmb { x } _ { k 1 } , \\pmb { x } _ { k 2 } \\} \\in \\mathcal { X }$ represent input sketches and $\\pmb { y } \\in \\mathcal { V }$ represent their corresponding category labels. Then $\\{ \\pmb { x } _ { k 1 } , \\pmb { x } _ { k 2 } \\}$ are taken to generate the independent exogenous variables:\n\n$$\n\\mathcal { E } _ { k 1 } : = \\{ \\epsilon _ { k 1 } ^ { 1 } , . . . , \\epsilon _ { k 1 } ^ { l - 1 } , \\epsilon _ { k 1 } ^ { l } \\} , \\quad \\mathcal { E } _ { k 2 } : = \\{ \\epsilon _ { k 2 } ^ { 1 } , . . . , \\epsilon _ { k 2 } ^ { l - 1 } , \\epsilon _ { k 2 } ^ { l } \\} .\n$$\n\nwhere the prior distribution is assumed to be standard Multivariate Gaussian. Subsequently, it is transformed by the Causal Layer into causal representations. For our goal is to learn the bond (the pure semantic properties) between cross-domain retrievals from these disentangled representations, we represent obtained causal representations with two distinct symbols, where task, a {z1,c{k1,k2}, . . . $\\{ z _ { \\{ k 1 , k 2 \\} } ^ { 1 , c } , \\ldots , z _ { \\{ k 1 , k 2 \\} } ^ { l - 1 , c } \\}$ denotes those unassociated with the associated semantic represen$\\{ z _ { k 1 } ^ { l , s } , z _ { k 2 } ^ { l , s } \\}$   \ntation in task. Consistent with our objectives, the task-related variable $\\pmb { S }$ governs the prospective classification $\\textbf {  { y } }$ . Then in generation process, we adopt an causal intervention to swap the original semantic attributes. Subsequently leveraging the post-intervention representation with other unassociated representations to reconstruct $\\{ \\tilde { { \\pmb { x } } } _ { k 1 } , \\tilde { { \\pmb { x } } } _ { k 2 } \\}$ .\n\nBased on the above description, we give a formal definition via SCMs, where we define parent node of variables $\\pmb { v }$ as $p a ( \\pmb { v } )$ . Then the probability associated with $\\pmb { v }$ can be expressed as $p \\left( \\pmb { v } \\mid p a ( \\pmb { v } ) , \\epsilon _ { v } \\right)$ . Hence, the observed variables of pair causal relationships in SCM can be articulated as follows:\n\n$$\n\\begin{array} { r l r } & { \\mathbf { x } _ { k 1 } \\sim p \\left( \\mathbf { x } _ { k 1 } \\mid \\epsilon _ { k 1 } ^ { 1 } , \\dots , \\epsilon _ { k 1 } ^ { l - 1 } , \\epsilon _ { k 1 } ^ { l } \\right) , \\mathbf { y } \\sim p \\left( \\mathbf { y } \\mid z _ { k 1 } ^ { l , s } , \\epsilon _ { k 1 } ^ { l } \\right) , } & \\\\ & { \\widetilde { \\mathbf { x } } _ { k 1 } \\sim p \\left( \\widetilde { \\mathbf { x } } _ { k 1 } \\mid z _ { k 1 } ^ { 1 , c } , \\dots , z _ { k 1 } ^ { l - 1 , c } , \\widetilde { z } _ { k 2 } ^ { l , s } \\right) . } & \\\\ & { \\mathbf { x } _ { k 2 } \\sim p \\left( \\mathbf { x } _ { k 2 } \\mid \\epsilon _ { k 2 } ^ { 1 } , \\dots , \\epsilon _ { k 2 } ^ { l - 1 } , \\epsilon _ { k 2 } ^ { l } \\right) , \\mathbf { y } \\sim p \\left( \\mathbf { y } \\mid z _ { k 2 } ^ { l , s } , \\epsilon _ { k 2 } ^ { l } \\right) , } & \\\\ & { \\widetilde { \\mathbf { x } } _ { k 2 } \\sim p \\left( \\widetilde { \\mathbf { x } } _ { k 1 } \\mid z _ { k 2 } ^ { 1 , c } , \\dots , z _ { k 2 } ^ { l - 1 , c } , \\widetilde { z } _ { k 1 } ^ { l , s } \\right) . } & { \\mathrm { ( ) } } \\end{array}\n$$\n\nAnd the unobserved variables are defined as follows:\n\n$$\n\\begin{array} { r } { \\begin{array} { r } { z _ { k 1 } ^ { 1 , c } \\sim p \\left( z _ { k 1 } ^ { 1 , c } \\mid x _ { k 1 } , \\epsilon _ { k 1 } ^ { 1 } \\right) , \\dots , z _ { k 1 } ^ { l - 1 , c } \\sim p \\left( z _ { k 1 } ^ { l - 1 , c } \\mid x _ { k 1 } , \\epsilon _ { k 1 } ^ { l - 1 } \\right) , } \\\\ { z _ { k 1 } ^ { l , s } \\sim p \\left( z _ { k 1 } ^ { l , s } \\mid x _ { k 1 } , \\epsilon _ { k 1 } ^ { l } \\right) . } \\end{array} } \\end{array}\n$$\n\n$$\n\\begin{array} { r } { \\begin{array} { r } { z _ { k 2 } ^ { 1 , c } \\sim p \\left( z _ { k 2 } ^ { 1 , c } \\mid x _ { k 2 } , \\epsilon _ { k 2 } ^ { 1 } \\right) , \\dots , z _ { k 2 } ^ { l - 1 , c } \\sim p \\left( z _ { k 2 } ^ { l - 1 , c } \\mid x _ { k 2 } , \\epsilon _ { k 2 } ^ { l - 1 } \\right) , } \\\\ { z _ { k 2 } ^ { l , s } \\sim p \\left( z _ { k 2 } ^ { l , s } \\mid x _ { k 2 } , \\epsilon _ { k 2 } ^ { l } \\right) . \\qquad ( 5 ) } \\end{array} } \\end{array}\n$$\n\nOnce the causal representations are obtained, we design a Mask Layer [77] to reconstruct itself named causal intervention. The purpose of this is to validate whether the hypothesized zl,s{k1,k2} truly represents the semantic attribute. And this step depicts how children are generated by their corresponding parental variables. Specifically, the designed causal intervention are operated under two VAE structures with shared parameters. We define the causal representations $\\{ Z _ { k 1 } , Z _ { k 2 } \\}$ of the pair input $\\{ \\pmb { x } _ { k 1 } , \\pmb { x } _ { k 2 } \\}$ as follows:\n\n$$\nZ _ { k 1 } = \\{ z _ { k 1 } ^ { c , 1 } , \\ldots , z _ { k 1 } ^ { c , l - 1 } , z _ { k 1 } ^ { s , l } \\} , \\quad Z _ { k 2 } = \\{ z _ { k 2 } ^ { c , 1 } , \\ldots , z _ { k 2 } ^ { c , l - 1 } , z _ { k 2 } ^ { s , l } \\} .\n$$\n\nThen we define the associated weights matrix as $\\textbf { A } \\in \\mathbb { R } ^ { l }$ , where $\\mathbf { A }$ is the weight vector that implements the causal intervention between two causal features. Hence, the causal intervention can be represented in the following form:\n\n$$\n\\left( \\widetilde { Z } _ { k 1 } , \\widetilde { Z } _ { k 2 } \\right) = \\left( \\boldsymbol { \\psi } \\{ Z _ { k 1 } , Z _ { k 2 } \\} \\right) ^ { T } \\times \\mathbf { A }\n$$\n\nwhere $\\psi$ represents the Concatenate operation in pytorch, the permutation matrix $\\mathbf { A }$ is utilized for the purpose of swapping the semantic properties $\\left( z _ { k 1 } ^ { l , s } , z _ { k 2 } ^ { l , s } \\right)$ of corresponding positions in the two representations. This layer makes intervention or “do-operation” possible. Intervention in causality refers to modifying a certain part of a system by external forces and one is interested in the outcome of such manipulation. After this layer, the reconstructed input $\\{ \\widetilde { Z } _ { k 1 } , \\widetilde { Z } _ { k 2 } \\}$ can be defined as follows:\n\n$$\n\\widetilde { Z } _ { k 1 } = \\{ z _ { k 1 } ^ { c , 1 } , \\ldots , z _ { k 1 } ^ { c , l - 1 } , z _ { k 2 } ^ { s , l } \\} , \\quad \\widetilde { Z } _ { k 2 } = \\{ z _ { k 2 } ^ { c , 1 } , \\ldots , z _ { k 2 } ^ { c , l - 1 } , z _ { k 1 } ^ { s , l } \\} .\n$$\n\nSince the semantic attributes of the sample in the same category are completely consistent, the noise confounders may be different. Through the above causal intervention, if output images generated using reconstructed input $\\left( \\widetilde { Z } _ { k 1 } , \\widetilde { Z } _ { k 2 } \\right)$ are the same as inputs, it proves that we have indeed found the pure semantic properties. On the contrary, if the output images generated are different from the inputs, it means that the interaction term at this time is still a noise confounder. Besides exploring semantic properties, we further enable the model to support intervention in the causal system to generate counterfactual data, which does not exist in the training data; details are shown in section IV-E.1.\n\n# B. Cross-Domain Semantic-Aligned Network for Sketch-Based 3D Shape Retrieval\n\n1) Sketch Causal Disentangled Learning Network: To guarantee effective disentanglement of the causal latent variables, our disentanglement model is built upon two interactive VAE frameworks [60] for both inference and generation in the sketch branch Fig.3 (a). The original VAE model produces a latent representation by optimizing the variational lower bound on log-likelihood of the data, defined as follows:\n\n$$\n\\begin{array} { r } { \\log p ( \\pmb { x } ) \\geq \\mathbb { E } _ { z \\sim q ( z \\mid \\pmb { x } ) } [ \\log p ( \\pmb { x } \\mid z ) ] - \\mathcal { D } _ { \\mathrm { K L } } ( q ( z \\mid \\pmb { x } ) \\| p ( z ) ) . } \\end{array}\n$$\n\n![](images/df9ba2951d2b39f23379e166e3b08584272c13c122b96b3f8abf43ae1d1119be.jpg)  \nFig. 3. The overall framework of the cross-domain semantic-aligned network for sketch-based 3D shape retrieval. (a) represents the cross-domain semantic-aligned network for sketch-based 3D shape retrieval; (b) represents the 3D shape deature extraction network; (c) represents the learning strategy.\n\nwhere $\\mathcal { D } _ { \\mathrm { K L } } \\left( \\cdot \\| \\cdot \\right)$ represents the Kullback-Leibler (KL) divergence, $q ( \\boldsymbol { z } | \\boldsymbol { x } )$ and $p ( \\pmb { x } | \\pmb { z } )$ are the conditional probability distributions of the encoder and decoder respectively. The distribution $p ( z )$ is the prior on the latent space, modeled as $\\mathcal { N } \\sim ( 0 , 1 )$ . The encoder returns mean $\\mu$ and variance $\\sigma ^ { 2 }$ of a normal distribution, such that $z \\sim \\mathcal { N } ( \\mu , \\sigma ^ { 2 } )$ , and the decoder utilizes the mean and variance to reconstruct the input image, both parameterized by neural networks.\n\nAccording to above formulas, we give a probabilistic formulation of the proposed sketch inference model. Specifically, we treat both $\\mathbf { z }$ and $\\epsilon$ as latent variables, letting $\\mathbf { h } \\left( x \\right)$ denotes the encoder. Then inference models can be defined as follows:\n\n$$\n\\begin{array} { r } { q _ { \\phi } ( Z _ { k 1 } , \\mathcal { E } _ { k 1 } | \\pmb { x } _ { k 1 } ) \\equiv q ( Z _ { k 1 } | \\mathcal { E } _ { k 1 } ) q _ { \\zeta } ( \\mathcal { E } _ { k 1 } - \\mathbf { h } ( \\pmb { x } _ { k 1 } ) ) . } \\\\ { q _ { \\phi } ( Z _ { k 2 } , \\mathcal { E } _ { k 2 } | \\pmb { x } _ { k 2 } ) \\equiv q ( Z _ { k 2 } | \\mathcal { E } _ { k 2 } ) q _ { \\zeta } ( \\mathcal { E } _ { k 2 } - \\mathbf { h } ( \\pmb { x } _ { k 2 } ) ) . } \\end{array}\n$$\n\nwhich are obtained by assuming the following encoding processes:\n\n$$\n\\begin{array} { r } { \\mathcal { E } _ { k 1 } = \\mathbf { h } ( \\pmb { x } _ { k 1 } ) + \\boldsymbol { \\zeta } , \\quad \\mathcal { E } _ { k 2 } = \\mathbf { h } ( \\pmb { x } _ { k 2 } ) + \\boldsymbol { \\zeta } . } \\end{array}\n$$\n\nwhere $\\phi$ is the parameter of distribution $q , \\zeta$ is the vectors of independent noise with probability density $q _ { \\zeta }$ . Specifically, the entire process of causal inference can be can be simplified and represented as:\n\n$$\n\\begin{array} { r } { q _ { \\phi } ( Z _ { k 1 } \\vert \\pmb { x } _ { k 1 } ) = \\mathcal { F } _ { R E L U } ( \\mathcal { F } _ { M L P } ( \\pmb { x } _ { k 1 } ) ) \\ast 3 . } \\\\ { q _ { \\phi } ( Z _ { k 2 } \\vert \\pmb { x } _ { k 2 } ) = \\mathcal { F } _ { R E L U } ( \\mathcal { F } _ { M L P } ( \\pmb { x } _ { k 2 } ) ) \\ast 3 . } \\end{array}\n$$\n\nThen, we use the Reparameterization Trick to enable the gradient propagation over the learned distribution.\n\nIn the generation process, we let $\\mathbf { f } \\left( z \\right)$ denotes the decoder. Hence, the probabilistic formulation can be represented as follows:\n\n$$\n\\begin{array} { r l } & { p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 1 } , \\pmb { y } | \\widetilde { Z } _ { k 1 } , \\mathcal { E } _ { k 1 } ) = p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 1 } | \\widetilde { Z } _ { k 1 } ) \\equiv p _ { \\xi } ( \\pmb { x } _ { k 1 } - \\pmb { \\operatorname { f } } ( \\widetilde { Z } _ { k 1 } ) ) . } \\\\ & { p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 2 } , \\pmb { y } | \\widetilde { Z } _ { k 2 } , \\mathcal { E } _ { k 2 } ) = p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 2 } | \\widetilde { Z } _ { k 2 } ) \\equiv p _ { \\xi } ( \\pmb { x } _ { k 2 } - \\pmb { \\operatorname { f } } ( \\widetilde { Z } _ { k 2 } ) ) . } \\end{array}\n$$\n\nwhich are obtained by assuming the following decoding processes:\n\n$$\n\\widetilde { \\pmb { x } } _ { k 1 } = \\mathbf { f } ( \\widetilde { Z } _ { k 1 } ) + \\pmb { \\xi } , \\quad \\widetilde { \\pmb { x } } _ { k 2 } = \\mathbf { f } ( \\widetilde { Z } _ { k 2 } ) + \\pmb { \\xi } .\n$$\n\nwhere $\\theta$ is the parameter of distribution $\\mathbf { \\omega } _ { p , \\ \\xi }$ is the vectors of independent noise with probability density $p _ { \\xi }$ . Similarly to the causal inference, the entire process of generation can be can be simplified and represented as:\n\n$$\n\\begin{array} { r } { p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 1 } | \\widetilde { Z } _ { k 1 } ) = \\mathcal { F } _ { R E L U } ( \\mathcal { F } _ { M L P } ( \\widetilde { Z } _ { k 1 } ) ) * 3 . } \\\\ { p _ { \\theta } ( \\widetilde { \\pmb { x } } _ { k 2 } | \\widetilde { Z } _ { k 2 } ) = \\mathcal { F } _ { R E L U } ( \\mathcal { F } _ { M L P } ( \\widetilde { Z } _ { k 2 } ) ) * 3 . } \\end{array}\n$$\n\nWhen $\\zeta$ and $\\xi$ are infinitesimal, the encoder and decoder can be regarded as deterministic ones.\n\n2) $3 D$ Shape Feature Extraction Network: Following the previous methods [23], [18], [19] we adopt the widely used multi-view representation for 3D shapes. That is, a 3D shape is rendered to $V$ views by placing $V$ virtual cameras around the 3D shape evenly $V = 1 2$ in this paper). Subsequently, the multiple views pass through the backbone network, all branches share the same parameters. An average pooling layer is used to fuse the final 3D shape features output.\n\n# C. Learning Strategy\n\nUnlike the traditional formulation, the proposed SCDL requires considering at least two branches that utilize intervened interactive causal representations. Given dataset $\\mathcal { X }$ with the empirical data distribution $q _ { \\mathcal { X } } ( \\pmb { x } )$ , the parameters $\\theta$ and $\\phi$ are learned by optimizing the following evidence lower bound (ELBO):\n\n$$\n\\begin{array} { r l } & { \\mathbb { E } _ { q _ { \\mathcal { X } } } \\left[ \\log p _ { \\theta } ( \\pmb { x } _ { o } ) \\right] } \\\\ & { \\quad \\ge \\mathrm { E L B O } = \\mathbb { E } _ { q _ { \\mathcal { X } } } \\left[ \\mathbb { E } _ { \\epsilon , z \\sim q _ { \\phi } } \\left[ \\log p _ { \\theta } ( \\pmb { x } _ { o } , \\pmb { y } \\mid \\mathsf { z } _ { o } , \\pmb { \\epsilon } ) \\right] \\right. } \\\\ & { \\quad \\quad \\left. - { \\mathcal { D } } _ { K L } \\left( q _ { \\phi } ( \\epsilon , \\mathsf { z } _ { i } \\mid \\pmb { x } _ { i } ) \\| p _ { \\theta } ( \\epsilon , \\mathsf { z } _ { o } ) \\right) \\right] . } \\end{array}\n$$\n\nwhere $\\mathbf { \\Delta } _ { \\mathbf { { \\boldsymbol { x } } } _ { i } } , \\mathbf { \\Delta } _ { \\mathbf { { \\boldsymbol { x } } } _ { o } }$ represents sketch images from input and output, respectively. And the correspondences between variables are as follows:\n\n$$\n\\begin{array} { r l } & { { \\pmb x } _ { i } = { \\pmb x } _ { k 1 } \\Longrightarrow { \\pmb x } _ { o } = \\widetilde { { \\pmb x } } _ { k 1 } , { \\tau } _ { i } \\in Z _ { k 1 } , z _ { o } \\in \\widetilde { Z } _ { k 1 } ; } \\\\ & { { \\pmb x } _ { i } = { \\pmb x } _ { k 2 } \\Longrightarrow { \\pmb x } _ { o } = \\widetilde { { \\pmb x } } _ { k 2 } , { z } _ { i } \\in Z _ { k 2 } , { z } _ { o } \\in \\widetilde { Z } _ { k 2 } . } \\end{array}\n$$\n\nIt allows us to decompose sketch inputs into a disentangled representation with swappable components, that can be recombined to synthesize new samples. Our purpose is to disentangle the sketch high-dimensional content into confounders and semantic parts. Such VAE models are trained by optimizing the sum of reconstruction $( \\mathcal { L } _ { r e c } )$ and KL divergence $( \\mathcal { L } _ { k l } )$ losses via gradient descent, defined as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { r e c } ( \\phi , \\theta ) = - \\mathbb { E } _ { q ( \\boldsymbol { z } _ { i } | \\boldsymbol { x } _ { i } ) } \\left[ \\log p _ { \\theta } ( \\boldsymbol { x } _ { o } \\mid \\boldsymbol { z } _ { o } ) \\right] . } \\\\ { \\mathcal { L } _ { k l } = \\mathcal { D } _ { \\mathrm { K L } } \\left[ q _ { \\phi } ( \\boldsymbol { z } _ { i } \\mid \\boldsymbol { x } _ { i } ) \\lVert p _ { \\theta } ( \\boldsymbol { z } _ { o } ) \\right] . } \\end{array}\n$$\n\nThe corresponding relationship between the variables is the same as in Formula 21. The prior over latent variables is a centered isotropic multivariate Gaussian, $p _ { \\theta } ( \\pmb { x } _ { o } ) = \\mathcal { N } \\sim$ $( { \\pmb x } _ { o } ; 0 , { \\pmb x } _ { i } )$ . In practice, we simplify:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { r e c } = \\| \\pmb { x } _ { i } - \\pmb { x } _ { o } \\| _ { 2 } ^ { 2 } . } \\end{array}\n$$\n\nBesides sketch-disentangled learning, we also perform cross-modality manifold which ensures aligning semantic features of sketches and 3D shapes. More specifically, we train the semantic attributes with a Triplet Center Loss [78] as follows:\n\n$$\n\\mathcal { L } _ { t c l } = \\mathrm { m a x } \\left. 0 , \\tau + \\mathcal { D } _ { s e } \\left( \\mathbf { f } ^ { * } , \\delta _ { y _ { j } } \\right) - \\mathrm { m i n } _ { g \\neq j } \\mathcal { D } _ { s e } \\left( \\mathbf { f } ^ { * } , \\delta _ { y _ { g } } \\right) \\right. .\n$$\n\nwhere $\\mathcal { D } _ { s e } ( \\cdot )$ represents the squared Euclidean distance funcn. For each triple input represents corresponding $\\{ \\pmb { x } _ { k 1 } , \\pmb { x } _ { k 2 } , \\pmb { x } _ { 3 D } \\}$ ts $\\mathbf { f } ^ { * }$ $\\{ z _ { k 1 } ^ { s , l } , z _ { k 2 } ^ { s , l } , z _ { 3 D } \\}$ Intuitively, $\\mathcal { L } _ { t c l }$ is to enforce the distances between the samples and their corresponding center $\\delta _ { y _ { j } }$ (positive center) smaller than the distances between the samples and their nearest negative center $\\delta _ { y _ { g } }$ by a margin $\\pmb { \\tau }$ . Then, the classification loss $\\mathcal { L } _ { a m }$ is defined by the AM-softmax Loss [79]:\n\n$$\n\\mathcal { L } _ { a m } = l o g \\frac { e ^ { \\gamma \\left( \\overline { { \\delta } } _ { y _ { j } } \\overline { { \\mathbf { f } ^ { * } } } - \\mu \\right) } } { e ^ { \\gamma \\left( \\overline { { \\delta } } _ { y _ { j } } \\overline { { \\mathbf { f } ^ { * } } } - \\mu \\right) } + \\sum _ { g = 1 , g \\neq j } ^ { n } e ^ { \\gamma \\left( \\overline { { \\delta } } _ { y _ { g } } \\overline { { \\mathbf { f } ^ { * } } } \\right) } } .\n$$\n\nwhere $\\gamma$ is a scale factor that compensates for the normalization of feature and weight, $\\pmb { n }$ is the total number of categories, $\\pmb { \\mu }$ is the margin, $\\begin{array} { r } { \\overline { { \\delta } } _ { y _ { j } } = \\frac { \\delta _ { y _ { j } } } { \\| \\delta _ { y _ { j } } \\| _ { 2 } } } \\\\ { . } \\end{array}$ and $\\begin{array} { r } { \\overline { { \\mathbf { f } ^ { * } } } = \\frac { \\mathbf { f } ^ { * } } { \\Vert \\mathbf { f } ^ { * } \\Vert _ { 2 } } } \\end{array}$ are the normalized version. Conclusively, our comprehensive optimization objective is formulated as:\n\n$$\n\\mathcal { L } = \\lambda _ { 1 } \\mathcal { L } _ { r e c } + \\lambda _ { 2 } \\mathcal { L } _ { k l } + \\lambda _ { 3 } \\mathcal { L } _ { t c l } + \\lambda _ { 4 } \\mathcal { L } _ { a m }\n$$\n\nwhere $\\lambda _ { 1 } , \\lambda _ { 2 } , \\lambda _ { 3 } , \\lambda _ { 4 }$ are weighting hyperparameters.\n\n# IV. EXPERIMENTS\n\n# A. Datasets\n\nFollowing previous literature, we use the three widely used benchmark datasets and experimental settings, including artificial 3D shapes of the engineering field and natural 3D shapes existing in the real world. Specific details of these datasets are shown in Table II, and Fig.4 shows some samples from the two datasets.\n\nTABLE II DETAILS OF DATASETS   \n\n<table><tr><td></td><td>SHREC&#x27;13</td><td>SHREC&#x27;14</td><td>PART-SHREC&#x27;14</td></tr><tr><td>Class</td><td>90</td><td>171</td><td>48</td></tr><tr><td>Sketch</td><td>7200</td><td>13680</td><td>3840</td></tr><tr><td>train/test</td><td>50/301 perclass</td><td>50/30 per class</td><td>50/30 per class</td></tr><tr><td>3D Shape</td><td>1258</td><td>8987</td><td>7238</td></tr><tr><td>train/test</td><td>1258/1258</td><td>8987/8987</td><td>5812/1426</td></tr></table>\n\n![](images/23f0f3d87a39997bee79c08743223189103149650255d6d147bfe9beb178c72a.jpg)  \nFig. 4. Samples from two benchmarks: (a) the SHREC’13 dataset, (b) the SHREC’14 dataset. Images in the first three columns are sketches, while images in the last three columns are 3D shapes. Samples in the same row belong to the same class.\n\nSHREC’13 [6], [80] is a large-scale dataset for sketch-based 3D shape retrieval. This dataset consists of 7,200 sketches and 1,258 shapes from 90 classes, by collecting humandrawn sketches [81] and 3D shapes from the Princeton Shape Benchmark (PSB) [2] that share common categories. For each class, there are totally 80 sketches, where 50 images are used for training and 30 images for test. The number of 3D shapes in different categories varies from 4 to 184, about 14 on average.\n\nSHREC’14 [7], [82] is a sketch track benchmark larger than SHREC 2013. It totally contains 13,680 sketches and 8,987 3D shapes, grouped into 171 classes. The 3D shapes are collected from various datasets, including SHREC 2012 [83] and the Toyohashi Shape Benchmark (TSB) [84]. Similar to SHREC 2013, there are 80 images for sketches, and about $5 3 3 \\mathrm { D }$ shapes on average for each class. The sketches are further split into 8,550 training data and 5,130 test data, where for each class, 50 images are used for training and the rest 30 images for test. The number of 3D shapes in different categories varies from 1 to 632, about 53 on average.\n\nPART-SHREC’14 is collected from SHREC’14, which selects 48 classes that have more than 50 shapes in SHREC’14, totally 7238 3D shapes and 3840 sketches. The 3D shapes are randomly split into a training set of 5812 samples and a test set of 1426 samples.\n\n# B. Implementation Details\n\nThe hardware environment for the experiment is Intel (R) i7 CPU $^ +$ NVIDIA 4090 GPU-24GB, and the software environment is Ubuntu $+$ CUDA $1 0 . 1 +$ Pythorch 1.7.1+Python 3.7.10.\n\nData Prepossessing: In experiments, each 3D shape is rendered into 12 gray view images around the $\\mathbf { Z }$ -axis every 30◦ using the rendering scheme. After that, all view images and sketches are uniformly resized into the resolution of $2 2 4 \\times 2 2 4 \\times 3$ , then random horizontally flipped with a 0.5 probability and normalized by subtracting the ImageNet mean [85].\n\nArchitecture settings: For backbone networks, we adopt the ResNet-50 [86] or the Inception-ResNet-v2 (I-R-v2) [87] pretrained on ImageNet, by removing the last fully connected layer. Specifically, for each 3D shape, 12 view features are extracted first through 12 weighted shared backbone networks, and then a max-pooling layer is added before ‘conv5_1’ in the ResNet-50 and $\\cdot _ { 1 0 \\times }$ Inception-ResNet-C’ in the InceptionResNet-v2 to fuse 12 view features. The encoder and decoder are both composed of three MLPs for [2048, 1024, 512] in ResNet-50 and [4096, 2048, 512] in I-R-v2.\n\nParameter Settings: During training, an end-to-end joint optimization strategy is adopted with the SGD optimizer, whose initial learning rate is $1 0 ^ { - 3 }$ , momentum is 0.9, and decay rate is $1 0 ^ { - 4 }$ . The mini-batch size is set to 16 by the GPU memory size, the maximum epoch number is set to 100, and after each 30 epochs $l r = l r \\times 1 0 ^ { - 1 }$ . The hyperparameters $\\lambda _ { 1 } , \\lambda _ { 2 } , \\lambda _ { 3 } , \\lambda$ $\\lambda _ { 4 }$ in Eq.27 are set to 1, the scale factor $\\gamma$ is set to 15, the margin $\\pmb { \\tau }$ and $\\pmb { \\mu }$ are set to 0.35.\n\n# C. Evaluation Metrics\n\nWe compared our proposed method with the state-of-the-art methods using several widely-adopted metrics for sketchbased 3D shape retrieval: nearest neighbor (NN), first tier (FT), second tier (ST), E-measure (E), discounted cumulated gain (DCG), and mean average precision (mAP). In addition, precision-recall curves is also provided for visually evaluating the performance.\n\n# D. Parameters Setting\n\nThere are four hyperparameters $( \\lambda _ { 1 , 2 , 3 , 4 }$ in Eq.27) in the loss function of our approach. To investigate the effect of these hyperparameters and further discuss the robustness of the method, we conduct experiments on two benchmarks. Fig. 5 shows the results of hyperparameters on SHREC’13 and SHREC’14, where the range of values on the horizontal axis is derived from commonly used empirical settings, while the vertical axis represents the corresponding comprehensive evaluation metric (mAP) values. In the following experiments, we modify only one hyperparameter in each experiment and keep the others fixed.\n\nAs illustrated in Fig. 5: (1) In general, as the hyperparameters increase from 0.1, model performance shows a rising trend, with peak effectiveness observed around values near 1. Beyond this range, additional increments yield limited improvement. (2) Further analysis reveals that $\\lambda _ { 1 }$ and $\\lambda _ { 4 }$ exert a more pronounced influence on mAP, with the difference between their best and lowest results reaching $6 . 4 \\%$ and $2 0 . 8 \\%$ , respectively. This outcome likely arises from the pivotal role of $L _ { m e s }$ in feature disentanglement and the dominant contribution of $L _ { a m }$ to cross-domain feature alignment. (3) In contrast, $\\lambda _ { 2 }$ and $\\lambda _ { 3 }$ appear to have a subtler impact on overall performance, mainly providing fine-tuning adjustments. (4) Across both datasets, although the SHREC’14 dataset displays slightly larger variations, model performance stabilizes in the range above 1.0, underscoring the robustness of the proposed approach across diverse datasets. Based on the above analysis, we set these four hyperparameters to 1 in the subsequent experiments.\n\n![](images/53d6d3d96a1b330be39498d48a5fcfda7399caebac507c0f2de3dbe7d85da7e7.jpg)  \nFig. 5. Analysis of the effect $( \\% )$ of different hyperparameter values, with the blue line representing the results on SHREC’13 and the red line representing the results on SHREC’14.\n\n# E. Comparisons With the State-of-the-Art Methods\n\nWe compare our SCDL with the state-of-the-art methods for SBSR, including Siamese [32], DCML [12], DCHML [13], TCL [78], Semantic [88], LWBR [29], DCA [35], DPSML [24], CGN [34], Shape2Vec [89], DSSH [25], HEAR [26], SUL [18] and $\\mathrm { H D A } ^ { 2 } \\mathrm { L }$ [19]. Most methods use ResNet-50 and Inception-ResNet-v2 (I-R-v2) as base networks; for a fair comparison, we adopt both as backbone networks.\n\n1) Retrieval on SHREC’13 Dataset: Table III shows the results of all performance metrics on SHREC’13. From those results, several conclusions and discussions are as follows.\n\n(1) Our SCDL achieves state-of-the-art performance on SHREC’13 for six valuation metrics. For example, when using ResNet-50 as the backbone network, in terms of the comprehensive evaluation metric mAP, our SCDL outperforms $\\mathrm { H D A } ^ { 2 } \\mathrm { L }$ , CGN and SUL by $0 . 4 \\%$ , $1 . 2 \\%$ , and $2 \\%$ , respectively. When utilizing I-R-V2 as the backbone network, SCDL establishes the highest mAP of $8 8 . 8 \\%$ , demonstrating the evident improvments to other methods. Furthermore, for other evaluation metrics, SCDL consistently achieves optimal results. These findings indicate the effectiveness of the proposed approach.\n\n(2) Results of SCDL are positively correlated with the performance of the backbone networks. When employing the I-R-V2 backbone network, SCDL achieves $8 4 . 9 \\%$ , $8 6 . 7 \\%$ , $9 1 . 3 \\%$ , $4 3 . 2 \\%$ , $9 1 . 5 \\%$ , and $8 8 . 8 \\%$ , higher results than when using the CCC network across all six evaluation metrics for NN, FT, ST, E, DCG, and mAP, respectively. These results indicate that our SCDL is not confined to a specific network; rather, it may achieve superior outcomes when utilizing a more advanced foundational network, showing the robust scalability of causal disentangled learning.\n\nTABLE III THE PERFORMANCE $( \\% )$ ON SHREC’13 DATASET. FOR EACH METRIC, THE BEST RESULT IS IN BOLD, AND THE SECOND-BEST RESULT IS UNDERLINED   \n\n<table><tr><td>Method</td><td>Backbone</td><td>NN</td><td>FT</td><td>ST</td><td>E</td><td>DCG</td><td>mAP</td></tr><tr><td>SBR-VC [6]</td><td>-</td><td>16.4</td><td>9.7</td><td>14.9</td><td>8.5</td><td>34.8</td><td>11.6</td></tr><tr><td>CDMR [27]</td><td>-</td><td>27.9</td><td>20.3</td><td>29.6</td><td>16.6</td><td>45.8</td><td>25.0</td></tr><tr><td>Siamese [32]</td><td>3-Layers</td><td>40.5</td><td>40.3</td><td>54.8</td><td>28.7</td><td>60.7</td><td>46.9</td></tr><tr><td>DCML[12]</td><td>AlexNet</td><td>65.0</td><td>63.4</td><td>71.9</td><td>34.8</td><td>76.6</td><td>67.4</td></tr><tr><td>DCHML [13]</td><td>AlexNet</td><td>73.0</td><td>71.5</td><td>77.3</td><td>36.8</td><td>81.6</td><td>74.4</td></tr><tr><td>TCL[78]</td><td>AlexNet</td><td>76.3</td><td>78.7</td><td>84.9</td><td>39.2</td><td>85.4</td><td>80.7</td></tr><tr><td>Semantic [88]</td><td>PointNet</td><td>82.3</td><td>82.8</td><td>86.0</td><td>40.3</td><td>88.4</td><td>84.3</td></tr><tr><td>LWBR [29]</td><td>ResNet-50</td><td>73.5</td><td>74.5</td><td>78.4</td><td>35.9</td><td>82.5</td><td>76.7</td></tr><tr><td>DCA [35]</td><td>ResNet-50</td><td>78.3</td><td>79.6</td><td>82.9</td><td>37.6</td><td>85.6</td><td>81.3</td></tr><tr><td>DPSML [24]</td><td>ResNet-50</td><td>81.9</td><td>83.4</td><td>87.5</td><td>41.5</td><td>89.2</td><td>85.7</td></tr><tr><td>CGN [34]</td><td>ResNet-50</td><td>83.2</td><td>85.3</td><td>90.2</td><td>41.9</td><td>90.1</td><td>87.0</td></tr><tr><td>DSSH [25]</td><td>ResNet-50</td><td>79.9</td><td>81.4</td><td>86.0</td><td>40.4</td><td>87.3</td><td>83.1</td></tr><tr><td>HEAR [26]</td><td>ResNet-50</td><td>82.1</td><td>83.7</td><td>87.8</td><td>40.9</td><td>88.8</td><td>85.4</td></tr><tr><td>SUL[18]</td><td>ResNet-50</td><td>82.4</td><td>84.3</td><td>89.3</td><td>41.7</td><td>89.6</td><td>86.2</td></tr><tr><td>HDA²L [19]</td><td>ResNet-50</td><td>82.5</td><td>84.3</td><td>90.5</td><td>42.0</td><td>90.3</td><td>87.8</td></tr><tr><td>SCDL</td><td>ResNet-50</td><td>84.5</td><td>86.1</td><td>90.8</td><td>42.2</td><td>90.7</td><td>88.2</td></tr><tr><td>DSSH [25]</td><td>I-R-v2</td><td>83.1</td><td>84.4</td><td>88.6</td><td>41.1</td><td>89.3</td><td>85.8</td></tr><tr><td>HEAR [26]</td><td>I-R-v2</td><td>84.2</td><td>85.6</td><td>88.8</td><td>41.3</td><td>90.0</td><td>86.9</td></tr><tr><td>SUL[18]</td><td>I-R-v2</td><td>84.5</td><td>85.8</td><td>90.0</td><td>42.0</td><td>90.3</td><td>87.1</td></tr><tr><td>HDA²L [19]</td><td>I-R-v2</td><td>84.7</td><td>86.0</td><td>91.1</td><td>43.0</td><td>91.2</td><td>88.3</td></tr><tr><td>SCDL</td><td>I-R-v2</td><td>84.9</td><td>86.7</td><td>91.3</td><td>43.2</td><td>91.5</td><td>88.8</td></tr></table>\n\n![](images/16f78b8371cfba2ddde40791decea02a5dcc7a1625707aec9e90cd07f2470f04.jpg)  \nFig. 6. Precision-recall curves of various methods on SHREC’13.\n\nTABLE IV THE PERFORMANCE $( \\% )$ ON SHREC’14 DATASET. FOR EACH METRIC, THE BEST RESULT IS IN BOLD, AND THE SECOND-BEST RESULT IS UNDERLINED   \n\n<table><tr><td>Method</td><td>Backbone</td><td>NN</td><td>FT</td><td>ST</td><td>E</td><td>DCG</td><td>mAP</td></tr><tr><td>SBR-VC [6]</td><td></td><td>9.5</td><td>5.0</td><td>8.1</td><td>3.7</td><td>31.9</td><td>5.0</td></tr><tr><td>CDMR [27]</td><td></td><td>10.9</td><td>5.7</td><td>8.9</td><td>4.1</td><td>32.8</td><td>5.4</td></tr><tr><td>Siamese [32]</td><td>3-Layers</td><td>23.9</td><td>21.2</td><td>31.6</td><td>14.0</td><td>49.6</td><td>22.8</td></tr><tr><td>DCML [12]</td><td>AlexNet</td><td>27.2</td><td>27.5</td><td>34.5</td><td>17.1</td><td>49.8</td><td>28.6</td></tr><tr><td>DCHML[13]</td><td>AlexNet</td><td>40.3</td><td>32.9</td><td>39.4</td><td>20.1</td><td>54.4</td><td>33.6</td></tr><tr><td>TCL [78]</td><td>AlexNet</td><td>58.5</td><td>45.5</td><td>53.9</td><td>40.3</td><td>66.6</td><td>47.7</td></tr><tr><td>Shape2Vec [89]</td><td>=</td><td>71.4</td><td>69.7</td><td>74.8</td><td>36.0</td><td>81.1</td><td>72.0</td></tr><tr><td>Semantic [88]</td><td>PointNet</td><td>80.4</td><td>74.9</td><td>81.3</td><td>39.5</td><td>87.0</td><td>78.0</td></tr><tr><td>LWBR [29]</td><td>ResNet-50</td><td>62.1</td><td>64.1</td><td>69.1</td><td>76.0</td><td>36.1</td><td>66.5</td></tr><tr><td>DCA [35]</td><td>ResNet-50</td><td>77.0</td><td>78.9</td><td>82.3</td><td>39.8</td><td>85.9</td><td>80.3</td></tr><tr><td>DPSML [24]</td><td>ResNet-50</td><td>77.4</td><td>79.8</td><td>84.9</td><td>41.5</td><td>87.7</td><td>81.3</td></tr><tr><td>CGN [34]</td><td>ResNet-50</td><td>78.9</td><td>81.1</td><td>85.0</td><td>41.8</td><td>88.1</td><td>83.0</td></tr><tr><td>DSSH [25]</td><td>ResNet-50</td><td>77.5</td><td>78.8</td><td>83.1</td><td>40.4</td><td>87.0</td><td>80.6</td></tr><tr><td>HEAR [26]</td><td>ResNet-50</td><td>79.2</td><td>80.7</td><td>84.6</td><td>40.9</td><td>87.8</td><td>82.2</td></tr><tr><td>SUL[18]</td><td>ResNet-50</td><td>79.4</td><td>81.9</td><td>86.3</td><td>41.8</td><td>88.9</td><td>83.4</td></tr><tr><td>HDA²L [19]</td><td>ResNet-50</td><td>79.2</td><td>81.7</td><td>86.5</td><td>41.9</td><td>89.0</td><td>83.5</td></tr><tr><td>SCDL</td><td>ResNet-50</td><td>81.0</td><td>82.8</td><td>87.3</td><td>42.1</td><td>89.3</td><td>84.0</td></tr><tr><td>DSSH [25]</td><td>I-R-v2</td><td>79.6</td><td>81.3</td><td>85.1</td><td>41.2</td><td>88.1</td><td>82.6</td></tr><tr><td>HEAR [26]</td><td>I-R-v2</td><td>80.9</td><td>82.6</td><td>86.3</td><td>41.4</td><td>89.0</td><td>83.4</td></tr><tr><td>SUL [18]</td><td>I-R-v2</td><td>81.1</td><td>82.9</td><td>87.1</td><td>42.0</td><td>89.5</td><td>83.9</td></tr><tr><td>HDA²L [19]</td><td>I-R-v2</td><td>81.1</td><td>83.2</td><td>87.3</td><td>42.2</td><td>89.5</td><td>84.1</td></tr><tr><td>SCDL</td><td>I-R-v2</td><td>81.5</td><td>83.9</td><td>87.7</td><td>42.5</td><td>89.8</td><td>84.8</td></tr></table>\n\nIn addition, the precision-recall curves are illustrated in Fig.6 to visualize our results on SHREC’13. The sky-blue curve represents the performance of our proposed method. As shown, SCDL occupies a prominent position at the top, maintaining consistently high precision across a broad range of recall values and decreasing more slowly when recall is low. Given that top-ranked retrieval results are preferable, the larger area under SCDL’s curve demonstrates its superior overall performance, indicating its effectiveness and reliability for SBSR.\n\n2) Retrieval on SHREC’14 Dataset: On the more challenging dataset SHREC‘14, our SCDL also outperforms the previous methods.\n\n(1) As illustrated in Table IV, SCDL achieves the highest mAP under all backbones and metrics. Specifically, our SCDL achieves $8 4 . 0 \\%$ on the comprehensive evaluation metric mAP with the ResNet-50 backbone network, which is $0 . 5 \\%$ , $0 . 6 \\%$ and $1 . 8 \\%$ higher by than the method $\\mathrm { H D A } ^ { 2 } \\mathrm { L }$ , SUL and HEAR, respectively. In others evaluation metrics, SCDL achieves $8 1 . 0 \\%$ , $8 2 . 8 \\%$ , $8 7 . 3 \\%$ , $4 2 . 1 \\%$ and $8 9 . 3 \\%$ on NN, FT, ST, E and DCG, respectively, surpassing the results of current comparative methods. The same scenario also occurs when using more powerful I-R-V2 as the backbone network.\n\n(2) Compared to SHREC’13, our SCDL exhibits more pronounced gains on SHREC’14. On SHREC’14, using ResNet-50 as the backbone network, SCDL achieves improvements over the suboptimal method across the six evaluation metrics (NN, FT, ST, E, DCG, and mAP) with net increases of $0 . 3 \\%$ , $0 . 3 \\%$ , $0 . 9 \\%$ , $0 . 0 \\%$ , $- 0 . 1 \\%$ and $0 . 1 \\%$ compared to SHREC’13. Similarly, with I-R-V2 as the backbone network, SCDL demonstrates improvements over the suboptimal method on SHREC’14 compared to SHREC’13, showing net increases of $0 . 2 \\%$ , $0 . 0 \\%$ , $0 . 2 \\%$ , $0 . 1 \\%$ , $0 . 0 \\%$ , and $0 . 2 \\%$ for the same six metrics, respectively. We attribute this to the amplified benefits of semantic causal disentangled learning with the increase in scale, complexity, and diversity of the dataset, resulting in more obvious improvements in results.\n\nSimilarly, Fig.7 illustrates the corresponding precision-recall curves of SCDL on SHREC’14. The sky-blue curve represents the SCDL method, which demonstrates a distinct advantage over other methods by maintaining high precision across a wide range of recall values (0.1-0.5). As the recall increases, the decline rate of the precision value in SCDL is the slowest, indicating its robustness in SBSR. Meanwhile, the SCDL curve’s larger area relative to other methods further suggests that it effectively balances precision and recall across different threshold levels.\n\n3) Retrieval on PART-SHREC’14 Dataset: In order to assess the generalization capabilities of our SCDL on unseen 3D shapes, we conducted experiments using the challenging PART-SHREC14 dataset. We selected four advanced techniques from the literature with documented results on PART-SHREC’14: Siamese [32], Semantic [88], DSSH [25] and $\\mathrm { H D A } ^ { 2 } \\mathrm { L }$ [19] for comparison. The comparative outcomes are summarized in Table V, revealing that our SCDL demonstrates further enhancements in six evaluation metrics, surpassing the current state-of-the-art performance. These findings illustrate the robust generalization capabilities of our approach.\n\n![](images/f6f54e0bb98bbd0dc9df38ea86c850f217d1e9517abdfc7483b7365acb0c44ce.jpg)  \nFig. 7. Precision-recall curves of various methods on SHREC’14.\n\nTABLE V THE PERFORMANCE $( \\% )$ ON PART-SHREC’14 DATASET. FOR EACH METRIC, THE BEST RESULT IS IN BOLD   \n\n<table><tr><td>Method</td><td>Backbone</td><td>NN</td><td>FT</td><td>ST</td><td>E</td><td>DCG</td><td>mAP</td></tr><tr><td>Siamese [32]</td><td>3-Layers</td><td>11.8</td><td>7.6</td><td>13.2</td><td>7.3</td><td>40.0</td><td>6.7</td></tr><tr><td>Semantic [88]</td><td>PointNet</td><td>84.0</td><td>63.4</td><td>74.5</td><td>52.6</td><td>84.8</td><td>67.6</td></tr><tr><td>DSSH [25]</td><td>ResNet-50</td><td>83.8</td><td>77.7</td><td>84.8</td><td>62.4</td><td>88.8</td><td>80.6</td></tr><tr><td>HDA²L [19]</td><td>ResNet-50</td><td>83.5</td><td>76.3</td><td>85.6</td><td>71.5</td><td>89.9</td><td>81.9</td></tr><tr><td>SCDL</td><td>ResNet-50</td><td>84.4</td><td>78.3</td><td>86.0</td><td>71.9</td><td>90.2</td><td>82.3</td></tr></table>\n\n# F. Ablation Study\n\nTo conduct multiple comparative experiments, all ablation experiments are implemented on SHREC’13 using the proposed SCDL with ResNet-50 as the backbone network.\n\n1) Influence of Different Components: We conduct comprehensive ablation studies using the comprehensive evaluation metric mAP $( \\% )$ to evaluate each component of the proposed method. Specifically, we sequentially add the following components to the backbone network: $\\mathrm { D L _ { s } }$ , $\\mathbf { S C M } _ { \\mathrm { s } }$ , $\\mathrm { D L } _ { 3 \\mathrm { D } }$ and $\\mathrm { S C M } _ { 3 \\mathrm { D } }$ , where DL represent the regular disentangled learning with one VAE framework, SCM represents the proposed mask-based causal representation, the subscript $\\pmb { s }$ and $3 D$ respectively denote the corresponding components acting on the sketch branch and the 3D shape branch. As shown in Table VI, we can observe that:\n\n• The deep learning-based baseline is simple yet powerful, achieving $8 5 . 4 \\%$ mAP, outperforming many previous methods.\n\n• As shown in the experimental results of No.2 and No.4, adding DL to the sketch branch results in a performance increase of $1 . 7 \\%$ from $8 5 . 4 \\%$ to $8 7 . 1 \\%$ . Similarly, adding DL into the 3D shape branch also results in a performance increase from $8 5 . 4 \\%$ to $8 5 . 7 \\%$ , indicating a $0 . 3 \\%$ improvement. Overall, compared to the backbone network, the addition of DL has shown improvements in both sketch and 3D shape branches. This suggests that features extracted directly from CNN contain noice attributes, and disentangled learning can eliminate the negative impact of some noisy attributes to a certain extent. In addition, the improvement in the sketch branch is more pronounced compared to the enhancement in the 3D shape branch. We attribute this to the fact that sketch data is hand-drawn, containing more subjective intentions compared to real-world 3D model data, thereby resulting in more noise attributes.\n\nTABLE VI THE MAP PERFORMANCE $( \\% )$ OF DIFFERENT COMPONENTS ON SHREC’13   \n\n<table><tr><td>Num</td><td>Backbone</td><td>DLs</td><td>SCMs</td><td>DL3D</td><td>SCM3D</td><td>mAP</td></tr><tr><td>1</td><td>三</td><td>1</td><td>1</td><td>1</td><td>1</td><td>85.4</td></tr><tr><td>2</td><td></td><td>√</td><td></td><td></td><td>1</td><td>87.1</td></tr><tr><td>3</td><td></td><td>√</td><td>√</td><td></td><td>1</td><td>88.2</td></tr><tr><td>4</td><td>√</td><td>1</td><td>1</td><td>「</td><td></td><td>85.7</td></tr><tr><td>5</td><td>√</td><td></td><td></td><td></td><td>√</td><td>86.1</td></tr><tr><td>6</td><td>\\</td><td>厂</td><td></td><td></td><td>\\</td><td>87.9</td></tr></table>\n\n• As shown in the experimental results of No.3 and No.5, building upon DL, the inclusion of SCM in the sketch branch leads to an improvement from $8 7 . 1 \\%$ to $8 8 . 2 \\%$ , resulting in a $1 . 1 \\%$ increase. Similarly, incorporating SCM into the 3D shape branch results in a performance increase from $8 5 . 7 \\%$ to $8 6 . 1 \\%$ , indicating a $0 . 4 \\%$ improvement. The performance improvements in both branches suggest that the designed SCM with causal intervention can effectively discern semantic attributes and confounders, which further facilitating performance improvement by combing the DL.\n\n• Based on the aforementioned results, a natural idea is to apply $_ { \\mathrm { D L + S C M } }$ simultaneously to both the sketch and 3D shape branches. As shown in the experimental results of No.6, the dual-branch structure achieves the mAP of $8 7 . 9 \\%$ , higher than the $8 6 . 1 \\%$ achieved solely on the 3D shape branch and lower than the $8 8 . 2 \\%$ achieved only on the sketch branch. We attribute this to the fact that sketches are sparse data with semantic attributes are concentrated in the feature dimensions, allowing the designed SCDL to effectively preserve their complete semantic attributes. On the other hand, 3D shapes are feature-dense with highly tangled, requiring further consideration tailored to their data characteristics. Moreover, categories of datasets are coarse-grained category, especially for animal categories. Apart from conventional one-hot labels, it lacks more fine-grained descriptions. Thus, under the same categorylevel semantics, multiple instance-level semantic attributes may exist, thereby reducing the experimental results.\n\n2) Influence of Different SCDL Levels: We conduct ablation studies on the SCDL network layers to assess performance under different configurations. Table VII shows mAP performance $( \\% )$ on the SHREC’13 dataset with varying numbers of confounders and semantic attributes. In the first five rows, increasing confounders from 1 to 3 (Configurations No.1-3) boosts mAP, peaking at $8 8 . 2 \\%$ in Configuration No.3. This result suggests that adding confounders enhances the separation of confounding information up to a point, after which mAP slightly declines (Configurations No.4-5), likely due to introduced unhelpful noise. With confounders fixed at 3, varying semantic attributes further reveals that Configuration No.6 (3 confounders, 2 semantic attributes) also achieves peak mAP, while additional semantic attributes (Configurations No.7-9) reduce performance, suggesting diminishing returns from added complexity.\n\nTABLE VII THE MAP PERFORMANCE $( \\% )$ OF DIFFERENT SCDL LAYERS ON SHREC’13   \n\n<table><tr><td>Num</td><td>Confounders</td><td>Semantic</td><td>mAP</td></tr><tr><td>1</td><td>1</td><td>1</td><td>86.9</td></tr><tr><td>2</td><td>2</td><td>1</td><td>87.6</td></tr><tr><td>3</td><td>3</td><td>1</td><td>88.2</td></tr><tr><td>4</td><td>4</td><td>1</td><td>88.0</td></tr><tr><td>5</td><td>5</td><td>1</td><td>87.8</td></tr><tr><td>6</td><td>3</td><td>2</td><td>88.2</td></tr><tr><td>7</td><td>3</td><td>3</td><td>88.0</td></tr><tr><td>8</td><td>3</td><td>4</td><td>87.7</td></tr><tr><td>9</td><td>3</td><td>5</td><td>87.6</td></tr></table>\n\nTABLE VIII THE mAP PERFORMANCE $( \\% )$ OF DIFFERENT LEARNING STRATEGIES ON SHREC’13   \n\n<table><tr><td>Num</td><td>Lce</td><td>LELBO</td><td>Lam</td><td>Ltcl</td><td>mAP</td></tr><tr><td>1</td><td>?</td><td></td><td>1</td><td>1</td><td>85.4</td></tr><tr><td>2</td><td></td><td>-&gt;&gt;&gt;</td><td></td><td>1</td><td>86.7</td></tr><tr><td>3</td><td></td><td></td><td>厂</td><td></td><td>87.0</td></tr><tr><td>4</td><td>1</td><td></td><td></td><td>√</td><td>88.2</td></tr></table>\n\nThe abvove results highlight the importance of balancing confounders and semantic attributes for optimal performance. While mAP remains stable across configurations, specific optimal settings (e.g., No.3 and No.6) emphasize the value of fine-tuning. Currently, ratio parameters are still set based on manual experience. Exploring an adaptive ratio method will be a future research direction.\n\n3) Influence of Learning Strategies: This section evaluates the impact of various learning strategies on the performance of SCDL. Four strategies involving different loss functions are tested: Cross Entropy Loss $( \\mathcal { L } _ { c e } )$ , ELBO $( \\mathcal { L } _ { E L B O } )$ combining reconstruction and KL divergence losses, AM-softmax Loss $( { \\mathcal { L } } _ { a m } )$ , and Triplet Center Loss $( \\mathcal { L } _ { t c l } )$ .\n\nFrom Table VIII, we can see that: $\\bullet$ When only utilizing $\\mathcal { L } _ { c e }$ (No.1), representing the backbone network, the performance reaches $8 5 . 4 \\%$ . • After adding the proposed $\\mathcal { L } _ { E L B O }$ (No.2), the mAP performance improves to $8 6 . 7 \\%$ , a gain of $1 . 3 \\%$ . This indicates the effectiveness of the designed VAE architecture. • We replace $\\mathcal { L } _ { c e }$ with $\\mathcal { L } _ { a m }$ (No.3), resulting in a further performance improvement, from $8 6 . 7 \\%$ up to $8 7 . 0 \\%$ . This suggests that $\\mathcal { L } _ { a m }$ exhibits better generalization compared to $\\mathcal { L } _ { c e }$ , with less susceptibility to noise or interference factors. • Furthermore, with the addition of metric loss $\\mathcal { L } _ { c e }$ (No.4), the mAP performance increases from $8 7 . 0 \\%$ to $8 8 . 2 \\%$ , indicating that the synergy strategy of classification loss and metric loss can achieve fine results.\n\n# G. Comparison of Computational Efficiency\n\nIn this section, we discuss performance of SCDL in terms of computational efficiency. Table IX shows comparative results of computational efficiency among previous SOTA methods, including CGN [34], HEAR [26], SUL [18], $\\mathrm { H D A } ^ { 2 } \\mathrm { L }$ [19], and our SCDL. We employ three common metrics to evaluate parameter count and computational complexity, where P, F, and I denote the parameter (M), FLOPs (G), and mean inference time (ms), respectively. Due to the absence of publicly available results on these three metrics in previous methods, the data in the table are derived from outcomes reproduced based on the methodologies outlined in the original papers.\n\nTABLE IX THE COMPARISON OF COMPUTATIONAL EFFICIENCY ACROSS METHODS, WHERE P, F, AND I REPRESENT THE NUMBER OF PARAMETERS (M), FLOPS (G), AND MEAN INFERENCE TIME (MS), RESPECTIVELY. THE BEST RESULTS ARE BOLDED, AND THE SECOND-BEST RESULT IS UNDERLINED   \n\n<table><tr><td>Method</td><td>P</td><td>F</td><td>I</td></tr><tr><td>CGN[34]</td><td>126.50</td><td>14.62</td><td>129.63</td></tr><tr><td>HEAR[26]</td><td>356.30</td><td>26.82</td><td>207.50</td></tr><tr><td>SUL[18]</td><td>103.70</td><td>13.68</td><td>135.72</td></tr><tr><td>HDA²L [19]</td><td>298.70</td><td>19.68</td><td>162.69</td></tr><tr><td>SCDL</td><td>115.40</td><td>10.56</td><td>112.75</td></tr></table>\n\nTABLE X THE RESULTS EVALUATION OF OUR SEMANTIC ATTRIBUTES AND THE DISENTANGLEMENT ON SHREC’13 WITH RESNET50   \n\n<table><tr><td>Num</td><td>Attributes</td><td>NN</td><td>FT</td><td>ST</td><td>E</td><td>DCG</td><td>mAP</td></tr><tr><td>1</td><td>ze1</td><td>56.1</td><td>57.8</td><td>62.7</td><td>31.9</td><td>64.3</td><td>52.2</td></tr><tr><td>2</td><td></td><td>65.4</td><td>66.2</td><td>70.2</td><td>36.8</td><td>71.9</td><td>60.7</td></tr><tr><td>3</td><td></td><td>58.5</td><td>60.3</td><td>69.7</td><td>35.9</td><td>68.5</td><td>57.5</td></tr><tr><td>4</td><td>z01+z02 +zc3</td><td>70.5</td><td>72.6</td><td>75.8</td><td>37.1</td><td>76.2</td><td>67.3</td></tr><tr><td>5</td><td>N</td><td>84.5</td><td>86.1</td><td>90.8</td><td>42.2</td><td>90.7</td><td>88.2</td></tr></table>\n\nBased on these results, our SCDL demonstrates superior performance in computational efficiency. Specifically, it achieves the second-lowest parameter count at 289.40/M and attains state-of-the-art results for the other two metrics, with 10.56/G FLOPs and an inference time of $1 1 2 . 7 5 / \\mathrm { m s }$ . These results indicate that the model maintains low complexity, enabling operation even with limited computational resources and delivering rapid response times during inference.\n\n# V. DISCUSSION\n\nTo further validate the robustness advantages of the semantic attribute latent variable over others, we conduct both quantitative and qualitative experiments.\n\n# A. Quantitatively Evaluation\n\nTo further valid the benefit regarding the robustness of semantic hidden variables over others, we implement a two-step tuning method for semantic attributes $z ^ { s }$ and other confounders $z ^ { \\bar { c 1 } } , \\ z ^ { c 2 }$ and $z ^ { c 3 }$ . Specifically, after training the whole model, we obtain the retrieval-related hidden variable $z ^ { s }$ and the retrieval-unrelated hidden variables $z ^ { c 1 }$ , $z ^ { c 2 }$ and $z ^ { c 3 }$ . We additionally train fully connected layers to retrieval 3D shapes respectively by $z ^ { s }  y$ , $z ^ { c 1 / 2 / 3 } \\xrightarrow { } y$ respectively, and $z ^ { c 1 } { + z ^ { c 2 } } + \\bar { z } ^ { c 3 } \\to \\bar { y }$ . The results, as shown in Table X, indicate that testing with individual confounders alone yields poor performance. A slight improvement is observed when all confounders are used together in testing. However, compared to using the semantic attribute, the test results with confounders consistently show a noticeable decrease in performance. This can validate the existence of semantic variables that are spuriously correlated to the retrieval task and can be learned during the data-fitting process. Separating the effective variables out during training can help avoid spurious correlation and therefore achieve more robustness.\n\n![](images/0e6371a16bf3b520626c0a55dc84064726b5767532b78e20fcd52267acdcae2b.jpg)  \nFig. 8. The visualization results of the qualitatively experiment on SHREC’14. R represents the original sketches, and G represents the generated counterfactual sketches.\n\n# B. Qualitatively Evaluation\n\nIn this section, we design an intervention experiment aims to verify whether the disentangled latent variables in SCDL (confounding and semantic attributes) exhibit interpretability consistent with our expectations. During the training process of SCDL, we employ the causal intervention operation to swap semantic latent variables of the same category and generate images of the same category. We attempt different-category sketch inputs with while keeping other components in SCDL unchanged with the aim of generating counterfactual sketches that do not exist in the dataset, demonstrating that we have identified the semantic attribute branch among numerous latent variables.\n\n• A generative model is trained. • Two sketches of the different category from the dataset is fed to the encoder to generate latent codes $Z _ { k 1 }$ and $Z _ { k 2 }$ , defined in Eq.6. • We swap the value of $z _ { k 1 } ^ { s , l }$ and $z _ { k 2 } ^ { s , l }$ corresponding to semantic attributes, defined in Eq.7 and Eq.8. • The intervened latent code $\\widetilde { Z } _ { k 1 }$ and $\\widetilde { Z } _ { k 2 }$ (defined in Eq.8) passes through the decoder to generate new sketches. Since the inputs are sketches from different categories, the reconstructed output will be the other category in the input pair.\n\nFig.8 shows some results of intervention experiments. Notably, the two categories intervened in the experiments have a certain level of interrelation; for instance, both ‘dog’ and ‘lion’ belong to the animals, and ‘pickup_truck’ and ‘suv’ fall under cars. The reason for such selection is to ensure that the generated sketches still possess some discriminative features. Conversely, selecting two entirely unrelated categories may result in generated sketches that are completely unrecognizable, looking like various random doodles.\n\nFrom Fig.8, it is evident that when we input sketches of different categories, the generated counterfactual sketches can meet our expectations, for 1) the generated counterfactual sketches are not present in the original dataset, yet still retain recognizability; 2) the generated counterfactual sketches are interpretable, corresponding to each group, such as ‘dog’ and ‘lion’, ‘pickup_truck’ and ‘suv’, where categories of each generated sketch remain unchanged, while the drawing style, abstraction level, etc., are similar to another sketch in each pair. This similarity suggests that the swapped latent variables precisely represent branches of semantic attributes, validating the effectiveness of the proposed SCDL.\n\n![](images/b5d80a924d5ac8b74f06acdbae82cae65dbbb70616ecbbcdd04690f51cb55bb1.jpg)  \nFig. 9. The t-SNE visualization of distribution of features from ten classes (colors) on SHREC’14, where circles represent sketches, triangles represent 3D shapes, (A) represents the trained features extracted by backbone network, and (B) represents the trained features extracted by SCDL.\n\n# VI. VISUALIZATION\n\nTo further illustrate the impact of the proposed SCDL and present diverse experimental results, we show two kinds of visualization results on SHREC’14, the one is the distribution of features, and the other is the instance-level retrieval results.\n\n# A. Distribution of Features\n\nWe extract the final features for testing sketches and 3D shapes of ten randomly selected categories on SHREC’14 through the backbone network and SCDL, then visualize them by the t-SNE method in Fig.9. As shown in the figure:\n\n• The t-SNE visualization result through the backbone network is shown in (A). From (A), we can see that the distribution of features is irregular in the common embedding space and the feature clusters for each category are relatively large. In addition, the distance between different classes is small, and the boundaries are blurry, even overlap.\n\n• The t-SNE result of the proposed SCDL is shown in (B). Compared with (A), above situations are significantly improved in (B). Specifically, the clustering range of all categories is more concentrated, and the margins between different categories are more apparent, with no overlapping or confusion.\n\n# B. Instance-Level Retrieval Results\n\nFig.10 visually compares the instance-level retrieval results of the backbone network and SCDL on SHREC’14. From these results, we can observe that the backbone network includes numerous erroneous instances, while the results from our proposed SCDL are nearly all correct. Further observation reveals that these erroneous retrieval results all stem from queries of similar categories, such as ‘chair’ and ‘armchair’, ‘bike’ and ‘motorbike’, ‘fish’, ‘dophlin’ and ‘shark’, etc. Similar categories of 3D shapes exhibit similar appearances and structures, making even human differentiation prone to error, greatly increasing the difficulty of retrieval. Even in such scenarios, our SCDL can still retrieve almost all correct results, with only one error appearing in each of the top-10 results in two categories. In summary, compared to the backbone network, the performance of SCDL is superior, yielding more accurate retrieval results.\n\n![](images/9cdf89d5ebe26f9a37caaa5a7b572110ca9933fe9a374e78d449f9cc2ba387a1.jpg)  \nFig. 10. Rop-10 retrieval results on SHREC’14. For each sketch query on the left, top row is the results of backbone network and bottom row corresponds to SCDL, where 3D shapes colored in purple are correct and in gray are wrong.\n\n# VII. CONCLUSION\n\nIn this paper, we addressed a key challenge in the sketchbased 3D shape retrieval (SBSR) task – how to eliminate confounders in sketch features. Specifically, we propose a novel sketch causal disentangled learning for SBSR, named SCDL. SCDL develops a structural causal model (SCM) in the sketch branch to disentangle several latent variables, in which a novel mask-based causal intervention is designed to identify semantic attributes and confounders effectively. The resulting disentangled sketch semantic features are finally matched with 3D shapes to complete more accurate retrieval tasks under a tailored learning strategy. Qualitative and quantitative experiments show the superiority of the proposed method.\n\n# REFERENCES\n\n[1] T. Funkhouser et al., “A search engine for 3D models,” ACM Trans. Graph., vol. 22, no. 1, pp. 83–105, Jan. 2003.\n\n[2] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, “The Princeton shape benchmark,” in Proc. Shape Model. Appl., 2004, pp. 167–388.   \n[3] L. Wang, X. Qian, X. Zhang, and X. Hou, “Sketch-based image retrieval with multi-clustering re-ranking,” IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 12, pp. 4929–4943, Dec. 2020.   \n[4] H. Ren et al., “ACNet: Approaching-and-Centralizing network for zeroshot sketch-based image retrieval,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 9, pp. 5022–5035, Sep. 2023.   \n[5] X. Wang, D. Peng, P. Hu, Y. Gong, and Y. Chen, “Cross-domain alignment for zero-shot sketch-based image retrieval,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 11, pp. 7024–7035, Nov. 2023.   \n[6] B. Li et al., “Shrec’13 track: Large scale sketch-based 3D shape retrieval,” in Proc. 6th Eurographics Workshop 3D Object Retr., 2013, pp. 89–96.   \n[7] B. Li et al., “Shrec’14 track: Extended large scale sketch-based 3D shape retrieval,” in Proc. Eurographics Workshop 3D Object Retr., 2014, pp. 121–130.   \n[8] B. Li et al., “Shrec’16 track: 3D sketch-based 3D shape retrieval,” in Proc. Eurographics Workshop 3D Object Retr. (3DOR), 2016, pp. 7–8.   \n[9] J. Qin et al., “SHREC’22 track: Sketch-based 3D shape retrieval in the wild,” Comput. Graph., vol. 107, pp. 104–115, Oct. 2022.   \n10] H. Lin, H. Wen, X. Song, M. Liu, Y. Hu, and L. Nie, “Fine-grained textual inversion network for zero-shot composed image retrieval,” in Proc. 47th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. 2024, pp. 240–250.   \n[11] H. Wen, X. Song, J. Yin, J. Wu, W. Guan, and L. Nie, “Self-training boosted multi-factor matching network for composed image retrieval,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 5, pp. 3665–3678, May 2024.   \n[12] G. Dai, J. Xie, F. Zhu, and Y. Fang, “Deep correlated metric learning for sketch-based 3D shape retrieval,” in Proc. Thirty-First AAAI Conf. Artif. Intell., vol. 31, Feb. 2017, pp. 4002–4008.   \n[13] G. Dai, J. Xie, and Y. Fang, “Deep correlated holistic metric learning for sketch-based 3D shape retrieval,” IEEE Trans. Image Process., vol. 27, no. 7, pp. 3374–3386, Jul. 2018.   \n[14] S. Lu, Y. Liu, and A. W.-K. Kong, “TF-ICON: Diffusion-based trainingfree cross-domain image composition,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2023, pp. 2294–2305.   \n[15] Y. Guo et al., “A broader study of cross-domain few-shot learning,” in Proc. 16th Eur. Conf. Comput. Vis., Glasgow, U.K. Cham, Switzerland: Springer, Aug. 2020, pp. 124–141.   \n[16] Y. Zou, X. Yang, Z. Yu, B. V. Kumar, and J. Kautz, “Joint disentangling and adaptation for cross-domain person re-identification,” in Proc. 16th Eur. Conf. Comput. Vis. (ECCV), Glasgow, U.K. Cham, Switzerland: Springer, Aug. 2020, pp. 87–104.   \n[17] M. Cao, Y. Bai, Z. Cao, L. Nie, and M. Zhang, “Efficient image-text retrieval via keyword-guided pre-screening,” IEEE Trans. Circuits Syst. Video Technol., vol. 34, no. 6, pp. 5132–5145, Jun. 2024.   \n[18] S. Liang, W. Dai, and Y. Wei, “Uncertainty learning for noise resistant sketch-based 3D shape retrieval,” IEEE Trans. Image Process., vol. 30, pp. 8632–8643, 2021.   \n[19] S. Bai and J. Bai, “HDA2L: Hierarchical domain-augmented adaptive learning for sketch-based 3D shape retrieval,” Knowledge-Based Syst., vol. 264, Mar. 2023, Art. no. 110302.   \n[20] W. Li, J. Bai, and H. Zheng, $\\mathrm { \\Omega } ^ { \\mathrm { 4 6 } } \\mathrm { D } ^ { 2 } \\mathrm { G L }$ : Dual-level dual-scale graph learning for sketch-based 3D shape retrieval,” Pattern Recognit., vol. 156, Dec. 2024, Art. no. 110768.   \n[21] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” ACM Trans. Graph., vol. 31, no. 4, pp. 1–10, Jul. 2012.   \n[22] P. Daras and A. Axenopoulos, “A 3D shape retrieval framework supporting multimodal queries,” Int. J. Comput. Vis., vol. 89, nos. 2–3, pp. 229–247, Sep. 2010.   \n[23] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional neural networks for 3D shape recognition,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 945–953.   \n[24] Y. Lei, Z. Zhou, P. Zhang, Y. Guo, Z. Ma, and L. Liu, “A sketch based 3D shape retrieval approach based on efficient deep point-to-subspace metric learning,” Pattern Recognit., vol. 96, Jan. 2019, Art. no. 106981.   \n[25] J. Chen et al., “Deep sketch-shape hashing with segmented 3D stochastic viewing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 791–800.   \n[26] J. Chen, J. Qin, Y. Shen, L. Liu, F. Zhu, and L. Shao, “Learning attentive and hierarchical representations for 3D shape recognition,” in Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, 2020, pp. 105–122.   \n[27] T. Furuya and R. Ohbuchi, “Ranking on cross-domain manifold for sketch-based 3D model retrieval,” in Proc. Int. Conf. Cyberworlds, Oct. 2013, pp. 274–281.   \n[28] H. Tabia and H. Laga, “Learning shape retrieval from different modalities,” Neurocomputing, vol. 253, pp. 24–33, Aug. 2017.   \n[29] J. Bai, M. Wang, and D. Kong, “Deep common semantic space embedding for sketch-based 3D model retrieval,” Entropy, vol. 21, no. 4, p. 369, Apr. 2019.   \n[30] Y. Zhao, Q. Liang, R. Ma, W. Nie, and Y. Su, “JFLN: Joint feature learning network for 2D sketch based 3D shape retrieval,” J. Vis. Commun. Image Represent., vol. 89, Nov. 2022, Art. no. 103668.   \n[31] P. Xu, Z. Song, Q. Yin, Y.-Z. Song, and L. Wang, “Deep self-supervised representation learning for free-hand sketch,” IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 4, pp. 1503–1513, Apr. 2021.   \n[32] F. Wang, L. Kang, and Y. Li, “Sketch-based 3D shape retrieval using convolutional neural networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 1875–1883.   \n[33] F. Zhu, J. Xie, and Y. Fang, “Learning cross-domain neural networks for sketch-based 3D shape retrieval,” in Proc. AAAI Conf. Artif. Intell., vol. 30, Mar. 2016, pp. 3683–3689.   \n[34] W. Dai and S. Liang, “Cross-modal guidance network for sketch-based 3D shape retrieval,” in Proc. IEEE Int. Conf. Multimedia Expo (ICME), Jul. 2020, pp. 1–6.   \n[35] J. Chen and Y. Fang, “Deep cross-modality adaptation via semantics preserving adversarial learning for sketch-based 3D shape retrieval,” in Proc. Eur. Conf. Comput. Vis. (ECCV), Jan. 2018, pp. 605–620.   \n[36] S. Bai, J. Bai, H. Xu, J. Tuo, and M. Liu, “PAGML: Precise alignment guided metric learning for sketch-based 3D shape retrieval,” Image Vis. Comput., vol. 136, Aug. 2023, Art. no. 104756.   \n[37] K. Chalupka, F. Eberhardt, and P. Perona, “Causal feature learning: An overview,” Behaviormetrika, vol. 44, no. 1, pp. 137–164, Jan. 2017.   \n[38] D. Zhang, H. Zhang, J. Tang, X.-S. Hua, and Q. Sun, “Causal intervention for weakly-supervised semantic segmentation,” in Proc. NIPS, vol. 33, 2020, pp. 655–666.   \n[39] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, and L. Bottou, “Discovering causal signals in images,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017, pp. 6979–6987.   \n[40] X. Zhang, P. Cui, R. Xu, L. Zhou, Y. He, and Z. Shen, “Deep stable learning for out-of-distribution generalization,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 5372–5382.   \n[41] Y. Pan, Z. Li, L. Zhang, and J. Tang, “Distilling knowledge in causal inference for unbiased visual question answering,” in Proc. 2nd ACM Int. Conf. Multimedia Asia, Mar. 2021, pp. 1–7.   \n[42] Y. Pan, Z. Li, L. Zhang, and J. Tang, “Causal inference with knowledge distilling and curriculum learning for unbiased VQA,” ACM Trans. Multimedia Comput., Commun., Appl., vol. 18, no. 3, pp. 1–23, Aug. 2022.   \n[43] W. Nie, R. Chen, W. Wang, B. Lepri, and N. Sebe, “T2TD: Text-3D generation model based on prior knowledge guidance,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 47, no. 1, pp. 172–189, Jan. 2025.   \n[44] W. Nie, R. Chang, M. Ren, Y. Su, and A. Liu, “I-GCN: Incremental graph convolution network for conversation emotion detection,” IEEE Trans. Multimedia, vol. 24, pp. 4471–4481, 2022.   \n[45] R. Chang, Y. Ma, W. Nie, J. Nie, Y. Zhu, and A.-A. Liu, “Causal disentanglement-based hidden Markov model for cross-domain bearing fault diagnosis,” IEEE Trans. Neural Netw. Learn. Syst., early access, Dec. 16, 2024, doi: 10.1109/TNNLS.2024.3513329.   \n[46] J. Pearl, “Does obesity shorten life? Or is it the soda? On nonmanipulable causes,” J. Causal Inference, vol. 6, no. 2, Sep. 2018, Art. no. 20182001.   \n[47] X. Yang, H. Zhang, and J. Cai, “Deconfounded image captioning: A causal retrospect,” in Proc. IEEE Trans. Pattern Anal. Mach. Intell., Dec. 2021, p. 1.   \n[48] W. Li, X. Su, D. Song, L. Wang, K. Zhang, and A.-A. Liu, “Towards deconfounded image-text matching with causal inference,” in Proc. 31st ACM Int. Conf. Multimedia, Oct. 2023, pp. 6264–6273.   \n[49] D. Yang et al., “Context de-confounded emotion recognition,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, pp. 19005–19015.   \n[50] H. Wei et al., “Synthesizing counterfactual samples for effective imagetext matching,” in Proc. 30th ACM Int. Conf. Multimedia, Oct. 2022, pp. 4355–4364.   \n[51] T. Wang, J. Huang, H. Zhang, and Q. Sun, “Visual commonsense R-CNN,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 10760–10770.   \n[52] S. Zhang et al., “DeVLBert: Learning deconfounded visio-linguistic representations,” in Proc. 28th ACM Int. Conf. Multimedia, Oct. 2020, pp. 4373–4382.   \n[53] X. Yang, H. Zhang, G. Qi, and J. Cai, “Causal attention for visionlanguage tasks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2021, pp. 9847–9857.   \n[54] I. Higgins et al., “Beta-vae: Learning basic visual concepts with a constrained variational framework,” in Proc. Int. Conf. Learn. Represent., 2016, pp. 1–11.   \n[55] C. Eastwood and C. K. I. Williams, “A framework for the quantitative evaluation of disentangled representations,” in Proc. Int. Conf. Learn. Represent., Feb. 2018, pp. 1–16.   \n[56] N. Hassanpour and R. Greiner, “Learning disentangled representations for CounterFactual regression,” in Proc. Int. Conf. Learn. Represent., Apr. 2020, pp. 1–9.   \n[57] Z. Zhang, L. Tran, F. Liu, and X. Liu, “On learning disentangled representations for gait recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 345–360, Jan. 2022.   \n[58] M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang, “CausalVAE: Disentangled representation learning via neural structural causal models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2021, pp. 9593–9602.   \n[59] L. Wang, Z. Cai, G. D. Melo, Z. Cao, and L. He, “Disentangled CVAEs with contrastive learning for explainable recommendation,” in Proc. AAAI Conf. Artif. Intell., vol. 37, Jun. 2023, pp. 13691–13699.   \n[60] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in Proc. Int. Conf. Learn. Represent., Jan. 2014, pp. 1–14.   \n[61] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, “InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets,” in Proc. Adv. Neural Inf. Y. LeCun, “Disentangling factors of variation in deep representation using adversarial training,” in Proc. Adv. Neural Inf. Process. Syst., vol. 29, 2016, pp. 1–10.   \n[63] Y. Wang et al., “Orthogonal deep features decomposition for ageinvariant face recognition,” in Proc. Eur. Conf. Comput. Vis. (ECCV), Jan. 2018, pp. 764–779.   \n[64] L. Wei, L. Hu, V. G. Kim, E. Yumer, and H. Li, “Real-time hair rendering using sequential adversarial networks,” in Proc. Eur. Conf. Comput. Vis. (ECCV), Jan. 2018, pp. 99–116.   \n[65] L. Yang and A. Yao, “Disentangling latent hands for image synthesis and pose estimation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 9869–9878.   \n[66] S. Wang, X. Chen, Q. Z. Sheng, Y. Zhang, and L. Yao, “Causal disentangled variational auto-encoder for preference understanding in recommendation,” in Proc. 46th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. 2023, pp. 1874–1878.   \n[67] Y. Zhao, D. Pan, J. Liu, X. Jia, and M. Wang, “Causal conditional hidden Markov model for multimodal traffic prediction,” in Proc. 37th AAAI Conf. Artif. Intell., 35th Conf. Innov. Appl. Artif. Intell., 13th Symp. Educ. Adv. Artif. Intell., Jan. 2023, pp. 4929–4936.   \n[68] J. Li, B. Wu, X. Sun, and Y. Wang, “Causal hidden Markov model for time series disease forecasting,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 12100–12109.   \n[69] W. Guo, H. Huang, X. Kong, and R. He, “Learning disentangled representation for cross-modal retrieval with deep mutual information estimation,” in Proc. 27th ACM Int. Conf. Multimedia, Oct. 2019, pp. 1712–1720.   \n[70] L. Qu, M. Liu, J. Wu, Z. Gao, and L. Nie, “Dynamic modality interaction modeling for image-text retrieval,” in Proc. 44th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., 2021, pp. 1104–1113.   \n[71] K. Zhang, Z. Mao, Q. Wang, and Y. Zhang, “Negative-aware attention framework for image-text matching,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 15661–15670.   \n[72] A. Sain, A. K. Bhunia, Y. Yang, T. Xiang, and Y.-Z. Song, “StyleMeUp: Towards style-agnostic sketch-based image retrieval,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 8500–8509.   \n[73] C. Chen, M. Ye, M. Qi, and B. Du, “Sketch transformer: Asymmetrical disentanglement learning from dynamic synthesis,” in Proc. 30th ACM Int. Conf. Multimedia, Oct. 2022, pp. 4012–4020.   \n[74] R. Xu, Z. Han, L. Hui, J. Qian, and J. Xie, “Domain disentangled generative adversarial network for zero-shot sketch-based 3D shape retrieval,” in Proc. AAAI Conf. Artif. Intell., vol. 36, Jun. 2022, pp. 2902–2910.   \n[75] J. Tian, K. Wang, X. Xu, Z. Cao, F. Shen, and H. T. Shen, “Multimodal disentanglement variational AutoEncoders for zero-shot cross-modal retrieval,” in Proc. 45th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. 2022, pp. 960–969.   \n[76] B. Gholami, M. El-Khamy, and K.-B. Song, “Latent feature disentanglement for visual domain generalization,” IEEE Trans. Image Process., vol. 32, pp. 5751–5763, 2023.   \n[77] I. Ng, S. Zhu, Z. Fang, H. Li, Z. Chen, and J. Wang, “Masked gradientbased causal structure learning,” in Proc. SIAM Int. Conf. Data Mining (SDM), 2022, pp. 424–432.   \n[78] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai, “Triplet-center loss for multi-view 3D object retrieval,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 1945–1954.   \n[79] F. Wang, J. Cheng, W. Liu, and H. Liu, “Additive margin softmax for face verification,” IEEE Signal Process. Lett., vol. 25, no. 7, pp. 926–930, Jul. 2018.   \n[80] B. Li et al., “A comparison of methods for sketch-based 3D shape retrieval,” Comput. Vis. Image Understand., vol. 119, pp. 57–80, Dec. 2013.   \n[81] M. Eitz, R. Richter, T. Boubekeur, K. Hildebrand, and M. Alexa, “Sketch-based shape retrieval,” ACM Trans. Graph., vol. 31, no. 4, pp. 1–10, Jul. 2012.   \n[82] B. Li et al., “A comparison of 3D shape retrieval methods based on a large-scale benchmark supporting multimodal queries,” Comput. Vis. Image Understand., vol. 131, pp. 1–27, Feb. 2015.   \n[83] B. Li et al., “Shrec’12 track: Generic 3D shape retrieval,” in Proc. Eurographics Workshop 3D Object Retr., 2012, pp. 119–126.   \n[84] A. Tatsuma, H. Koyanagi, and M. Aono, “A large-scale shape benchmark for 3D object retrieval: Toyohashi shape benchmark,” in Proc. Asia Pacific Signal Inf. Process. Assoc. Annu. Summit Conf., Dec. 2012, pp. 1–10.   \n[85] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A large-scale hierarchical image database,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., May 2009, pp. 248–255.   \n[86] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770–778.   \n[87] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet and the impact of residual connections on learning,” in Proc. 31st AAAI Conf. Artif. Intell., S. Singh and S. Markovitch, Eds., San Francisco, CA, USA: AAAI Press, 2017, pp. 4278–4284.   \n[88] A. Qi, Y.-Z. Song, and T. Xiang, “Semantic embedding for sketch-based 3D shape retrieval,” in Proc. BMVC, vol. 3, Jan. 2018, pp. 11–12.   \n[89] F. P. Tasse and N. Dodgson, “Shape2 Vec: Semantic-based descriptors for 3D shapes, sketches and images,” ACM Trans. Graph., vol. 35, no. 6, pp. 1–12, Nov. 2016.\n\n![](images/c4e31d20f8474e03fb81ab6f9603496c6fe5ceaa364a57fca786d72ea7b90da2.jpg)\n\nShaojin Bai received the B.E. degree from Northwestern Polytechnical University, China, in 2017, and the master’s degree from North Minzu University, China, in 2022. He is currently pursuing the Ph.D. degree with Tianjin University, China. His research interests include computer vision and machine learning.\n\n![](images/2b50a16ba616c76ccdfd71ac3c0c61c8c25bef3afd3be64c01bec1de446271a7.jpg)\n\nYalu Li received the B.E. and M.E. degrees in electrical engineering and automation from Nanjing University of Science and Technology, Nanjing, China. She is currently an Engineer with Beijing Institute of Remote Sensing Equipment. Her research interests include control science and engineering, target recognition and detection technology, and quality management.\n\n![](images/66978cb40f6e88e3516faacba3d55315dd490fcc99115855f545bacfecfe5009.jpg)\n\nRihao Chang received the master’s degree from the School of Electrical and Information Engineering, Tianjin University, in 2023. He is currently pursuing the Ph.D. degree with the School of Microelectronics, Tianjin University. His primary research interests include computer vision and cross-media information retrieval.\n\n![](images/dd3f27e07ea45e3449a4c24e3991cd2ddcaaf9e52b0e86cbfaee98dfb329d420.jpg)\n\nQi Liang received the B.S. degree in electronic science and technology from Taiyuan University of Technology, Taiyuan, China, and the Ph.D. degree in electronic engineering with Tianjin University, Tianjin, China. Currently, he is a Researcher with Tianjin Navigation Instruments Research Institute, Tianjin, China. His research interests include 3D shape recognition, cross-domain learning, and data mining.\n\n![](images/7018f3e83327687739e1e09b9e68ab8c4c721b60b50c5b93e410cd8d4716740e.jpg)\n\nWeizhi Nie (Member, IEEE) received the M.S. and Ph.D. degrees in electronic engineering from Tianjin University, China. He worked with the School of Computer, National University of Singapore, in 2016 and 2017, as a Visitor. He is currently a Professor with the School of Electrical and Information Engineering, Tianjin University. His research interests include multiple object tracking, computer vision, and 3D model retrieval.",
    "chunkCount": 116
  }
]